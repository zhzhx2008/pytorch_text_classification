nohup: ignoring input
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='0', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/bert-base-chinese', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_story': 0, 'news_culture': 1, 'news_agriculture': 2, 'news_military': 3, 'news_finance': 4, 'news_sports': 5, 'news_entertainment': 6, 'news_world': 7, 'news_game': 8, 'news_stock': 9, 'news_edu': 10, 'news_house': 11, 'news_travel': 12, 'news_tech': 13, 'news_car': 14}
index_labels_dict={0: 'news_story', 1: 'news_culture', 2: 'news_agriculture', 3: 'news_military', 4: 'news_finance', 5: 'news_sports', 6: 'news_entertainment', 7: 'news_world', 8: 'news_game', 9: 'news_stock', 10: 'news_edu', 11: 'news_house', 12: 'news_travel', 13: 'news_tech', 14: 'news_car'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	65
45	11
44	27
43	41
42	98
41	92
40	133
39	119
38	128
37	169
36	171
35	216
34	294
33	555
32	3351
31	2647
30	2397
29	2194
28	2237
27	2187
26	2247
25	2193
24	2281
23	2352
22	2237
21	2242
20	2031
19	2278
18	1894
17	1980
16	1801
15	1616
14	1491
13	1220
12	970
11	808
10	548
9	340
8	152
7	130
6	20
5	5
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 69s, train loss=2.3623, train acc=26.20%, dev loss=1.9793, dev acc=47.77%
saving, test loss=1.9896, test acc=46.38%
epoch: 2/10000, 69s, train loss=1.8513, train acc=47.38%, dev loss=1.6448, dev acc=51.59%
saving, test loss=1.6587, test acc=51.15%
epoch: 3/10000, 69s, train loss=1.6281, train acc=50.55%, dev loss=1.4982, dev acc=52.47%
saving, test loss=1.5137, test acc=52.61%
epoch: 4/10000, 69s, train loss=1.5281, train acc=51.56%, dev loss=1.4278, dev acc=52.81%
saving, test loss=1.4438, test acc=53.30%
epoch: 5/10000, 69s, train loss=1.4768, train acc=52.33%, dev loss=1.3894, dev acc=52.90%
saving, test loss=1.4055, test acc=53.55%
epoch: 6/10000, 69s, train loss=1.4468, train acc=52.85%, dev loss=1.3650, dev acc=53.26%
saving, test loss=1.3811, test acc=53.97%
epoch: 7/10000, 69s, train loss=1.4288, train acc=52.93%, dev loss=1.3491, dev acc=53.35%
saving, test loss=1.3648, test acc=54.21%
epoch: 8/10000, 69s, train loss=1.4170, train acc=52.88%, dev loss=1.3375, dev acc=53.58%
saving, test loss=1.3530, test acc=54.20%
epoch: 9/10000, 69s, train loss=1.4082, train acc=53.16%, dev loss=1.3296, dev acc=53.69%
saving, test loss=1.3450, test acc=54.22%
epoch: 10/10000, 69s, train loss=1.3944, train acc=53.40%, dev loss=1.3222, dev acc=53.79%
saving, test loss=1.3382, test acc=54.23%
epoch: 11/10000, 69s, train loss=1.3908, train acc=53.47%, dev loss=1.3166, dev acc=53.86%
saving, test loss=1.3319, test acc=54.30%
epoch: 12/10000, 69s, train loss=1.3871, train acc=53.59%, dev loss=1.3124, dev acc=53.95%
saving, test loss=1.3281, test acc=54.41%
epoch: 13/10000, 70s, train loss=1.3806, train acc=53.56%, dev loss=1.3091, dev acc=54.24%
saving, test loss=1.3253, test acc=54.45%
epoch: 14/10000, 70s, train loss=1.3831, train acc=53.49%, dev loss=1.3065, dev acc=54.12%
epoch: 15/10000, 70s, train loss=1.3794, train acc=53.76%, dev loss=1.3034, dev acc=54.07%
epoch: 16/10000, 70s, train loss=1.3715, train acc=53.86%, dev loss=1.3007, dev acc=54.40%
saving, test loss=1.3164, test acc=54.53%
epoch: 17/10000, 70s, train loss=1.3728, train acc=53.77%, dev loss=1.2987, dev acc=54.46%
saving, test loss=1.3146, test acc=54.63%
epoch: 18/10000, 70s, train loss=1.3723, train acc=53.95%, dev loss=1.2975, dev acc=54.52%
saving, test loss=1.3133, test acc=54.56%
epoch: 19/10000, 70s, train loss=1.3687, train acc=53.64%, dev loss=1.2955, dev acc=54.14%
epoch: 20/10000, 70s, train loss=1.3665, train acc=53.97%, dev loss=1.2943, dev acc=54.39%
epoch: 21/10000, 70s, train loss=1.3643, train acc=53.95%, dev loss=1.2936, dev acc=54.54%
saving, test loss=1.3095, test acc=54.57%
epoch: 22/10000, 70s, train loss=1.3635, train acc=54.04%, dev loss=1.2922, dev acc=54.61%
saving, test loss=1.3079, test acc=54.54%
epoch: 23/10000, 70s, train loss=1.3648, train acc=53.81%, dev loss=1.2912, dev acc=54.46%
epoch: 24/10000, 70s, train loss=1.3608, train acc=54.13%, dev loss=1.2906, dev acc=54.55%
epoch: 25/10000, 70s, train loss=1.3615, train acc=54.08%, dev loss=1.2899, dev acc=54.39%
epoch: 26/10000, 70s, train loss=1.3582, train acc=54.18%, dev loss=1.2899, dev acc=54.40%
epoch: 27/10000, 70s, train loss=1.3620, train acc=53.95%, dev loss=1.2892, dev acc=54.39%
time used=2360.6s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='0', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/ckiplab_albert-tiny-chinese', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_tech': 0, 'news_world': 1, 'news_culture': 2, 'news_military': 3, 'news_entertainment': 4, 'news_stock': 5, 'news_game': 6, 'news_sports': 7, 'news_agriculture': 8, 'news_house': 9, 'news_finance': 10, 'news_car': 11, 'news_story': 12, 'news_travel': 13, 'news_edu': 14}
index_labels_dict={0: 'news_tech', 1: 'news_world', 2: 'news_culture', 3: 'news_military', 4: 'news_entertainment', 5: 'news_stock', 6: 'news_game', 7: 'news_sports', 8: 'news_agriculture', 9: 'news_house', 10: 'news_finance', 11: 'news_car', 12: 'news_story', 13: 'news_travel', 14: 'news_edu'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/ckiplab_albert-tiny-chinese were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of AlbertModel were not initialized from the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/ckiplab_albert-tiny-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
CustomModel(
  (pretrained_model): AlbertModel(
    (embeddings): AlbertEmbeddings(
      (word_embeddings): Embedding(21128, 128, padding_idx=0)
      (position_embeddings): Embedding(512, 128)
      (token_type_embeddings): Embedding(2, 128)
      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder): AlbertTransformer(
      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=312, bias=True)
      (albert_layer_groups): ModuleList(
        (0): AlbertLayerGroup(
          (albert_layers): ModuleList(
            (0): AlbertLayer(
              (full_layer_layer_norm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
              (attention): AlbertAttention(
                (query): Linear(in_features=312, out_features=312, bias=True)
                (key): Linear(in_features=312, out_features=312, bias=True)
                (value): Linear(in_features=312, out_features=312, bias=True)
                (attention_dropout): Dropout(p=0.0, inplace=False)
                (output_dropout): Dropout(p=0.0, inplace=False)
                (dense): Linear(in_features=312, out_features=312, bias=True)
                (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
              )
              (ffn): Linear(in_features=312, out_features=1248, bias=True)
              (ffn_output): Linear(in_features=1248, out_features=312, bias=True)
              (activation): GELUActivation()
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (pooler): Linear(in_features=312, out_features=312, bias=True)
    (pooler_activation): Tanh()
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=312, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.embedding_hidden_mapping_in.weight False
pretrained_model.encoder.embedding_hidden_mapping_in.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias False
pretrained_model.pooler.weight False
pretrained_model.pooler.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 7s, train loss=2.6007, train acc=12.57%, dev loss=2.4707, dev acc=20.01%
saving, test loss=2.4781, test acc=18.80%
epoch: 2/10000, 7s, train loss=2.4484, train acc=20.08%, dev loss=2.3586, dev acc=25.22%
saving, test loss=2.3689, test acc=24.23%
epoch: 3/10000, 7s, train loss=2.3519, train acc=24.55%, dev loss=2.2750, dev acc=28.15%
saving, test loss=2.2878, test acc=27.66%
epoch: 4/10000, 7s, train loss=2.2828, train acc=27.37%, dev loss=2.2110, dev acc=30.81%
saving, test loss=2.2254, test acc=30.17%
epoch: 5/10000, 7s, train loss=2.2310, train acc=28.71%, dev loss=2.1610, dev acc=32.01%
saving, test loss=2.1766, test acc=31.87%
epoch: 6/10000, 7s, train loss=2.1921, train acc=30.26%, dev loss=2.1206, dev acc=33.58%
saving, test loss=2.1372, test acc=33.24%
epoch: 7/10000, 7s, train loss=2.1652, train acc=30.90%, dev loss=2.0883, dev acc=34.84%
saving, test loss=2.1060, test acc=34.31%
epoch: 8/10000, 7s, train loss=2.1391, train acc=31.70%, dev loss=2.0616, dev acc=35.98%
saving, test loss=2.0799, test acc=35.03%
epoch: 9/10000, 7s, train loss=2.1207, train acc=31.91%, dev loss=2.0393, dev acc=36.66%
saving, test loss=2.0583, test acc=35.57%
epoch: 10/10000, 7s, train loss=2.1020, train acc=32.28%, dev loss=2.0203, dev acc=37.16%
saving, test loss=2.0397, test acc=36.16%
epoch: 11/10000, 7s, train loss=2.0914, train acc=32.77%, dev loss=2.0038, dev acc=37.37%
saving, test loss=2.0235, test acc=36.59%
epoch: 12/10000, 7s, train loss=2.0830, train acc=33.01%, dev loss=1.9900, dev acc=38.01%
saving, test loss=2.0100, test acc=37.02%
epoch: 13/10000, 7s, train loss=2.0744, train acc=33.49%, dev loss=1.9783, dev acc=37.99%
epoch: 14/10000, 7s, train loss=2.0664, train acc=33.51%, dev loss=1.9680, dev acc=38.59%
saving, test loss=1.9887, test acc=37.57%
epoch: 15/10000, 7s, train loss=2.0650, train acc=33.29%, dev loss=1.9592, dev acc=38.42%
epoch: 16/10000, 7s, train loss=2.0529, train acc=33.96%, dev loss=1.9512, dev acc=38.40%
epoch: 17/10000, 7s, train loss=2.0499, train acc=33.91%, dev loss=1.9434, dev acc=38.64%
saving, test loss=1.9646, test acc=38.03%
epoch: 18/10000, 7s, train loss=2.0441, train acc=34.04%, dev loss=1.9371, dev acc=38.72%
saving, test loss=1.9584, test acc=38.14%
epoch: 19/10000, 7s, train loss=2.0406, train acc=34.11%, dev loss=1.9310, dev acc=38.91%
saving, test loss=1.9526, test acc=38.23%
epoch: 20/10000, 7s, train loss=2.0389, train acc=33.95%, dev loss=1.9256, dev acc=38.98%
saving, test loss=1.9478, test acc=38.39%
epoch: 21/10000, 7s, train loss=2.0399, train acc=34.07%, dev loss=1.9208, dev acc=39.13%
saving, test loss=1.9431, test acc=38.63%
epoch: 22/10000, 7s, train loss=2.0346, train acc=34.11%, dev loss=1.9163, dev acc=39.41%
saving, test loss=1.9390, test acc=38.59%
epoch: 23/10000, 7s, train loss=2.0321, train acc=34.25%, dev loss=1.9127, dev acc=39.45%
saving, test loss=1.9357, test acc=38.63%
epoch: 24/10000, 7s, train loss=2.0294, train acc=34.38%, dev loss=1.9094, dev acc=39.66%
saving, test loss=1.9324, test acc=38.69%
epoch: 25/10000, 7s, train loss=2.0268, train acc=34.19%, dev loss=1.9057, dev acc=39.81%
saving, test loss=1.9288, test acc=38.88%
epoch: 26/10000, 7s, train loss=2.0241, train acc=34.38%, dev loss=1.9027, dev acc=39.99%
saving, test loss=1.9263, test acc=38.82%
epoch: 27/10000, 7s, train loss=2.0228, train acc=34.58%, dev loss=1.8995, dev acc=39.92%
epoch: 28/10000, 7s, train loss=2.0203, train acc=34.47%, dev loss=1.8973, dev acc=39.73%
epoch: 29/10000, 7s, train loss=2.0188, train acc=34.46%, dev loss=1.8947, dev acc=39.96%
epoch: 30/10000, 7s, train loss=2.0257, train acc=34.29%, dev loss=1.8926, dev acc=39.97%
epoch: 31/10000, 7s, train loss=2.0164, train acc=34.81%, dev loss=1.8901, dev acc=40.03%
saving, test loss=1.9143, test acc=39.01%
epoch: 32/10000, 7s, train loss=2.0193, train acc=34.59%, dev loss=1.8884, dev acc=40.12%
saving, test loss=1.9129, test acc=39.04%
epoch: 33/10000, 7s, train loss=2.0136, train acc=34.56%, dev loss=1.8861, dev acc=40.18%
saving, test loss=1.9105, test acc=39.02%
epoch: 34/10000, 7s, train loss=2.0164, train acc=34.83%, dev loss=1.8845, dev acc=40.24%
saving, test loss=1.9090, test acc=39.12%
epoch: 35/10000, 7s, train loss=2.0123, train acc=34.82%, dev loss=1.8831, dev acc=40.16%
epoch: 36/10000, 7s, train loss=2.0150, train acc=34.44%, dev loss=1.8818, dev acc=40.27%
saving, test loss=1.9063, test acc=39.09%
epoch: 37/10000, 7s, train loss=2.0130, train acc=34.76%, dev loss=1.8803, dev acc=40.16%
epoch: 38/10000, 7s, train loss=2.0145, train acc=34.55%, dev loss=1.8793, dev acc=40.09%
epoch: 39/10000, 7s, train loss=2.0100, train acc=34.79%, dev loss=1.8781, dev acc=40.39%
saving, test loss=1.9031, test acc=39.22%
epoch: 40/10000, 7s, train loss=2.0101, train acc=34.84%, dev loss=1.8769, dev acc=40.14%
epoch: 41/10000, 7s, train loss=2.0099, train acc=34.80%, dev loss=1.8759, dev acc=40.07%
epoch: 42/10000, 7s, train loss=2.0082, train acc=34.77%, dev loss=1.8749, dev acc=40.35%
epoch: 43/10000, 7s, train loss=2.0125, train acc=34.81%, dev loss=1.8742, dev acc=40.42%
saving, test loss=1.8992, test acc=39.29%
epoch: 44/10000, 7s, train loss=2.0138, train acc=34.88%, dev loss=1.8729, dev acc=40.42%
epoch: 45/10000, 7s, train loss=2.0073, train acc=34.91%, dev loss=1.8721, dev acc=40.50%
saving, test loss=1.8972, test acc=39.43%
epoch: 46/10000, 7s, train loss=2.0043, train acc=34.91%, dev loss=1.8712, dev acc=40.31%
epoch: 47/10000, 7s, train loss=2.0101, train acc=34.90%, dev loss=1.8702, dev acc=40.48%
epoch: 48/10000, 7s, train loss=2.0036, train acc=34.96%, dev loss=1.8697, dev acc=40.37%
epoch: 49/10000, 7s, train loss=2.0052, train acc=34.95%, dev loss=1.8687, dev acc=40.50%
epoch: 50/10000, 7s, train loss=2.0057, train acc=34.78%, dev loss=1.8676, dev acc=40.55%
saving, test loss=1.8932, test acc=39.35%
epoch: 51/10000, 7s, train loss=2.0111, train acc=34.80%, dev loss=1.8673, dev acc=40.74%
saving, test loss=1.8926, test acc=39.48%
epoch: 52/10000, 7s, train loss=2.0063, train acc=34.86%, dev loss=1.8669, dev acc=40.57%
epoch: 53/10000, 7s, train loss=2.0087, train acc=34.70%, dev loss=1.8662, dev acc=40.52%
epoch: 54/10000, 7s, train loss=2.0076, train acc=34.96%, dev loss=1.8661, dev acc=40.61%
epoch: 55/10000, 7s, train loss=2.0067, train acc=35.05%, dev loss=1.8658, dev acc=40.52%
epoch: 56/10000, 7s, train loss=2.0070, train acc=34.96%, dev loss=1.8654, dev acc=40.57%
time used=514.6s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='0', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/ckiplab_bert-base-chinese', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_house': 0, 'news_sports': 1, 'news_finance': 2, 'news_culture': 3, 'news_world': 4, 'news_military': 5, 'news_stock': 6, 'news_edu': 7, 'news_story': 8, 'news_tech': 9, 'news_car': 10, 'news_entertainment': 11, 'news_game': 12, 'news_travel': 13, 'news_agriculture': 14}
index_labels_dict={0: 'news_house', 1: 'news_sports', 2: 'news_finance', 3: 'news_culture', 4: 'news_world', 5: 'news_military', 6: 'news_stock', 7: 'news_edu', 8: 'news_story', 9: 'news_tech', 10: 'news_car', 11: 'news_entertainment', 12: 'news_game', 13: 'news_travel', 14: 'news_agriculture'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/ckiplab_bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/ckiplab_bert-base-chinese and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
CustomModel(
  (pretrained_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 69s, train loss=2.3828, train acc=25.77%, dev loss=2.0330, dev acc=47.34%
saving, test loss=2.0398, test acc=47.05%
epoch: 2/10000, 69s, train loss=1.9215, train acc=45.67%, dev loss=1.7031, dev acc=51.33%
saving, test loss=1.7118, test acc=50.90%
epoch: 3/10000, 69s, train loss=1.6957, train acc=49.74%, dev loss=1.5466, dev acc=52.34%
saving, test loss=1.5565, test acc=52.49%
epoch: 4/10000, 69s, train loss=1.5934, train acc=50.43%, dev loss=1.4704, dev acc=52.75%
saving, test loss=1.4808, test acc=53.13%
epoch: 5/10000, 69s, train loss=1.5399, train acc=51.01%, dev loss=1.4280, dev acc=53.37%
saving, test loss=1.4392, test acc=53.40%
epoch: 6/10000, 69s, train loss=1.5067, train acc=51.57%, dev loss=1.4023, dev acc=53.54%
saving, test loss=1.4146, test acc=53.53%
epoch: 7/10000, 70s, train loss=1.4883, train acc=51.49%, dev loss=1.3834, dev acc=53.54%
epoch: 8/10000, 70s, train loss=1.4714, train acc=52.01%, dev loss=1.3713, dev acc=53.56%
saving, test loss=1.3847, test acc=53.94%
epoch: 9/10000, 70s, train loss=1.4623, train acc=51.69%, dev loss=1.3621, dev acc=53.64%
saving, test loss=1.3759, test acc=54.04%
epoch: 10/10000, 70s, train loss=1.4514, train acc=52.14%, dev loss=1.3557, dev acc=53.75%
saving, test loss=1.3702, test acc=53.90%
epoch: 11/10000, 70s, train loss=1.4466, train acc=52.08%, dev loss=1.3497, dev acc=53.71%
epoch: 12/10000, 70s, train loss=1.4458, train acc=52.21%, dev loss=1.3457, dev acc=53.60%
epoch: 13/10000, 70s, train loss=1.4389, train acc=52.20%, dev loss=1.3421, dev acc=53.60%
epoch: 14/10000, 70s, train loss=1.4368, train acc=52.19%, dev loss=1.3386, dev acc=53.79%
saving, test loss=1.3530, test acc=53.84%
epoch: 15/10000, 70s, train loss=1.4365, train acc=52.34%, dev loss=1.3359, dev acc=53.94%
saving, test loss=1.3504, test acc=53.97%
epoch: 16/10000, 70s, train loss=1.4284, train acc=52.53%, dev loss=1.3332, dev acc=53.71%
epoch: 17/10000, 70s, train loss=1.4287, train acc=52.31%, dev loss=1.3308, dev acc=53.80%
epoch: 18/10000, 70s, train loss=1.4283, train acc=52.18%, dev loss=1.3296, dev acc=53.84%
epoch: 19/10000, 70s, train loss=1.4295, train acc=52.27%, dev loss=1.3283, dev acc=53.97%
saving, test loss=1.3435, test acc=53.99%
epoch: 20/10000, 70s, train loss=1.4237, train acc=52.62%, dev loss=1.3267, dev acc=54.07%
saving, test loss=1.3419, test acc=53.96%
epoch: 21/10000, 70s, train loss=1.4236, train acc=52.40%, dev loss=1.3254, dev acc=54.05%
epoch: 22/10000, 70s, train loss=1.4189, train acc=52.60%, dev loss=1.3245, dev acc=54.03%
epoch: 23/10000, 70s, train loss=1.4243, train acc=52.19%, dev loss=1.3237, dev acc=53.77%
epoch: 24/10000, 70s, train loss=1.4186, train acc=52.62%, dev loss=1.3230, dev acc=53.88%
epoch: 25/10000, 70s, train loss=1.4224, train acc=52.42%, dev loss=1.3221, dev acc=53.95%
time used=2141.6s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='0', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_albert_chinese_small', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_tech': 0, 'news_story': 1, 'news_game': 2, 'news_agriculture': 3, 'news_finance': 4, 'news_car': 5, 'news_house': 6, 'news_edu': 7, 'news_military': 8, 'news_culture': 9, 'news_world': 10, 'news_travel': 11, 'news_sports': 12, 'news_entertainment': 13, 'news_stock': 14}
index_labels_dict={0: 'news_tech', 1: 'news_story', 2: 'news_game', 3: 'news_agriculture', 4: 'news_finance', 5: 'news_car', 6: 'news_house', 7: 'news_edu', 8: 'news_military', 9: 'news_culture', 10: 'news_world', 11: 'news_travel', 12: 'news_sports', 13: 'news_entertainment', 14: 'news_stock'}
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'AlbertTokenizer'. 
The class this function is called from is 'BertTokenizer'.
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_albert_chinese_small were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): AlbertModel(
    (embeddings): AlbertEmbeddings(
      (word_embeddings): Embedding(21128, 128, padding_idx=0)
      (position_embeddings): Embedding(512, 128)
      (token_type_embeddings): Embedding(2, 128)
      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder): AlbertTransformer(
      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=384, bias=True)
      (albert_layer_groups): ModuleList(
        (0): AlbertLayerGroup(
          (albert_layers): ModuleList(
            (0): AlbertLayer(
              (full_layer_layer_norm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (attention): AlbertAttention(
                (query): Linear(in_features=384, out_features=384, bias=True)
                (key): Linear(in_features=384, out_features=384, bias=True)
                (value): Linear(in_features=384, out_features=384, bias=True)
                (attention_dropout): Dropout(p=0.0, inplace=False)
                (output_dropout): Dropout(p=0.0, inplace=False)
                (dense): Linear(in_features=384, out_features=384, bias=True)
                (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              )
              (ffn): Linear(in_features=384, out_features=1536, bias=True)
              (ffn_output): Linear(in_features=1536, out_features=384, bias=True)
              (activation): GELUActivation()
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (pooler): Linear(in_features=384, out_features=384, bias=True)
    (pooler_activation): Tanh()
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=384, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.embedding_hidden_mapping_in.weight False
pretrained_model.encoder.embedding_hidden_mapping_in.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias False
pretrained_model.pooler.weight False
pretrained_model.pooler.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 12s, train loss=2.5437, train acc=17.06%, dev loss=2.3577, dev acc=27.14%
saving, test loss=2.3639, test acc=27.59%
epoch: 2/10000, 12s, train loss=2.2767, train acc=31.05%, dev loss=2.1538, dev acc=36.02%
saving, test loss=2.1616, test acc=36.66%
epoch: 3/10000, 12s, train loss=2.1099, train acc=36.72%, dev loss=2.0135, dev acc=39.79%
saving, test loss=2.0221, test acc=41.35%
epoch: 4/10000, 12s, train loss=2.0016, train acc=39.25%, dev loss=1.9159, dev acc=42.04%
saving, test loss=1.9251, test acc=42.84%
epoch: 5/10000, 12s, train loss=1.9228, train acc=41.45%, dev loss=1.8461, dev acc=43.67%
saving, test loss=1.8559, test acc=43.95%
epoch: 6/10000, 12s, train loss=1.8714, train acc=42.16%, dev loss=1.7952, dev acc=44.04%
saving, test loss=1.8056, test acc=45.02%
epoch: 7/10000, 12s, train loss=1.8389, train acc=42.48%, dev loss=1.7579, dev acc=44.81%
saving, test loss=1.7687, test acc=45.53%
epoch: 8/10000, 12s, train loss=1.8030, train acc=43.24%, dev loss=1.7289, dev acc=45.07%
saving, test loss=1.7399, test acc=45.83%
epoch: 9/10000, 12s, train loss=1.7865, train acc=43.38%, dev loss=1.7066, dev acc=45.41%
saving, test loss=1.7177, test acc=46.26%
epoch: 10/10000, 12s, train loss=1.7697, train acc=43.62%, dev loss=1.6882, dev acc=45.61%
saving, test loss=1.6999, test acc=46.52%
epoch: 11/10000, 12s, train loss=1.7572, train acc=43.83%, dev loss=1.6737, dev acc=45.93%
saving, test loss=1.6860, test acc=46.61%
epoch: 12/10000, 12s, train loss=1.7463, train acc=44.00%, dev loss=1.6613, dev acc=46.06%
saving, test loss=1.6738, test acc=46.88%
epoch: 13/10000, 12s, train loss=1.7372, train acc=44.29%, dev loss=1.6513, dev acc=46.27%
saving, test loss=1.6645, test acc=47.17%
epoch: 14/10000, 12s, train loss=1.7311, train acc=44.30%, dev loss=1.6427, dev acc=46.46%
saving, test loss=1.6561, test acc=47.39%
epoch: 15/10000, 12s, train loss=1.7305, train acc=44.21%, dev loss=1.6356, dev acc=46.68%
saving, test loss=1.6495, test acc=47.35%
epoch: 16/10000, 12s, train loss=1.7192, train acc=44.69%, dev loss=1.6290, dev acc=46.63%
epoch: 17/10000, 12s, train loss=1.7165, train acc=44.65%, dev loss=1.6232, dev acc=46.74%
saving, test loss=1.6378, test acc=47.56%
epoch: 18/10000, 12s, train loss=1.7155, train acc=44.64%, dev loss=1.6185, dev acc=46.83%
saving, test loss=1.6335, test acc=47.60%
epoch: 19/10000, 12s, train loss=1.7095, train acc=45.03%, dev loss=1.6145, dev acc=46.85%
saving, test loss=1.6295, test acc=47.56%
epoch: 20/10000, 12s, train loss=1.7083, train acc=44.89%, dev loss=1.6103, dev acc=46.89%
saving, test loss=1.6258, test acc=47.62%
epoch: 21/10000, 12s, train loss=1.7036, train acc=44.92%, dev loss=1.6070, dev acc=46.96%
saving, test loss=1.6229, test acc=47.86%
epoch: 22/10000, 12s, train loss=1.7044, train acc=44.72%, dev loss=1.6036, dev acc=46.98%
saving, test loss=1.6195, test acc=47.80%
epoch: 23/10000, 12s, train loss=1.7031, train acc=44.96%, dev loss=1.6013, dev acc=46.93%
epoch: 24/10000, 12s, train loss=1.6982, train acc=45.19%, dev loss=1.5989, dev acc=46.93%
epoch: 25/10000, 12s, train loss=1.7024, train acc=44.95%, dev loss=1.5965, dev acc=46.89%
epoch: 26/10000, 12s, train loss=1.6970, train acc=44.90%, dev loss=1.5944, dev acc=47.02%
saving, test loss=1.6116, test acc=47.79%
epoch: 27/10000, 12s, train loss=1.6953, train acc=45.09%, dev loss=1.5926, dev acc=46.96%
epoch: 28/10000, 12s, train loss=1.6967, train acc=45.13%, dev loss=1.5906, dev acc=47.10%
saving, test loss=1.6082, test acc=47.89%
epoch: 29/10000, 12s, train loss=1.6906, train acc=45.27%, dev loss=1.5892, dev acc=47.11%
saving, test loss=1.6069, test acc=47.88%
epoch: 30/10000, 12s, train loss=1.6934, train acc=45.08%, dev loss=1.5875, dev acc=47.00%
epoch: 31/10000, 12s, train loss=1.6930, train acc=45.14%, dev loss=1.5864, dev acc=47.06%
epoch: 32/10000, 12s, train loss=1.6856, train acc=45.38%, dev loss=1.5854, dev acc=47.23%
saving, test loss=1.6029, test acc=47.93%
epoch: 33/10000, 12s, train loss=1.6906, train acc=45.26%, dev loss=1.5848, dev acc=47.25%
saving, test loss=1.6020, test acc=48.06%
epoch: 34/10000, 12s, train loss=1.6918, train acc=45.30%, dev loss=1.5834, dev acc=47.25%
epoch: 35/10000, 12s, train loss=1.6862, train acc=45.39%, dev loss=1.5823, dev acc=47.25%
epoch: 36/10000, 12s, train loss=1.6859, train acc=45.63%, dev loss=1.5814, dev acc=47.38%
saving, test loss=1.5988, test acc=48.07%
epoch: 37/10000, 12s, train loss=1.6898, train acc=45.37%, dev loss=1.5805, dev acc=47.21%
epoch: 38/10000, 12s, train loss=1.6894, train acc=45.14%, dev loss=1.5804, dev acc=47.25%
epoch: 39/10000, 12s, train loss=1.6887, train acc=45.29%, dev loss=1.5794, dev acc=47.25%
epoch: 40/10000, 12s, train loss=1.6844, train acc=45.50%, dev loss=1.5790, dev acc=47.25%
epoch: 41/10000, 12s, train loss=1.6864, train acc=45.16%, dev loss=1.5785, dev acc=47.34%
time used=692.0s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='0', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_albert_chinese_tiny', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_world': 0, 'news_culture': 1, 'news_agriculture': 2, 'news_edu': 3, 'news_finance': 4, 'news_stock': 5, 'news_story': 6, 'news_car': 7, 'news_sports': 8, 'news_military': 9, 'news_house': 10, 'news_travel': 11, 'news_game': 12, 'news_tech': 13, 'news_entertainment': 14}
index_labels_dict={0: 'news_world', 1: 'news_culture', 2: 'news_agriculture', 3: 'news_edu', 4: 'news_finance', 5: 'news_stock', 6: 'news_story', 7: 'news_car', 8: 'news_sports', 9: 'news_military', 10: 'news_house', 11: 'news_travel', 12: 'news_game', 13: 'news_tech', 14: 'news_entertainment'}
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'AlbertTokenizer'. 
The class this function is called from is 'BertTokenizer'.
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_albert_chinese_tiny were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): AlbertModel(
    (embeddings): AlbertEmbeddings(
      (word_embeddings): Embedding(21128, 128, padding_idx=0)
      (position_embeddings): Embedding(512, 128)
      (token_type_embeddings): Embedding(2, 128)
      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder): AlbertTransformer(
      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=312, bias=True)
      (albert_layer_groups): ModuleList(
        (0): AlbertLayerGroup(
          (albert_layers): ModuleList(
            (0): AlbertLayer(
              (full_layer_layer_norm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
              (attention): AlbertAttention(
                (query): Linear(in_features=312, out_features=312, bias=True)
                (key): Linear(in_features=312, out_features=312, bias=True)
                (value): Linear(in_features=312, out_features=312, bias=True)
                (attention_dropout): Dropout(p=0.0, inplace=False)
                (output_dropout): Dropout(p=0.0, inplace=False)
                (dense): Linear(in_features=312, out_features=312, bias=True)
                (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
              )
              (ffn): Linear(in_features=312, out_features=1248, bias=True)
              (ffn_output): Linear(in_features=1248, out_features=312, bias=True)
              (activation): GELUActivation()
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (pooler): Linear(in_features=312, out_features=312, bias=True)
    (pooler_activation): Tanh()
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=312, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.embedding_hidden_mapping_in.weight False
pretrained_model.encoder.embedding_hidden_mapping_in.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight False
pretrained_model.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias False
pretrained_model.pooler.weight False
pretrained_model.pooler.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 7s, train loss=2.5393, train acc=15.67%, dev loss=2.3574, dev acc=28.04%
saving, test loss=2.3643, test acc=27.68%
epoch: 2/10000, 7s, train loss=2.2891, train acc=28.90%, dev loss=2.1579, dev acc=36.51%
saving, test loss=2.1675, test acc=36.58%
epoch: 3/10000, 7s, train loss=2.1296, train acc=34.68%, dev loss=2.0231, dev acc=39.60%
saving, test loss=2.0351, test acc=40.06%
epoch: 4/10000, 7s, train loss=2.0271, train acc=37.46%, dev loss=1.9313, dev acc=41.72%
saving, test loss=1.9454, test acc=41.98%
epoch: 5/10000, 7s, train loss=1.9600, train acc=39.05%, dev loss=1.8674, dev acc=42.92%
saving, test loss=1.8826, test acc=43.11%
epoch: 6/10000, 7s, train loss=1.9139, train acc=40.10%, dev loss=1.8215, dev acc=43.38%
saving, test loss=1.8378, test acc=43.72%
epoch: 7/10000, 7s, train loss=1.8785, train acc=40.64%, dev loss=1.7874, dev acc=43.72%
saving, test loss=1.8048, test acc=44.09%
epoch: 8/10000, 7s, train loss=1.8530, train acc=41.25%, dev loss=1.7613, dev acc=44.17%
saving, test loss=1.7793, test acc=44.46%
epoch: 9/10000, 7s, train loss=1.8373, train acc=41.51%, dev loss=1.7415, dev acc=44.58%
saving, test loss=1.7605, test acc=44.70%
epoch: 10/10000, 7s, train loss=1.8230, train acc=41.79%, dev loss=1.7256, dev acc=44.81%
saving, test loss=1.7453, test acc=44.90%
epoch: 11/10000, 7s, train loss=1.8126, train acc=41.92%, dev loss=1.7127, dev acc=44.83%
saving, test loss=1.7325, test acc=45.20%
epoch: 12/10000, 7s, train loss=1.8042, train acc=42.15%, dev loss=1.7017, dev acc=44.92%
saving, test loss=1.7219, test acc=45.49%
epoch: 13/10000, 7s, train loss=1.7965, train acc=42.42%, dev loss=1.6931, dev acc=44.98%
saving, test loss=1.7136, test acc=45.49%
epoch: 14/10000, 7s, train loss=1.7875, train acc=42.62%, dev loss=1.6858, dev acc=45.09%
saving, test loss=1.7067, test acc=45.58%
epoch: 15/10000, 7s, train loss=1.7870, train acc=42.67%, dev loss=1.6795, dev acc=45.00%
epoch: 16/10000, 7s, train loss=1.7811, train acc=42.54%, dev loss=1.6740, dev acc=45.18%
saving, test loss=1.6952, test acc=45.90%
epoch: 17/10000, 7s, train loss=1.7754, train acc=42.76%, dev loss=1.6693, dev acc=45.09%
epoch: 18/10000, 7s, train loss=1.7716, train acc=42.95%, dev loss=1.6648, dev acc=45.22%
saving, test loss=1.6861, test acc=45.96%
epoch: 19/10000, 7s, train loss=1.7716, train acc=43.02%, dev loss=1.6610, dev acc=45.45%
saving, test loss=1.6824, test acc=45.99%
epoch: 20/10000, 7s, train loss=1.7671, train acc=43.11%, dev loss=1.6579, dev acc=45.65%
saving, test loss=1.6795, test acc=46.22%
epoch: 21/10000, 7s, train loss=1.7758, train acc=42.81%, dev loss=1.6552, dev acc=45.80%
saving, test loss=1.6769, test acc=46.34%
epoch: 22/10000, 7s, train loss=1.7653, train acc=43.26%, dev loss=1.6527, dev acc=45.65%
epoch: 23/10000, 7s, train loss=1.7669, train acc=42.85%, dev loss=1.6510, dev acc=45.73%
epoch: 24/10000, 7s, train loss=1.7640, train acc=42.94%, dev loss=1.6490, dev acc=45.75%
epoch: 25/10000, 7s, train loss=1.7617, train acc=43.00%, dev loss=1.6465, dev acc=45.80%
epoch: 26/10000, 7s, train loss=1.7614, train acc=43.26%, dev loss=1.6452, dev acc=45.97%
saving, test loss=1.6674, test acc=46.61%
epoch: 27/10000, 7s, train loss=1.7625, train acc=43.07%, dev loss=1.6436, dev acc=45.82%
epoch: 28/10000, 7s, train loss=1.7590, train acc=43.13%, dev loss=1.6422, dev acc=45.97%
epoch: 29/10000, 7s, train loss=1.7558, train acc=43.31%, dev loss=1.6408, dev acc=46.08%
saving, test loss=1.6633, test acc=46.60%
epoch: 30/10000, 7s, train loss=1.7602, train acc=43.26%, dev loss=1.6394, dev acc=46.08%
epoch: 31/10000, 7s, train loss=1.7607, train acc=42.99%, dev loss=1.6383, dev acc=46.20%
saving, test loss=1.6613, test acc=46.70%
epoch: 32/10000, 7s, train loss=1.7567, train acc=43.11%, dev loss=1.6372, dev acc=46.21%
saving, test loss=1.6602, test acc=46.72%
epoch: 33/10000, 7s, train loss=1.7554, train acc=43.37%, dev loss=1.6362, dev acc=46.14%
epoch: 34/10000, 7s, train loss=1.7575, train acc=43.27%, dev loss=1.6361, dev acc=46.31%
saving, test loss=1.6590, test acc=46.72%
epoch: 35/10000, 7s, train loss=1.7542, train acc=43.36%, dev loss=1.6353, dev acc=46.33%
saving, test loss=1.6579, test acc=46.81%
epoch: 36/10000, 7s, train loss=1.7514, train acc=43.37%, dev loss=1.6348, dev acc=46.21%
epoch: 37/10000, 7s, train loss=1.7564, train acc=43.33%, dev loss=1.6339, dev acc=46.31%
epoch: 38/10000, 7s, train loss=1.7569, train acc=43.20%, dev loss=1.6332, dev acc=46.44%
saving, test loss=1.6559, test acc=46.73%
epoch: 39/10000, 7s, train loss=1.7485, train acc=43.44%, dev loss=1.6327, dev acc=46.31%
epoch: 40/10000, 7s, train loss=1.7560, train acc=43.46%, dev loss=1.6325, dev acc=46.31%
epoch: 41/10000, 7s, train loss=1.7557, train acc=43.46%, dev loss=1.6315, dev acc=46.38%
epoch: 42/10000, 7s, train loss=1.7549, train acc=43.32%, dev loss=1.6311, dev acc=46.48%
saving, test loss=1.6541, test acc=46.90%
epoch: 43/10000, 7s, train loss=1.7560, train acc=43.48%, dev loss=1.6311, dev acc=46.35%
epoch: 44/10000, 7s, train loss=1.7558, train acc=43.33%, dev loss=1.6301, dev acc=46.63%
saving, test loss=1.6531, test acc=47.08%
epoch: 45/10000, 7s, train loss=1.7553, train acc=43.27%, dev loss=1.6304, dev acc=46.36%
epoch: 46/10000, 7s, train loss=1.7504, train acc=43.41%, dev loss=1.6300, dev acc=46.55%
epoch: 47/10000, 7s, train loss=1.7599, train acc=43.46%, dev loss=1.6295, dev acc=46.61%
epoch: 48/10000, 7s, train loss=1.7510, train acc=43.47%, dev loss=1.6293, dev acc=46.44%
epoch: 49/10000, 7s, train loss=1.7502, train acc=43.54%, dev loss=1.6290, dev acc=46.61%
time used=478.7s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='0', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_3L312_clue_tiny', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_travel': 0, 'news_story': 1, 'news_agriculture': 2, 'news_game': 3, 'news_edu': 4, 'news_sports': 5, 'news_house': 6, 'news_car': 7, 'news_entertainment': 8, 'news_finance': 9, 'news_military': 10, 'news_world': 11, 'news_tech': 12, 'news_culture': 13, 'news_stock': 14}
index_labels_dict={0: 'news_travel', 1: 'news_story', 2: 'news_agriculture', 3: 'news_game', 4: 'news_edu', 5: 'news_sports', 6: 'news_house', 7: 'news_car', 8: 'news_entertainment', 9: 'news_finance', 10: 'news_military', 11: 'news_world', 12: 'news_tech', 13: 'news_culture', 14: 'news_stock'}
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. 
The class this function is called from is 'BertTokenizer'.
max_sent_len=147
147	1
60	1
56	2
55	3
54	1
53	1
52	5
51	6
50	9
49	13
48	6
47	10
46	66
45	16
44	30
43	46
42	119
41	113
40	119
39	136
38	145
37	209
36	226
35	295
34	383
33	893
32	3585
31	2638
30	2365
29	2220
28	2158
27	2131
26	2255
25	2210
24	2285
23	2255
22	2180
21	2183
20	1999
19	2240
18	1818
17	1934
16	1716
15	1570
14	1422
13	1173
12	932
11	776
10	518
9	317
8	144
7	125
6	16
5	1
4	4
max_sent_len=51
max_sent_len=51
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_3L312_clue_tiny were not used when initializing RobertaModel: ['bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.0.output.dense.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'cls.predictions.decoder.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'cls.predictions.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'cls.predictions.decoder.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.query.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_3L312_clue_tiny and are newly initialized: ['pooler.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'pooler.dense.bias', 'encoder.layer.0.attention.self.query.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
CustomModel(
  (pretrained_model): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(8021, 312, padding_idx=1)
      (position_embeddings): Embedding(512, 312, padding_idx=1)
      (token_type_embeddings): Embedding(2, 312)
      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=312, out_features=312, bias=True)
              (key): Linear(in_features=312, out_features=312, bias=True)
              (value): Linear(in_features=312, out_features=312, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=312, out_features=312, bias=True)
              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=312, out_features=1248, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=1248, out_features=312, bias=True)
            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=312, out_features=312, bias=True)
              (key): Linear(in_features=312, out_features=312, bias=True)
              (value): Linear(in_features=312, out_features=312, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=312, out_features=312, bias=True)
              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=312, out_features=1248, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=1248, out_features=312, bias=True)
            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=312, out_features=312, bias=True)
              (key): Linear(in_features=312, out_features=312, bias=True)
              (value): Linear(in_features=312, out_features=312, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=312, out_features=312, bias=True)
              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=312, out_features=1248, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=1248, out_features=312, bias=True)
            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=312, out_features=312, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=312, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 6s, train loss=2.6709, train acc=9.44%, dev loss=2.5860, dev acc=10.94%
saving, test loss=2.5903, test acc=10.71%
epoch: 2/10000, 6s, train loss=2.6286, train acc=10.21%, dev loss=2.5726, dev acc=12.18%
saving, test loss=2.5767, test acc=11.87%
epoch: 3/10000, 6s, train loss=2.6106, train acc=10.79%, dev loss=2.5644, dev acc=12.61%
saving, test loss=2.5684, test acc=12.41%
epoch: 4/10000, 6s, train loss=2.5946, train acc=11.52%, dev loss=2.5583, dev acc=12.78%
saving, test loss=2.5622, test acc=12.68%
epoch: 5/10000, 6s, train loss=2.5836, train acc=11.99%, dev loss=2.5527, dev acc=13.04%
saving, test loss=2.5567, test acc=13.12%
epoch: 6/10000, 6s, train loss=2.5748, train acc=12.57%, dev loss=2.5475, dev acc=13.85%
saving, test loss=2.5512, test acc=13.57%
epoch: 7/10000, 6s, train loss=2.5670, train acc=12.99%, dev loss=2.5427, dev acc=13.87%
saving, test loss=2.5465, test acc=13.67%
epoch: 8/10000, 6s, train loss=2.5609, train acc=13.36%, dev loss=2.5379, dev acc=14.92%
saving, test loss=2.5414, test acc=14.98%
epoch: 9/10000, 6s, train loss=2.5570, train acc=13.70%, dev loss=2.5335, dev acc=15.24%
saving, test loss=2.5368, test acc=15.00%
epoch: 10/10000, 6s, train loss=2.5519, train acc=13.91%, dev loss=2.5293, dev acc=15.63%
saving, test loss=2.5323, test acc=15.59%
epoch: 11/10000, 6s, train loss=2.5484, train acc=14.20%, dev loss=2.5255, dev acc=16.06%
saving, test loss=2.5285, test acc=16.15%
epoch: 12/10000, 6s, train loss=2.5445, train acc=14.48%, dev loss=2.5217, dev acc=16.38%
saving, test loss=2.5245, test acc=16.45%
epoch: 13/10000, 6s, train loss=2.5421, train acc=14.90%, dev loss=2.5182, dev acc=16.81%
saving, test loss=2.5210, test acc=16.76%
epoch: 14/10000, 6s, train loss=2.5390, train acc=14.93%, dev loss=2.5149, dev acc=17.09%
saving, test loss=2.5177, test acc=16.97%
epoch: 15/10000, 6s, train loss=2.5362, train acc=15.12%, dev loss=2.5116, dev acc=17.34%
saving, test loss=2.5142, test acc=17.37%
epoch: 16/10000, 6s, train loss=2.5347, train acc=15.15%, dev loss=2.5085, dev acc=17.50%
saving, test loss=2.5110, test acc=17.63%
epoch: 17/10000, 6s, train loss=2.5343, train acc=15.09%, dev loss=2.5059, dev acc=17.63%
saving, test loss=2.5083, test acc=17.62%
epoch: 18/10000, 6s, train loss=2.5315, train acc=15.23%, dev loss=2.5031, dev acc=17.73%
saving, test loss=2.5053, test acc=17.93%
epoch: 19/10000, 6s, train loss=2.5266, train acc=15.89%, dev loss=2.5004, dev acc=18.40%
saving, test loss=2.5023, test acc=18.09%
epoch: 20/10000, 6s, train loss=2.5276, train acc=15.71%, dev loss=2.4979, dev acc=18.33%
epoch: 21/10000, 6s, train loss=2.5258, train acc=15.65%, dev loss=2.4954, dev acc=18.91%
saving, test loss=2.4974, test acc=18.63%
epoch: 22/10000, 6s, train loss=2.5270, train acc=15.66%, dev loss=2.4932, dev acc=18.89%
epoch: 23/10000, 6s, train loss=2.5232, train acc=15.67%, dev loss=2.4910, dev acc=19.12%
saving, test loss=2.4930, test acc=18.58%
epoch: 24/10000, 6s, train loss=2.5217, train acc=16.12%, dev loss=2.4888, dev acc=19.47%
saving, test loss=2.4906, test acc=19.24%
epoch: 25/10000, 6s, train loss=2.5211, train acc=15.82%, dev loss=2.4869, dev acc=19.64%
saving, test loss=2.4887, test acc=19.12%
epoch: 26/10000, 6s, train loss=2.5181, train acc=16.23%, dev loss=2.4847, dev acc=19.77%
saving, test loss=2.4863, test acc=19.49%
epoch: 27/10000, 6s, train loss=2.5188, train acc=16.00%, dev loss=2.4829, dev acc=19.94%
saving, test loss=2.4847, test acc=19.45%
epoch: 28/10000, 6s, train loss=2.5169, train acc=16.21%, dev loss=2.4809, dev acc=19.96%
saving, test loss=2.4825, test acc=19.54%
epoch: 29/10000, 6s, train loss=2.5171, train acc=16.19%, dev loss=2.4794, dev acc=20.07%
saving, test loss=2.4811, test acc=19.57%
epoch: 30/10000, 6s, train loss=2.5155, train acc=16.25%, dev loss=2.4778, dev acc=20.18%
saving, test loss=2.4796, test acc=19.67%
epoch: 31/10000, 6s, train loss=2.5157, train acc=16.12%, dev loss=2.4763, dev acc=20.31%
saving, test loss=2.4780, test acc=19.86%
epoch: 32/10000, 6s, train loss=2.5158, train acc=16.09%, dev loss=2.4748, dev acc=20.54%
saving, test loss=2.4765, test acc=19.86%
epoch: 33/10000, 6s, train loss=2.5139, train acc=16.21%, dev loss=2.4734, dev acc=20.54%
epoch: 34/10000, 6s, train loss=2.5128, train acc=16.19%, dev loss=2.4719, dev acc=20.52%
epoch: 35/10000, 6s, train loss=2.5116, train acc=16.46%, dev loss=2.4705, dev acc=20.76%
saving, test loss=2.4722, test acc=20.19%
epoch: 36/10000, 6s, train loss=2.5115, train acc=16.16%, dev loss=2.4691, dev acc=20.90%
saving, test loss=2.4705, test acc=20.31%
epoch: 37/10000, 6s, train loss=2.5130, train acc=16.17%, dev loss=2.4679, dev acc=20.90%
epoch: 38/10000, 6s, train loss=2.5110, train acc=16.37%, dev loss=2.4668, dev acc=20.75%
epoch: 39/10000, 6s, train loss=2.5092, train acc=16.37%, dev loss=2.4655, dev acc=20.82%
epoch: 40/10000, 6s, train loss=2.5096, train acc=16.40%, dev loss=2.4644, dev acc=20.95%
saving, test loss=2.4656, test acc=20.46%
epoch: 41/10000, 6s, train loss=2.5111, train acc=16.10%, dev loss=2.4633, dev acc=21.08%
saving, test loss=2.4646, test acc=20.68%
epoch: 42/10000, 6s, train loss=2.5103, train acc=16.35%, dev loss=2.4623, dev acc=21.16%
saving, test loss=2.4634, test acc=20.70%
epoch: 43/10000, 6s, train loss=2.5081, train acc=16.45%, dev loss=2.4614, dev acc=21.10%
epoch: 44/10000, 6s, train loss=2.5074, train acc=16.69%, dev loss=2.4606, dev acc=21.18%
saving, test loss=2.4618, test acc=20.84%
epoch: 45/10000, 6s, train loss=2.5071, train acc=16.45%, dev loss=2.4595, dev acc=20.97%
epoch: 46/10000, 6s, train loss=2.5073, train acc=16.37%, dev loss=2.4584, dev acc=21.18%
epoch: 47/10000, 6s, train loss=2.5081, train acc=16.28%, dev loss=2.4577, dev acc=21.14%
epoch: 48/10000, 6s, train loss=2.5103, train acc=16.22%, dev loss=2.4570, dev acc=21.33%
saving, test loss=2.4580, test acc=20.89%
epoch: 49/10000, 6s, train loss=2.5081, train acc=16.36%, dev loss=2.4563, dev acc=21.21%
epoch: 50/10000, 6s, train loss=2.5092, train acc=16.22%, dev loss=2.4557, dev acc=21.35%
saving, test loss=2.4565, test acc=21.19%
epoch: 51/10000, 6s, train loss=2.5094, train acc=16.42%, dev loss=2.4551, dev acc=21.33%
epoch: 52/10000, 6s, train loss=2.5050, train acc=16.61%, dev loss=2.4544, dev acc=21.06%
epoch: 53/10000, 6s, train loss=2.5081, train acc=16.38%, dev loss=2.4538, dev acc=21.33%
epoch: 54/10000, 6s, train loss=2.5060, train acc=16.46%, dev loss=2.4531, dev acc=21.25%
epoch: 55/10000, 6s, train loss=2.5075, train acc=16.43%, dev loss=2.4523, dev acc=21.27%
time used=456.9s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='0', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_3L768_clue_tiny', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_military': 0, 'news_agriculture': 1, 'news_finance': 2, 'news_game': 3, 'news_sports': 4, 'news_tech': 5, 'news_car': 6, 'news_culture': 7, 'news_travel': 8, 'news_story': 9, 'news_world': 10, 'news_edu': 11, 'news_stock': 12, 'news_entertainment': 13, 'news_house': 14}
index_labels_dict={0: 'news_military', 1: 'news_agriculture', 2: 'news_finance', 3: 'news_game', 4: 'news_sports', 5: 'news_tech', 6: 'news_car', 7: 'news_culture', 8: 'news_travel', 9: 'news_story', 10: 'news_world', 11: 'news_edu', 12: 'news_stock', 13: 'news_entertainment', 14: 'news_house'}
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. 
The class this function is called from is 'BertTokenizer'.
max_sent_len=147
147	1
60	1
56	2
55	3
54	1
53	1
52	5
51	6
50	9
49	13
48	6
47	10
46	66
45	16
44	30
43	46
42	119
41	113
40	119
39	136
38	145
37	209
36	226
35	295
34	383
33	893
32	3585
31	2638
30	2365
29	2220
28	2158
27	2131
26	2255
25	2210
24	2285
23	2255
22	2180
21	2183
20	1999
19	2240
18	1818
17	1934
16	1716
15	1570
14	1422
13	1173
12	932
11	776
10	518
9	317
8	144
7	125
6	16
5	1
4	4
max_sent_len=51
max_sent_len=51
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_3L768_clue_tiny were not used when initializing RobertaModel: ['cls.seq_relationship.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.embeddings.position_embeddings.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.2.output.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.output.dense.bias', 'cls.predictions.decoder.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.weight', 'cls.predictions.decoder.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'cls.predictions.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_3L768_clue_tiny and are newly initialized: ['encoder.layer.0.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.bias', 'pooler.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
CustomModel(
  (pretrained_model): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(8021, 768, padding_idx=1)
      (position_embeddings): Embedding(512, 768, padding_idx=1)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 18s, train loss=2.6672, train acc=9.70%, dev loss=2.5711, dev acc=13.10%
saving, test loss=2.5768, test acc=13.01%
epoch: 2/10000, 18s, train loss=2.6090, train acc=11.28%, dev loss=2.5431, dev acc=14.15%
saving, test loss=2.5478, test acc=14.27%
epoch: 3/10000, 18s, train loss=2.5775, train acc=12.59%, dev loss=2.5219, dev acc=15.85%
saving, test loss=2.5262, test acc=15.69%
epoch: 4/10000, 18s, train loss=2.5533, train acc=14.09%, dev loss=2.5019, dev acc=17.07%
saving, test loss=2.5059, test acc=17.41%
epoch: 5/10000, 18s, train loss=2.5306, train acc=15.28%, dev loss=2.4843, dev acc=18.23%
saving, test loss=2.4881, test acc=18.41%
epoch: 6/10000, 18s, train loss=2.5137, train acc=16.62%, dev loss=2.4679, dev acc=19.21%
saving, test loss=2.4711, test acc=19.44%
epoch: 7/10000, 18s, train loss=2.5000, train acc=17.22%, dev loss=2.4535, dev acc=20.09%
saving, test loss=2.4562, test acc=20.40%
epoch: 8/10000, 18s, train loss=2.4862, train acc=17.81%, dev loss=2.4397, dev acc=20.86%
saving, test loss=2.4418, test acc=21.24%
epoch: 9/10000, 18s, train loss=2.4755, train acc=18.65%, dev loss=2.4275, dev acc=21.50%
saving, test loss=2.4292, test acc=22.12%
epoch: 10/10000, 18s, train loss=2.4659, train acc=18.97%, dev loss=2.4159, dev acc=21.80%
saving, test loss=2.4173, test acc=22.58%
epoch: 11/10000, 18s, train loss=2.4564, train acc=19.47%, dev loss=2.4052, dev acc=22.26%
saving, test loss=2.4059, test acc=23.21%
epoch: 12/10000, 18s, train loss=2.4524, train acc=19.70%, dev loss=2.3952, dev acc=22.71%
saving, test loss=2.3955, test acc=23.98%
epoch: 13/10000, 18s, train loss=2.4440, train acc=19.98%, dev loss=2.3865, dev acc=23.43%
saving, test loss=2.3864, test acc=24.53%
epoch: 14/10000, 18s, train loss=2.4390, train acc=20.36%, dev loss=2.3784, dev acc=23.74%
saving, test loss=2.3780, test acc=24.83%
epoch: 15/10000, 18s, train loss=2.4330, train acc=20.37%, dev loss=2.3703, dev acc=24.55%
saving, test loss=2.3695, test acc=25.36%
epoch: 16/10000, 18s, train loss=2.4313, train acc=20.60%, dev loss=2.3636, dev acc=24.91%
saving, test loss=2.3625, test acc=25.80%
epoch: 17/10000, 18s, train loss=2.4245, train acc=20.81%, dev loss=2.3576, dev acc=24.94%
saving, test loss=2.3564, test acc=25.80%
epoch: 18/10000, 18s, train loss=2.4239, train acc=20.78%, dev loss=2.3509, dev acc=25.49%
saving, test loss=2.3492, test acc=26.38%
epoch: 19/10000, 18s, train loss=2.4208, train acc=21.04%, dev loss=2.3456, dev acc=25.67%
saving, test loss=2.3436, test acc=26.65%
epoch: 20/10000, 18s, train loss=2.4177, train acc=21.04%, dev loss=2.3408, dev acc=25.75%
saving, test loss=2.3384, test acc=26.82%
epoch: 21/10000, 18s, train loss=2.4182, train acc=20.92%, dev loss=2.3360, dev acc=25.96%
saving, test loss=2.3333, test acc=27.14%
epoch: 22/10000, 18s, train loss=2.4146, train acc=21.44%, dev loss=2.3314, dev acc=25.99%
saving, test loss=2.3286, test acc=27.27%
epoch: 23/10000, 18s, train loss=2.4111, train acc=21.27%, dev loss=2.3278, dev acc=26.11%
saving, test loss=2.3247, test acc=27.34%
epoch: 24/10000, 18s, train loss=2.4064, train acc=21.54%, dev loss=2.3231, dev acc=26.89%
saving, test loss=2.3201, test acc=27.88%
epoch: 25/10000, 18s, train loss=2.4088, train acc=21.08%, dev loss=2.3191, dev acc=26.76%
epoch: 26/10000, 18s, train loss=2.4019, train acc=21.59%, dev loss=2.3160, dev acc=26.87%
epoch: 27/10000, 18s, train loss=2.4014, train acc=21.77%, dev loss=2.3121, dev acc=27.25%
saving, test loss=2.3083, test acc=28.14%
epoch: 28/10000, 18s, train loss=2.4037, train acc=21.69%, dev loss=2.3091, dev acc=27.40%
saving, test loss=2.3054, test acc=28.17%
epoch: 29/10000, 18s, train loss=2.4028, train acc=21.65%, dev loss=2.3063, dev acc=27.34%
epoch: 30/10000, 18s, train loss=2.3985, train acc=22.00%, dev loss=2.3033, dev acc=27.79%
saving, test loss=2.2987, test acc=29.02%
epoch: 31/10000, 18s, train loss=2.3987, train acc=21.86%, dev loss=2.3005, dev acc=27.64%
epoch: 32/10000, 18s, train loss=2.3926, train acc=22.27%, dev loss=2.2981, dev acc=27.66%
epoch: 33/10000, 18s, train loss=2.3910, train acc=22.34%, dev loss=2.2953, dev acc=27.42%
epoch: 34/10000, 18s, train loss=2.3937, train acc=22.06%, dev loss=2.2921, dev acc=27.85%
saving, test loss=2.2875, test acc=29.28%
epoch: 35/10000, 18s, train loss=2.3926, train acc=21.83%, dev loss=2.2907, dev acc=27.90%
saving, test loss=2.2859, test acc=29.40%
epoch: 36/10000, 18s, train loss=2.3954, train acc=21.96%, dev loss=2.2885, dev acc=28.15%
saving, test loss=2.2838, test acc=29.44%
epoch: 37/10000, 18s, train loss=2.3978, train acc=21.86%, dev loss=2.2867, dev acc=28.19%
saving, test loss=2.2814, test acc=29.52%
epoch: 38/10000, 18s, train loss=2.3921, train acc=22.09%, dev loss=2.2854, dev acc=27.92%
epoch: 39/10000, 18s, train loss=2.3933, train acc=22.38%, dev loss=2.2832, dev acc=28.04%
epoch: 40/10000, 18s, train loss=2.3911, train acc=22.12%, dev loss=2.2810, dev acc=28.86%
saving, test loss=2.2757, test acc=29.94%
epoch: 41/10000, 18s, train loss=2.3883, train acc=22.07%, dev loss=2.2810, dev acc=28.30%
epoch: 42/10000, 18s, train loss=2.3904, train acc=22.06%, dev loss=2.2791, dev acc=28.50%
epoch: 43/10000, 18s, train loss=2.3883, train acc=22.12%, dev loss=2.2773, dev acc=28.19%
epoch: 44/10000, 18s, train loss=2.3906, train acc=22.23%, dev loss=2.2763, dev acc=27.94%
epoch: 45/10000, 18s, train loss=2.3860, train acc=22.30%, dev loss=2.2754, dev acc=28.52%
time used=1101.1s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='0', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_base', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_entertainment': 0, 'news_world': 1, 'news_finance': 2, 'news_sports': 3, 'news_stock': 4, 'news_agriculture': 5, 'news_culture': 6, 'news_car': 7, 'news_travel': 8, 'news_tech': 9, 'news_game': 10, 'news_story': 11, 'news_edu': 12, 'news_house': 13, 'news_military': 14}
index_labels_dict={0: 'news_entertainment', 1: 'news_world', 2: 'news_finance', 3: 'news_sports', 4: 'news_stock', 5: 'news_agriculture', 6: 'news_culture', 7: 'news_car', 8: 'news_travel', 9: 'news_tech', 10: 'news_game', 11: 'news_story', 12: 'news_edu', 13: 'news_house', 14: 'news_military'}
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. 
The class this function is called from is 'BertTokenizer'.
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_base were not used when initializing RobertaModel: ['bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'cls.predictions.decoder.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.output.dense.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.output.dense.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'cls.predictions.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.key.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_base and are newly initialized: ['encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.10.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.2.attention.self.key.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.bias', 'pooler.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.0.attention.self.key.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
CustomModel(
  (pretrained_model): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=1)
      (position_embeddings): Embedding(512, 768, padding_idx=1)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 69s, train loss=2.7061, train acc=9.41%, dev loss=2.5738, dev acc=12.71%
saving, test loss=2.5795, test acc=12.77%
epoch: 2/10000, 69s, train loss=2.6227, train acc=11.46%, dev loss=2.5324, dev acc=15.14%
saving, test loss=2.5383, test acc=14.41%
epoch: 3/10000, 69s, train loss=2.5836, train acc=12.92%, dev loss=2.5077, dev acc=16.51%
saving, test loss=2.5139, test acc=15.68%
epoch: 4/10000, 69s, train loss=2.5515, train acc=14.28%, dev loss=2.4850, dev acc=17.84%
saving, test loss=2.4916, test acc=17.30%
epoch: 5/10000, 69s, train loss=2.5261, train acc=15.80%, dev loss=2.4652, dev acc=18.78%
saving, test loss=2.4721, test acc=18.33%
epoch: 6/10000, 69s, train loss=2.5086, train acc=16.45%, dev loss=2.4489, dev acc=19.10%
saving, test loss=2.4558, test acc=18.75%
epoch: 7/10000, 70s, train loss=2.4946, train acc=17.14%, dev loss=2.4341, dev acc=19.98%
saving, test loss=2.4407, test acc=19.36%
epoch: 8/10000, 70s, train loss=2.4815, train acc=18.01%, dev loss=2.4208, dev acc=20.52%
saving, test loss=2.4269, test acc=20.24%
epoch: 9/10000, 70s, train loss=2.4709, train acc=18.58%, dev loss=2.4103, dev acc=21.06%
saving, test loss=2.4165, test acc=20.70%
epoch: 10/10000, 70s, train loss=2.4633, train acc=18.89%, dev loss=2.4001, dev acc=21.53%
saving, test loss=2.4064, test acc=21.34%
epoch: 11/10000, 70s, train loss=2.4592, train acc=19.05%, dev loss=2.3914, dev acc=21.80%
saving, test loss=2.3976, test acc=21.71%
epoch: 12/10000, 70s, train loss=2.4555, train acc=19.27%, dev loss=2.3821, dev acc=22.43%
saving, test loss=2.3879, test acc=22.15%
epoch: 13/10000, 70s, train loss=2.4481, train acc=19.72%, dev loss=2.3766, dev acc=22.79%
saving, test loss=2.3823, test acc=22.19%
epoch: 14/10000, 70s, train loss=2.4431, train acc=19.76%, dev loss=2.3705, dev acc=22.84%
saving, test loss=2.3760, test acc=22.21%
epoch: 15/10000, 70s, train loss=2.4417, train acc=19.70%, dev loss=2.3647, dev acc=23.18%
saving, test loss=2.3703, test acc=22.53%
epoch: 16/10000, 70s, train loss=2.4390, train acc=19.80%, dev loss=2.3587, dev acc=23.63%
saving, test loss=2.3644, test acc=23.18%
epoch: 17/10000, 70s, train loss=2.4365, train acc=20.00%, dev loss=2.3548, dev acc=23.69%
saving, test loss=2.3606, test acc=23.13%
epoch: 18/10000, 70s, train loss=2.4338, train acc=20.28%, dev loss=2.3492, dev acc=24.53%
saving, test loss=2.3543, test acc=23.94%
epoch: 19/10000, 70s, train loss=2.4321, train acc=20.29%, dev loss=2.3461, dev acc=24.59%
saving, test loss=2.3509, test acc=24.05%
epoch: 20/10000, 70s, train loss=2.4295, train acc=20.23%, dev loss=2.3429, dev acc=24.74%
saving, test loss=2.3475, test acc=24.20%
epoch: 21/10000, 70s, train loss=2.4296, train acc=20.24%, dev loss=2.3402, dev acc=24.74%
epoch: 22/10000, 70s, train loss=2.4266, train acc=20.53%, dev loss=2.3371, dev acc=24.96%
saving, test loss=2.3417, test acc=24.61%
epoch: 23/10000, 70s, train loss=2.4297, train acc=20.36%, dev loss=2.3340, dev acc=25.26%
saving, test loss=2.3386, test acc=24.75%
epoch: 24/10000, 70s, train loss=2.4262, train acc=20.64%, dev loss=2.3321, dev acc=25.58%
saving, test loss=2.3365, test acc=24.67%
epoch: 25/10000, 70s, train loss=2.4283, train acc=20.09%, dev loss=2.3313, dev acc=25.13%
epoch: 26/10000, 70s, train loss=2.4259, train acc=20.68%, dev loss=2.3287, dev acc=25.09%
epoch: 27/10000, 70s, train loss=2.4248, train acc=20.46%, dev loss=2.3260, dev acc=25.24%
epoch: 28/10000, 70s, train loss=2.4233, train acc=20.63%, dev loss=2.3237, dev acc=25.64%
saving, test loss=2.3283, test acc=25.02%
epoch: 29/10000, 70s, train loss=2.4229, train acc=20.68%, dev loss=2.3231, dev acc=25.49%
epoch: 30/10000, 70s, train loss=2.4306, train acc=20.40%, dev loss=2.3224, dev acc=25.26%
epoch: 31/10000, 70s, train loss=2.4229, train acc=20.68%, dev loss=2.3197, dev acc=25.86%
saving, test loss=2.3244, test acc=25.13%
epoch: 32/10000, 70s, train loss=2.4221, train acc=20.62%, dev loss=2.3183, dev acc=25.58%
epoch: 33/10000, 70s, train loss=2.4232, train acc=20.46%, dev loss=2.3165, dev acc=25.90%
saving, test loss=2.3212, test acc=25.28%
epoch: 34/10000, 70s, train loss=2.4229, train acc=20.66%, dev loss=2.3154, dev acc=25.69%
epoch: 35/10000, 70s, train loss=2.4209, train acc=20.73%, dev loss=2.3142, dev acc=26.05%
saving, test loss=2.3189, test acc=25.51%
epoch: 36/10000, 70s, train loss=2.4240, train acc=20.92%, dev loss=2.3129, dev acc=25.88%
epoch: 37/10000, 70s, train loss=2.4216, train acc=20.79%, dev loss=2.3130, dev acc=25.58%
epoch: 38/10000, 70s, train loss=2.4209, train acc=20.62%, dev loss=2.3139, dev acc=25.64%
epoch: 39/10000, 70s, train loss=2.4191, train acc=20.61%, dev loss=2.3117, dev acc=25.86%
epoch: 40/10000, 70s, train loss=2.4163, train acc=21.03%, dev loss=2.3096, dev acc=25.62%
time used=3536.3s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='0', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_clue_large', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_entertainment': 0, 'news_edu': 1, 'news_finance': 2, 'news_game': 3, 'news_stock': 4, 'news_military': 5, 'news_agriculture': 6, 'news_culture': 7, 'news_house': 8, 'news_travel': 9, 'news_tech': 10, 'news_world': 11, 'news_sports': 12, 'news_car': 13, 'news_story': 14}
index_labels_dict={0: 'news_entertainment', 1: 'news_edu', 2: 'news_finance', 3: 'news_game', 4: 'news_stock', 5: 'news_military', 6: 'news_agriculture', 7: 'news_culture', 8: 'news_house', 9: 'news_travel', 10: 'news_tech', 11: 'news_world', 12: 'news_sports', 13: 'news_car', 14: 'news_story'}
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. 
The class this function is called from is 'BertTokenizer'.
max_sent_len=147
147	1
60	1
56	2
55	3
54	1
53	1
52	5
51	6
50	9
49	13
48	6
47	10
46	66
45	16
44	30
43	46
42	119
41	113
40	119
39	136
38	145
37	209
36	226
35	295
34	383
33	893
32	3585
31	2638
30	2365
29	2220
28	2158
27	2131
26	2255
25	2210
24	2285
23	2255
22	2180
21	2183
20	1999
19	2240
18	1818
17	1934
16	1716
15	1570
14	1422
13	1173
12	932
11	776
10	518
9	317
8	144
7	125
6	16
5	1
4	4
max_sent_len=51
max_sent_len=51
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_clue_large were not used when initializing RobertaModel: ['bert.encoder.layer.17.attention.self.value.bias', 'bert.encoder.layer.20.attention.output.dense.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.23.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.17.attention.output.dense.bias', 'bert.encoder.layer.13.attention.self.value.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.17.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.19.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.21.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.attention.output.dense.bias', 'cls.predictions.decoder.weight', 'bert.encoder.layer.23.attention.self.value.weight', 'bert.encoder.layer.21.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.20.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.18.attention.self.value.bias', 'bert.encoder.layer.14.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.14.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.21.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.13.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.20.attention.self.value.weight', 'bert.encoder.layer.12.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.21.attention.self.value.bias', 'bert.encoder.layer.15.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.14.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.18.attention.output.dense.weight', 'bert.encoder.layer.16.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.19.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.19.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.13.attention.self.key.weight', 'bert.encoder.layer.12.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.19.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.18.output.LayerNorm.weight', 'bert.encoder.layer.13.attention.self.key.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.15.intermediate.dense.bias', 'bert.encoder.layer.12.attention.self.key.weight', 'bert.encoder.layer.18.attention.self.key.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.22.attention.self.value.bias', 'bert.encoder.layer.13.attention.output.LayerNorm.bias', 'bert.encoder.layer.20.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.22.output.dense.weight', 'bert.encoder.layer.12.attention.output.dense.weight', 'bert.encoder.layer.16.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.16.attention.self.query.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.21.output.dense.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.12.attention.self.query.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.self.query.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.22.output.LayerNorm.weight', 'bert.encoder.layer.19.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.12.attention.self.query.weight', 'bert.encoder.layer.13.intermediate.dense.weight', 'bert.encoder.layer.17.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.self.key.weight', 'bert.encoder.layer.22.output.dense.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.15.attention.output.LayerNorm.bias', 'bert.encoder.layer.15.output.LayerNorm.weight', 'bert.encoder.layer.17.output.dense.bias', 'bert.encoder.layer.16.attention.output.LayerNorm.bias', 'bert.encoder.layer.19.attention.self.key.weight', 'bert.encoder.layer.18.intermediate.dense.bias', 'bert.encoder.layer.16.output.LayerNorm.weight', 'bert.encoder.layer.17.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.23.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.14.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.14.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.bias', 'bert.encoder.layer.20.output.LayerNorm.weight', 'bert.encoder.layer.14.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.21.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.23.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.21.attention.output.dense.bias', 'bert.encoder.layer.14.attention.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.self.query.weight', 'bert.encoder.layer.21.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.18.attention.self.key.bias', 'bert.encoder.layer.19.attention.self.query.bias', 'bert.encoder.layer.22.attention.self.value.weight', 'bert.encoder.layer.23.attention.output.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.23.attention.self.key.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.20.attention.self.value.bias', 'bert.encoder.layer.16.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.18.output.dense.weight', 'bert.encoder.layer.13.attention.self.query.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.14.attention.self.key.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.19.output.LayerNorm.weight', 'cls.predictions.bias', 'bert.encoder.layer.17.attention.self.key.bias', 'bert.encoder.layer.21.output.LayerNorm.weight', 'bert.encoder.layer.17.intermediate.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.14.attention.self.query.weight', 'bert.encoder.layer.14.intermediate.dense.bias', 'bert.encoder.layer.12.output.LayerNorm.weight', 'bert.encoder.layer.20.output.dense.weight', 'bert.encoder.layer.20.attention.self.key.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.13.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.18.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.15.intermediate.dense.weight', 'bert.encoder.layer.22.attention.output.LayerNorm.weight', 'bert.encoder.layer.18.attention.self.query.bias', 'bert.encoder.layer.21.attention.self.key.bias', 'bert.encoder.layer.18.intermediate.dense.weight', 'bert.encoder.layer.22.attention.self.key.bias', 'bert.encoder.layer.20.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.15.attention.self.query.weight', 'bert.encoder.layer.18.attention.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.23.attention.output.dense.weight', 'bert.encoder.layer.15.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.21.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.23.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.intermediate.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.13.attention.output.LayerNorm.weight', 'bert.encoder.layer.22.output.LayerNorm.bias', 'bert.encoder.layer.15.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.14.attention.self.query.bias', 'bert.encoder.layer.15.attention.output.dense.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.14.output.dense.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.16.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.13.output.LayerNorm.weight', 'bert.encoder.layer.14.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.15.attention.self.value.bias', 'bert.encoder.layer.23.output.dense.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.21.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.17.attention.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.12.output.dense.weight', 'bert.encoder.layer.20.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.20.intermediate.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.20.attention.self.query.weight', 'bert.encoder.layer.23.attention.self.value.bias', 'bert.encoder.layer.22.attention.output.dense.bias', 'bert.encoder.layer.19.attention.self.value.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.19.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.18.attention.output.LayerNorm.bias', 'bert.encoder.layer.17.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.self.key.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.19.intermediate.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.16.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.17.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.22.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.19.attention.self.query.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.19.output.dense.weight', 'bert.encoder.layer.22.intermediate.dense.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.23.output.LayerNorm.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.12.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.18.attention.self.query.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.20.intermediate.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.16.output.dense.weight', 'bert.encoder.layer.22.attention.self.key.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.12.attention.self.key.bias', 'bert.encoder.layer.22.intermediate.dense.bias', 'bert.encoder.layer.22.attention.output.LayerNorm.bias', 'bert.encoder.layer.23.intermediate.dense.weight', 'bert.encoder.layer.17.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.12.intermediate.dense.weight', 'bert.encoder.layer.12.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.output.dense.bias', 'bert.encoder.layer.15.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.21.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.23.attention.self.query.weight', 'bert.encoder.layer.14.attention.self.key.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.16.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.14.attention.self.value.bias', 'bert.encoder.layer.17.attention.self.query.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.22.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.13.output.dense.bias', 'bert.encoder.layer.13.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.21.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'cls.predictions.decoder.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.15.attention.self.key.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.19.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.13.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.18.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.19.attention.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.self.value.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.15.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.16.intermediate.dense.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.intermediate.dense.weight', 'bert.encoder.layer.12.attention.self.value.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.17.attention.self.key.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.18.output.dense.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.13.output.dense.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.19.attention.output.dense.bias', 'bert.encoder.layer.21.intermediate.dense.weight', 'bert.encoder.layer.18.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.16.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.22.attention.output.dense.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.17.attention.self.query.bias', 'bert.encoder.layer.17.output.LayerNorm.bias', 'bert.encoder.layer.13.intermediate.dense.bias', 'bert.encoder.layer.23.intermediate.dense.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.13.attention.output.dense.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_clue_large and are newly initialized: ['encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.13.output.dense.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.14.output.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.21.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.17.output.dense.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.19.output.dense.bias', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.17.output.dense.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.14.output.dense.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.12.output.dense.bias', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.20.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.20.attention.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.19.output.dense.weight', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.18.output.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.18.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.20.output.dense.bias', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.23.attention.self.value.bias', 'pooler.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.15.output.dense.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.19.attention.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.22.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.6.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.21.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
CustomModel(
  (pretrained_model): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=1)
      (position_embeddings): Embedding(512, 1024, padding_idx=1)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (12): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (13): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (14): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (15): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (16): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (17): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (18): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (19): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (20): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (21): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (22): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (23): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=1024, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.attention.self.query.weight False
pretrained_model.encoder.layer.12.attention.self.query.bias False
pretrained_model.encoder.layer.12.attention.self.key.weight False
pretrained_model.encoder.layer.12.attention.self.key.bias False
pretrained_model.encoder.layer.12.attention.self.value.weight False
pretrained_model.encoder.layer.12.attention.self.value.bias False
pretrained_model.encoder.layer.12.attention.output.dense.weight False
pretrained_model.encoder.layer.12.attention.output.dense.bias False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.intermediate.dense.weight False
pretrained_model.encoder.layer.12.intermediate.dense.bias False
pretrained_model.encoder.layer.12.output.dense.weight False
pretrained_model.encoder.layer.12.output.dense.bias False
pretrained_model.encoder.layer.12.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.attention.self.query.weight False
pretrained_model.encoder.layer.13.attention.self.query.bias False
pretrained_model.encoder.layer.13.attention.self.key.weight False
pretrained_model.encoder.layer.13.attention.self.key.bias False
pretrained_model.encoder.layer.13.attention.self.value.weight False
pretrained_model.encoder.layer.13.attention.self.value.bias False
pretrained_model.encoder.layer.13.attention.output.dense.weight False
pretrained_model.encoder.layer.13.attention.output.dense.bias False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.intermediate.dense.weight False
pretrained_model.encoder.layer.13.intermediate.dense.bias False
pretrained_model.encoder.layer.13.output.dense.weight False
pretrained_model.encoder.layer.13.output.dense.bias False
pretrained_model.encoder.layer.13.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.attention.self.query.weight False
pretrained_model.encoder.layer.14.attention.self.query.bias False
pretrained_model.encoder.layer.14.attention.self.key.weight False
pretrained_model.encoder.layer.14.attention.self.key.bias False
pretrained_model.encoder.layer.14.attention.self.value.weight False
pretrained_model.encoder.layer.14.attention.self.value.bias False
pretrained_model.encoder.layer.14.attention.output.dense.weight False
pretrained_model.encoder.layer.14.attention.output.dense.bias False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.intermediate.dense.weight False
pretrained_model.encoder.layer.14.intermediate.dense.bias False
pretrained_model.encoder.layer.14.output.dense.weight False
pretrained_model.encoder.layer.14.output.dense.bias False
pretrained_model.encoder.layer.14.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.attention.self.query.weight False
pretrained_model.encoder.layer.15.attention.self.query.bias False
pretrained_model.encoder.layer.15.attention.self.key.weight False
pretrained_model.encoder.layer.15.attention.self.key.bias False
pretrained_model.encoder.layer.15.attention.self.value.weight False
pretrained_model.encoder.layer.15.attention.self.value.bias False
pretrained_model.encoder.layer.15.attention.output.dense.weight False
pretrained_model.encoder.layer.15.attention.output.dense.bias False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.intermediate.dense.weight False
pretrained_model.encoder.layer.15.intermediate.dense.bias False
pretrained_model.encoder.layer.15.output.dense.weight False
pretrained_model.encoder.layer.15.output.dense.bias False
pretrained_model.encoder.layer.15.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.attention.self.query.weight False
pretrained_model.encoder.layer.16.attention.self.query.bias False
pretrained_model.encoder.layer.16.attention.self.key.weight False
pretrained_model.encoder.layer.16.attention.self.key.bias False
pretrained_model.encoder.layer.16.attention.self.value.weight False
pretrained_model.encoder.layer.16.attention.self.value.bias False
pretrained_model.encoder.layer.16.attention.output.dense.weight False
pretrained_model.encoder.layer.16.attention.output.dense.bias False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.intermediate.dense.weight False
pretrained_model.encoder.layer.16.intermediate.dense.bias False
pretrained_model.encoder.layer.16.output.dense.weight False
pretrained_model.encoder.layer.16.output.dense.bias False
pretrained_model.encoder.layer.16.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.attention.self.query.weight False
pretrained_model.encoder.layer.17.attention.self.query.bias False
pretrained_model.encoder.layer.17.attention.self.key.weight False
pretrained_model.encoder.layer.17.attention.self.key.bias False
pretrained_model.encoder.layer.17.attention.self.value.weight False
pretrained_model.encoder.layer.17.attention.self.value.bias False
pretrained_model.encoder.layer.17.attention.output.dense.weight False
pretrained_model.encoder.layer.17.attention.output.dense.bias False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.intermediate.dense.weight False
pretrained_model.encoder.layer.17.intermediate.dense.bias False
pretrained_model.encoder.layer.17.output.dense.weight False
pretrained_model.encoder.layer.17.output.dense.bias False
pretrained_model.encoder.layer.17.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.attention.self.query.weight False
pretrained_model.encoder.layer.18.attention.self.query.bias False
pretrained_model.encoder.layer.18.attention.self.key.weight False
pretrained_model.encoder.layer.18.attention.self.key.bias False
pretrained_model.encoder.layer.18.attention.self.value.weight False
pretrained_model.encoder.layer.18.attention.self.value.bias False
pretrained_model.encoder.layer.18.attention.output.dense.weight False
pretrained_model.encoder.layer.18.attention.output.dense.bias False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.intermediate.dense.weight False
pretrained_model.encoder.layer.18.intermediate.dense.bias False
pretrained_model.encoder.layer.18.output.dense.weight False
pretrained_model.encoder.layer.18.output.dense.bias False
pretrained_model.encoder.layer.18.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.attention.self.query.weight False
pretrained_model.encoder.layer.19.attention.self.query.bias False
pretrained_model.encoder.layer.19.attention.self.key.weight False
pretrained_model.encoder.layer.19.attention.self.key.bias False
pretrained_model.encoder.layer.19.attention.self.value.weight False
pretrained_model.encoder.layer.19.attention.self.value.bias False
pretrained_model.encoder.layer.19.attention.output.dense.weight False
pretrained_model.encoder.layer.19.attention.output.dense.bias False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.intermediate.dense.weight False
pretrained_model.encoder.layer.19.intermediate.dense.bias False
pretrained_model.encoder.layer.19.output.dense.weight False
pretrained_model.encoder.layer.19.output.dense.bias False
pretrained_model.encoder.layer.19.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.attention.self.query.weight False
pretrained_model.encoder.layer.20.attention.self.query.bias False
pretrained_model.encoder.layer.20.attention.self.key.weight False
pretrained_model.encoder.layer.20.attention.self.key.bias False
pretrained_model.encoder.layer.20.attention.self.value.weight False
pretrained_model.encoder.layer.20.attention.self.value.bias False
pretrained_model.encoder.layer.20.attention.output.dense.weight False
pretrained_model.encoder.layer.20.attention.output.dense.bias False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.intermediate.dense.weight False
pretrained_model.encoder.layer.20.intermediate.dense.bias False
pretrained_model.encoder.layer.20.output.dense.weight False
pretrained_model.encoder.layer.20.output.dense.bias False
pretrained_model.encoder.layer.20.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.attention.self.query.weight False
pretrained_model.encoder.layer.21.attention.self.query.bias False
pretrained_model.encoder.layer.21.attention.self.key.weight False
pretrained_model.encoder.layer.21.attention.self.key.bias False
pretrained_model.encoder.layer.21.attention.self.value.weight False
pretrained_model.encoder.layer.21.attention.self.value.bias False
pretrained_model.encoder.layer.21.attention.output.dense.weight False
pretrained_model.encoder.layer.21.attention.output.dense.bias False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.intermediate.dense.weight False
pretrained_model.encoder.layer.21.intermediate.dense.bias False
pretrained_model.encoder.layer.21.output.dense.weight False
pretrained_model.encoder.layer.21.output.dense.bias False
pretrained_model.encoder.layer.21.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.attention.self.query.weight False
pretrained_model.encoder.layer.22.attention.self.query.bias False
pretrained_model.encoder.layer.22.attention.self.key.weight False
pretrained_model.encoder.layer.22.attention.self.key.bias False
pretrained_model.encoder.layer.22.attention.self.value.weight False
pretrained_model.encoder.layer.22.attention.self.value.bias False
pretrained_model.encoder.layer.22.attention.output.dense.weight False
pretrained_model.encoder.layer.22.attention.output.dense.bias False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.intermediate.dense.weight False
pretrained_model.encoder.layer.22.intermediate.dense.bias False
pretrained_model.encoder.layer.22.output.dense.weight False
pretrained_model.encoder.layer.22.output.dense.bias False
pretrained_model.encoder.layer.22.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.attention.self.query.weight False
pretrained_model.encoder.layer.23.attention.self.query.bias False
pretrained_model.encoder.layer.23.attention.self.key.weight False
pretrained_model.encoder.layer.23.attention.self.key.bias False
pretrained_model.encoder.layer.23.attention.self.value.weight False
pretrained_model.encoder.layer.23.attention.self.value.bias False
pretrained_model.encoder.layer.23.attention.output.dense.weight False
pretrained_model.encoder.layer.23.attention.output.dense.bias False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.intermediate.dense.weight False
pretrained_model.encoder.layer.23.intermediate.dense.bias False
pretrained_model.encoder.layer.23.output.dense.weight False
pretrained_model.encoder.layer.23.output.dense.bias False
pretrained_model.encoder.layer.23.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 223s, train loss=2.7525, train acc=9.06%, dev loss=2.5707, dev acc=13.89%
saving, test loss=2.5788, test acc=12.68%
epoch: 2/10000, 226s, train loss=2.6340, train acc=11.31%, dev loss=2.5056, dev acc=17.54%
saving, test loss=2.5165, test acc=16.98%
epoch: 3/10000, 225s, train loss=2.5904, train acc=12.76%, dev loss=2.4698, dev acc=18.42%
saving, test loss=2.4807, test acc=18.55%
epoch: 4/10000, 226s, train loss=2.5584, train acc=14.28%, dev loss=2.4431, dev acc=20.28%
saving, test loss=2.4535, test acc=20.09%
epoch: 5/10000, 227s, train loss=2.5358, train acc=15.29%, dev loss=2.4218, dev acc=21.93%
saving, test loss=2.4308, test acc=21.39%
epoch: 6/10000, 228s, train loss=2.5207, train acc=15.87%, dev loss=2.4075, dev acc=22.86%
saving, test loss=2.4157, test acc=22.39%
epoch: 7/10000, 227s, train loss=2.5055, train acc=16.57%, dev loss=2.3973, dev acc=23.14%
saving, test loss=2.4047, test acc=22.84%
epoch: 8/10000, 228s, train loss=2.4980, train acc=17.24%, dev loss=2.3866, dev acc=23.31%
saving, test loss=2.3933, test acc=23.16%
epoch: 9/10000, 229s, train loss=2.4900, train acc=17.28%, dev loss=2.3798, dev acc=23.61%
saving, test loss=2.3865, test acc=23.59%
epoch: 10/10000, 229s, train loss=2.4848, train acc=17.76%, dev loss=2.3734, dev acc=23.65%
saving, test loss=2.3795, test acc=23.85%
epoch: 11/10000, 229s, train loss=2.4867, train acc=17.53%, dev loss=2.3681, dev acc=24.31%
saving, test loss=2.3736, test acc=24.34%
epoch: 12/10000, 229s, train loss=2.4772, train acc=18.04%, dev loss=2.3643, dev acc=24.53%
saving, test loss=2.3695, test acc=24.38%
epoch: 13/10000, 229s, train loss=2.4800, train acc=18.11%, dev loss=2.3614, dev acc=24.64%
saving, test loss=2.3676, test acc=24.32%
epoch: 14/10000, 229s, train loss=2.4756, train acc=18.15%, dev loss=2.3588, dev acc=24.91%
saving, test loss=2.3650, test acc=24.71%
epoch: 15/10000, 229s, train loss=2.4776, train acc=18.07%, dev loss=2.3556, dev acc=24.57%
epoch: 16/10000, 230s, train loss=2.4738, train acc=18.31%, dev loss=2.3534, dev acc=24.72%
epoch: 17/10000, 230s, train loss=2.4717, train acc=18.35%, dev loss=2.3510, dev acc=25.19%
saving, test loss=2.3555, test acc=25.32%
epoch: 18/10000, 230s, train loss=2.4715, train acc=18.43%, dev loss=2.3492, dev acc=25.06%
epoch: 19/10000, 230s, train loss=2.4724, train acc=18.18%, dev loss=2.3501, dev acc=25.13%
epoch: 20/10000, 230s, train loss=2.4711, train acc=18.40%, dev loss=2.3478, dev acc=24.93%
epoch: 21/10000, 230s, train loss=2.4671, train acc=18.62%, dev loss=2.3473, dev acc=25.32%
saving, test loss=2.3532, test acc=25.10%
epoch: 22/10000, 230s, train loss=2.4707, train acc=18.50%, dev loss=2.3443, dev acc=25.13%
epoch: 23/10000, 230s, train loss=2.4739, train acc=18.32%, dev loss=2.3467, dev acc=25.34%
saving, test loss=2.3533, test acc=24.97%
epoch: 24/10000, 230s, train loss=2.4719, train acc=18.35%, dev loss=2.3449, dev acc=24.98%
epoch: 25/10000, 233s, train loss=2.4703, train acc=18.63%, dev loss=2.3437, dev acc=24.98%
epoch: 26/10000, 313s, train loss=2.4693, train acc=18.57%, dev loss=2.3459, dev acc=24.98%
epoch: 27/10000, 237s, train loss=2.4719, train acc=18.39%, dev loss=2.3441, dev acc=24.64%
epoch: 28/10000, 226s, train loss=2.4696, train acc=18.53%, dev loss=2.3438, dev acc=24.87%
time used=8053.1s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='0', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_clue_tiny', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_sports': 0, 'news_agriculture': 1, 'news_edu': 2, 'news_military': 3, 'news_story': 4, 'news_culture': 5, 'news_travel': 6, 'news_house': 7, 'news_world': 8, 'news_finance': 9, 'news_car': 10, 'news_game': 11, 'news_entertainment': 12, 'news_stock': 13, 'news_tech': 14}
index_labels_dict={0: 'news_sports', 1: 'news_agriculture', 2: 'news_edu', 3: 'news_military', 4: 'news_story', 5: 'news_culture', 6: 'news_travel', 7: 'news_house', 8: 'news_world', 9: 'news_finance', 10: 'news_car', 11: 'news_game', 12: 'news_entertainment', 13: 'news_stock', 14: 'news_tech'}
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. 
The class this function is called from is 'BertTokenizer'.
max_sent_len=147
147	1
60	1
56	2
55	3
54	1
53	1
52	5
51	6
50	9
49	13
48	6
47	10
46	66
45	16
44	30
43	46
42	119
41	113
40	119
39	136
38	145
37	209
36	226
35	295
34	383
33	893
32	3585
31	2638
30	2365
29	2220
28	2158
27	2131
26	2255
25	2210
24	2285
23	2255
22	2180
21	2183
20	1999
19	2240
18	1818
17	1934
16	1716
15	1570
14	1422
13	1173
12	932
11	776
10	518
9	317
8	144
7	125
6	16
5	1
4	4
max_sent_len=51
max_sent_len=51
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_clue_tiny were not used when initializing RobertaModel: ['bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'cls.seq_relationship.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'cls.predictions.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'cls.predictions.transform.dense.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'cls.predictions.decoder.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_clue_tiny and are newly initialized: ['encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.self.value.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
CustomModel(
  (pretrained_model): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(8021, 312, padding_idx=1)
      (position_embeddings): Embedding(512, 312, padding_idx=1)
      (token_type_embeddings): Embedding(2, 312)
      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=312, out_features=312, bias=True)
              (key): Linear(in_features=312, out_features=312, bias=True)
              (value): Linear(in_features=312, out_features=312, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=312, out_features=312, bias=True)
              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=312, out_features=1248, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=1248, out_features=312, bias=True)
            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=312, out_features=312, bias=True)
              (key): Linear(in_features=312, out_features=312, bias=True)
              (value): Linear(in_features=312, out_features=312, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=312, out_features=312, bias=True)
              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=312, out_features=1248, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=1248, out_features=312, bias=True)
            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=312, out_features=312, bias=True)
              (key): Linear(in_features=312, out_features=312, bias=True)
              (value): Linear(in_features=312, out_features=312, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=312, out_features=312, bias=True)
              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=312, out_features=1248, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=1248, out_features=312, bias=True)
            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=312, out_features=312, bias=True)
              (key): Linear(in_features=312, out_features=312, bias=True)
              (value): Linear(in_features=312, out_features=312, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=312, out_features=312, bias=True)
              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=312, out_features=1248, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=1248, out_features=312, bias=True)
            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=312, out_features=312, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=312, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 8s, train loss=2.6897, train acc=8.93%, dev loss=2.5911, dev acc=12.22%
saving, test loss=2.5952, test acc=11.75%
epoch: 2/10000, 12s, train loss=2.6309, train acc=10.21%, dev loss=2.5721, dev acc=12.46%
saving, test loss=2.5760, test acc=12.30%
epoch: 3/10000, 12s, train loss=2.6080, train acc=11.13%, dev loss=2.5628, dev acc=12.52%
saving, test loss=2.5668, test acc=12.92%
epoch: 4/10000, 12s, train loss=2.5947, train acc=11.72%, dev loss=2.5558, dev acc=13.10%
saving, test loss=2.5599, test acc=13.19%
epoch: 5/10000, 12s, train loss=2.5818, train acc=12.53%, dev loss=2.5492, dev acc=13.77%
saving, test loss=2.5533, test acc=13.62%
epoch: 6/10000, 9s, train loss=2.5723, train acc=12.97%, dev loss=2.5429, dev acc=14.84%
saving, test loss=2.5470, test acc=14.46%
epoch: 7/10000, 9s, train loss=2.5632, train acc=13.93%, dev loss=2.5370, dev acc=15.25%
saving, test loss=2.5410, test acc=15.07%
epoch: 8/10000, 12s, train loss=2.5561, train acc=14.23%, dev loss=2.5314, dev acc=16.34%
saving, test loss=2.5354, test acc=16.25%
epoch: 9/10000, 12s, train loss=2.5512, train acc=14.61%, dev loss=2.5262, dev acc=16.57%
saving, test loss=2.5299, test acc=16.62%
epoch: 10/10000, 12s, train loss=2.5450, train acc=15.03%, dev loss=2.5211, dev acc=17.43%
saving, test loss=2.5249, test acc=17.15%
epoch: 11/10000, 12s, train loss=2.5394, train acc=15.28%, dev loss=2.5162, dev acc=17.63%
saving, test loss=2.5198, test acc=17.50%
epoch: 12/10000, 7s, train loss=2.5365, train acc=15.57%, dev loss=2.5117, dev acc=18.03%
saving, test loss=2.5154, test acc=17.91%
epoch: 13/10000, 12s, train loss=2.5319, train acc=15.79%, dev loss=2.5072, dev acc=18.52%
saving, test loss=2.5106, test acc=18.26%
epoch: 14/10000, 11s, train loss=2.5307, train acc=15.89%, dev loss=2.5030, dev acc=18.91%
saving, test loss=2.5064, test acc=18.46%
epoch: 15/10000, 12s, train loss=2.5253, train acc=16.11%, dev loss=2.4990, dev acc=19.21%
saving, test loss=2.5025, test acc=18.53%
epoch: 16/10000, 12s, train loss=2.5238, train acc=16.18%, dev loss=2.4951, dev acc=19.70%
saving, test loss=2.4984, test acc=19.13%
epoch: 17/10000, 10s, train loss=2.5194, train acc=16.56%, dev loss=2.4915, dev acc=19.88%
saving, test loss=2.4948, test acc=19.24%
epoch: 18/10000, 9s, train loss=2.5179, train acc=16.25%, dev loss=2.4879, dev acc=20.33%
saving, test loss=2.4911, test acc=19.35%
epoch: 19/10000, 12s, train loss=2.5136, train acc=16.72%, dev loss=2.4842, dev acc=21.01%
saving, test loss=2.4873, test acc=19.89%
epoch: 20/10000, 12s, train loss=2.5122, train acc=16.71%, dev loss=2.4809, dev acc=20.93%
epoch: 21/10000, 12s, train loss=2.5112, train acc=16.78%, dev loss=2.4778, dev acc=21.40%
saving, test loss=2.4808, test acc=20.44%
epoch: 22/10000, 12s, train loss=2.5077, train acc=16.98%, dev loss=2.4746, dev acc=21.29%
epoch: 23/10000, 10s, train loss=2.5073, train acc=16.70%, dev loss=2.4718, dev acc=21.48%
saving, test loss=2.4748, test acc=20.59%
epoch: 24/10000, 9s, train loss=2.5049, train acc=17.10%, dev loss=2.4690, dev acc=21.61%
saving, test loss=2.4718, test acc=20.58%
epoch: 25/10000, 12s, train loss=2.5026, train acc=17.31%, dev loss=2.4663, dev acc=21.81%
saving, test loss=2.4690, test acc=20.84%
epoch: 26/10000, 12s, train loss=2.5057, train acc=16.86%, dev loss=2.4640, dev acc=21.63%
epoch: 27/10000, 12s, train loss=2.5023, train acc=17.12%, dev loss=2.4615, dev acc=21.93%
saving, test loss=2.4642, test acc=20.85%
epoch: 28/10000, 12s, train loss=2.4985, train acc=17.20%, dev loss=2.4593, dev acc=21.96%
saving, test loss=2.4621, test acc=21.19%
epoch: 29/10000, 9s, train loss=2.4988, train acc=17.26%, dev loss=2.4571, dev acc=22.04%
saving, test loss=2.4596, test acc=21.22%
epoch: 30/10000, 9s, train loss=2.4986, train acc=17.27%, dev loss=2.4551, dev acc=22.11%
saving, test loss=2.4574, test acc=21.22%
epoch: 31/10000, 13s, train loss=2.4977, train acc=17.32%, dev loss=2.4530, dev acc=22.23%
saving, test loss=2.4553, test acc=21.30%
epoch: 32/10000, 13s, train loss=2.4944, train acc=17.10%, dev loss=2.4509, dev acc=22.45%
saving, test loss=2.4531, test acc=21.60%
epoch: 33/10000, 13s, train loss=2.4962, train acc=17.42%, dev loss=2.4490, dev acc=22.38%
epoch: 34/10000, 12s, train loss=2.4931, train acc=17.40%, dev loss=2.4472, dev acc=22.66%
saving, test loss=2.4494, test acc=21.79%
epoch: 35/10000, 13s, train loss=2.4904, train acc=17.88%, dev loss=2.4454, dev acc=22.62%
epoch: 36/10000, 13s, train loss=2.4921, train acc=17.27%, dev loss=2.4437, dev acc=22.58%
epoch: 37/10000, 13s, train loss=2.4913, train acc=17.41%, dev loss=2.4422, dev acc=22.43%
epoch: 38/10000, 13s, train loss=2.4932, train acc=17.49%, dev loss=2.4404, dev acc=22.66%
epoch: 39/10000, 13s, train loss=2.4900, train acc=17.67%, dev loss=2.4388, dev acc=22.68%
saving, test loss=2.4407, test acc=22.13%
epoch: 40/10000, 13s, train loss=2.4901, train acc=17.36%, dev loss=2.4375, dev acc=22.71%
saving, test loss=2.4393, test acc=22.27%
epoch: 41/10000, 12s, train loss=2.4909, train acc=17.41%, dev loss=2.4362, dev acc=22.88%
saving, test loss=2.4381, test acc=22.21%
epoch: 42/10000, 13s, train loss=2.4890, train acc=17.71%, dev loss=2.4346, dev acc=22.77%
epoch: 43/10000, 13s, train loss=2.4891, train acc=17.40%, dev loss=2.4334, dev acc=22.88%
epoch: 44/10000, 10s, train loss=2.4889, train acc=17.48%, dev loss=2.4322, dev acc=22.66%
epoch: 45/10000, 9s, train loss=2.4867, train acc=17.63%, dev loss=2.4310, dev acc=22.71%
epoch: 46/10000, 12s, train loss=2.4879, train acc=17.34%, dev loss=2.4297, dev acc=22.73%
time used=717.6s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='0', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_large', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_sports': 0, 'news_entertainment': 1, 'news_agriculture': 2, 'news_finance': 3, 'news_edu': 4, 'news_tech': 5, 'news_car': 6, 'news_culture': 7, 'news_house': 8, 'news_travel': 9, 'news_military': 10, 'news_game': 11, 'news_stock': 12, 'news_world': 13, 'news_story': 14}
index_labels_dict={0: 'news_sports', 1: 'news_entertainment', 2: 'news_agriculture', 3: 'news_finance', 4: 'news_edu', 5: 'news_tech', 6: 'news_car', 7: 'news_culture', 8: 'news_house', 9: 'news_travel', 10: 'news_military', 11: 'news_game', 12: 'news_stock', 13: 'news_world', 14: 'news_story'}
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. 
The class this function is called from is 'BertTokenizer'.
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_large were not used when initializing RobertaModel: ['bert.encoder.layer.21.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.23.attention.self.query.bias', 'bert.encoder.layer.20.attention.self.query.bias', 'bert.encoder.layer.12.attention.output.LayerNorm.bias', 'bert.encoder.layer.12.attention.self.query.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.23.attention.output.LayerNorm.weight', 'bert.encoder.layer.17.attention.self.query.bias', 'bert.encoder.layer.22.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.21.attention.self.key.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.18.attention.self.key.bias', 'bert.encoder.layer.18.intermediate.dense.weight', 'bert.encoder.layer.14.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.15.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.20.output.LayerNorm.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.17.attention.self.key.weight', 'bert.encoder.layer.17.attention.self.query.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.13.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.self.value.weight', 'bert.encoder.layer.18.attention.output.LayerNorm.weight', 'cls.predictions.bias', 'bert.encoder.layer.12.attention.self.key.weight', 'bert.encoder.layer.22.output.dense.weight', 'bert.encoder.layer.19.attention.output.dense.bias', 'bert.encoder.layer.14.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.23.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.output.dense.bias', 'bert.encoder.layer.15.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.23.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.18.attention.self.query.bias', 'bert.encoder.layer.23.output.dense.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.22.intermediate.dense.bias', 'bert.encoder.layer.19.output.dense.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.20.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.22.attention.self.query.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.19.output.dense.weight', 'bert.encoder.layer.22.attention.output.dense.bias', 'bert.encoder.layer.20.attention.self.query.weight', 'bert.encoder.layer.15.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.19.output.LayerNorm.bias', 'cls.seq_relationship.bias', 'bert.encoder.layer.21.intermediate.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.self.query.weight', 'bert.encoder.layer.17.output.LayerNorm.bias', 'bert.encoder.layer.17.attention.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.13.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.12.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.23.attention.self.value.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.22.attention.self.key.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.19.attention.self.key.weight', 'bert.encoder.layer.22.attention.self.query.weight', 'bert.encoder.layer.22.attention.self.key.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.16.attention.self.key.weight', 'bert.encoder.layer.12.attention.self.key.bias', 'bert.encoder.layer.20.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.15.output.LayerNorm.bias', 'bert.encoder.layer.16.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.23.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.14.output.LayerNorm.weight', 'bert.encoder.layer.23.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.19.attention.self.key.bias', 'bert.encoder.layer.14.attention.self.query.weight', 'bert.encoder.layer.22.output.dense.bias', 'bert.encoder.layer.12.output.dense.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.22.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.15.attention.output.dense.weight', 'bert.encoder.layer.16.attention.self.key.bias', 'bert.encoder.layer.18.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.19.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.bias', 'bert.encoder.layer.13.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.14.attention.self.query.bias', 'bert.encoder.layer.12.attention.self.value.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.14.attention.output.LayerNorm.weight', 'bert.encoder.layer.18.output.dense.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.23.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.bias', 'bert.encoder.layer.13.output.dense.bias', 'bert.encoder.layer.15.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.14.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.14.attention.self.key.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.17.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.13.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.14.intermediate.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.12.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.19.attention.output.dense.weight', 'bert.encoder.layer.12.output.LayerNorm.weight', 'bert.encoder.layer.22.attention.output.dense.weight', 'bert.encoder.layer.22.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.14.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.17.output.dense.bias', 'bert.encoder.layer.15.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.20.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'cls.predictions.decoder.bias', 'bert.encoder.layer.23.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.16.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.13.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'cls.predictions.decoder.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.13.attention.self.value.bias', 'bert.encoder.layer.14.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.18.output.LayerNorm.bias', 'bert.encoder.layer.13.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.21.attention.self.value.weight', 'bert.encoder.layer.16.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.12.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.18.output.LayerNorm.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.20.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.16.intermediate.dense.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.18.attention.output.LayerNorm.bias', 'bert.encoder.layer.19.attention.self.query.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.21.output.LayerNorm.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.23.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.21.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.19.attention.self.value.bias', 'bert.encoder.layer.13.attention.output.LayerNorm.bias', 'bert.encoder.layer.20.attention.self.key.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.16.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.18.attention.output.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.19.attention.output.LayerNorm.weight', 'bert.encoder.layer.18.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.23.output.dense.weight', 'bert.encoder.layer.20.attention.self.key.bias', 'bert.encoder.layer.15.attention.output.LayerNorm.bias', 'bert.encoder.layer.13.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.21.attention.self.key.weight', 'bert.encoder.layer.16.output.LayerNorm.weight', 'bert.encoder.layer.21.attention.self.value.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.14.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.17.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.19.attention.self.query.bias', 'bert.encoder.layer.17.attention.self.value.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.18.intermediate.dense.bias', 'bert.encoder.layer.17.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.15.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.15.attention.self.key.weight', 'bert.encoder.layer.23.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.13.intermediate.dense.weight', 'bert.encoder.layer.16.attention.output.dense.weight', 'bert.encoder.layer.13.attention.self.key.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.22.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.17.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.21.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.17.attention.self.value.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.13.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.17.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.15.attention.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.19.attention.output.LayerNorm.bias', 'bert.encoder.layer.20.attention.self.value.weight', 'bert.encoder.layer.18.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.16.output.dense.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.19.intermediate.dense.weight', 'bert.encoder.layer.19.intermediate.dense.bias', 'bert.encoder.layer.17.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.13.attention.self.query.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.12.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.12.intermediate.dense.bias', 'bert.encoder.layer.14.intermediate.dense.bias', 'bert.encoder.layer.15.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.18.attention.self.value.bias', 'bert.encoder.layer.20.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.20.attention.output.dense.bias', 'bert.encoder.layer.20.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.12.attention.self.value.bias', 'bert.encoder.layer.22.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.22.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.17.intermediate.dense.bias', 'bert.encoder.layer.21.output.LayerNorm.bias', 'bert.encoder.layer.15.intermediate.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.16.intermediate.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.19.attention.self.value.weight', 'bert.encoder.layer.15.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.13.intermediate.dense.bias', 'bert.encoder.layer.21.intermediate.dense.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.16.output.LayerNorm.bias', 'bert.encoder.layer.23.attention.self.value.bias', 'bert.encoder.layer.17.attention.output.dense.bias', 'bert.encoder.layer.12.attention.output.dense.weight', 'bert.encoder.layer.16.attention.output.dense.bias', 'bert.encoder.layer.16.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.23.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.20.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.14.attention.output.dense.weight', 'bert.encoder.layer.13.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.20.intermediate.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.22.attention.self.value.weight', 'bert.encoder.layer.15.attention.self.value.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.21.output.dense.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.18.attention.self.query.weight', 'bert.encoder.layer.18.attention.self.value.weight', 'bert.encoder.layer.16.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.21.attention.self.query.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_large and are newly initialized: ['encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.20.attention.self.query.bias', 'pooler.dense.bias', 'encoder.layer.23.output.dense.bias', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.19.output.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.17.output.dense.bias', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.14.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.14.output.dense.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.21.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.13.output.dense.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.16.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.15.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.18.output.dense.bias', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.17.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.18.output.dense.weight', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.22.output.dense.bias', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.19.output.LayerNorm.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.22.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.15.output.dense.bias', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.21.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.20.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.8.attention.self.key.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.23.output.dense.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.12.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.19.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
CustomModel(
  (pretrained_model): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=1)
      (position_embeddings): Embedding(512, 1024, padding_idx=1)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (12): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (13): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (14): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (15): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (16): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (17): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (18): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (19): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (20): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (21): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (22): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (23): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=1024, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.attention.self.query.weight False
pretrained_model.encoder.layer.12.attention.self.query.bias False
pretrained_model.encoder.layer.12.attention.self.key.weight False
pretrained_model.encoder.layer.12.attention.self.key.bias False
pretrained_model.encoder.layer.12.attention.self.value.weight False
pretrained_model.encoder.layer.12.attention.self.value.bias False
pretrained_model.encoder.layer.12.attention.output.dense.weight False
pretrained_model.encoder.layer.12.attention.output.dense.bias False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.intermediate.dense.weight False
pretrained_model.encoder.layer.12.intermediate.dense.bias False
pretrained_model.encoder.layer.12.output.dense.weight False
pretrained_model.encoder.layer.12.output.dense.bias False
pretrained_model.encoder.layer.12.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.attention.self.query.weight False
pretrained_model.encoder.layer.13.attention.self.query.bias False
pretrained_model.encoder.layer.13.attention.self.key.weight False
pretrained_model.encoder.layer.13.attention.self.key.bias False
pretrained_model.encoder.layer.13.attention.self.value.weight False
pretrained_model.encoder.layer.13.attention.self.value.bias False
pretrained_model.encoder.layer.13.attention.output.dense.weight False
pretrained_model.encoder.layer.13.attention.output.dense.bias False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.intermediate.dense.weight False
pretrained_model.encoder.layer.13.intermediate.dense.bias False
pretrained_model.encoder.layer.13.output.dense.weight False
pretrained_model.encoder.layer.13.output.dense.bias False
pretrained_model.encoder.layer.13.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.attention.self.query.weight False
pretrained_model.encoder.layer.14.attention.self.query.bias False
pretrained_model.encoder.layer.14.attention.self.key.weight False
pretrained_model.encoder.layer.14.attention.self.key.bias False
pretrained_model.encoder.layer.14.attention.self.value.weight False
pretrained_model.encoder.layer.14.attention.self.value.bias False
pretrained_model.encoder.layer.14.attention.output.dense.weight False
pretrained_model.encoder.layer.14.attention.output.dense.bias False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.intermediate.dense.weight False
pretrained_model.encoder.layer.14.intermediate.dense.bias False
pretrained_model.encoder.layer.14.output.dense.weight False
pretrained_model.encoder.layer.14.output.dense.bias False
pretrained_model.encoder.layer.14.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.attention.self.query.weight False
pretrained_model.encoder.layer.15.attention.self.query.bias False
pretrained_model.encoder.layer.15.attention.self.key.weight False
pretrained_model.encoder.layer.15.attention.self.key.bias False
pretrained_model.encoder.layer.15.attention.self.value.weight False
pretrained_model.encoder.layer.15.attention.self.value.bias False
pretrained_model.encoder.layer.15.attention.output.dense.weight False
pretrained_model.encoder.layer.15.attention.output.dense.bias False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.intermediate.dense.weight False
pretrained_model.encoder.layer.15.intermediate.dense.bias False
pretrained_model.encoder.layer.15.output.dense.weight False
pretrained_model.encoder.layer.15.output.dense.bias False
pretrained_model.encoder.layer.15.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.attention.self.query.weight False
pretrained_model.encoder.layer.16.attention.self.query.bias False
pretrained_model.encoder.layer.16.attention.self.key.weight False
pretrained_model.encoder.layer.16.attention.self.key.bias False
pretrained_model.encoder.layer.16.attention.self.value.weight False
pretrained_model.encoder.layer.16.attention.self.value.bias False
pretrained_model.encoder.layer.16.attention.output.dense.weight False
pretrained_model.encoder.layer.16.attention.output.dense.bias False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.intermediate.dense.weight False
pretrained_model.encoder.layer.16.intermediate.dense.bias False
pretrained_model.encoder.layer.16.output.dense.weight False
pretrained_model.encoder.layer.16.output.dense.bias False
pretrained_model.encoder.layer.16.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.attention.self.query.weight False
pretrained_model.encoder.layer.17.attention.self.query.bias False
pretrained_model.encoder.layer.17.attention.self.key.weight False
pretrained_model.encoder.layer.17.attention.self.key.bias False
pretrained_model.encoder.layer.17.attention.self.value.weight False
pretrained_model.encoder.layer.17.attention.self.value.bias False
pretrained_model.encoder.layer.17.attention.output.dense.weight False
pretrained_model.encoder.layer.17.attention.output.dense.bias False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.intermediate.dense.weight False
pretrained_model.encoder.layer.17.intermediate.dense.bias False
pretrained_model.encoder.layer.17.output.dense.weight False
pretrained_model.encoder.layer.17.output.dense.bias False
pretrained_model.encoder.layer.17.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.attention.self.query.weight False
pretrained_model.encoder.layer.18.attention.self.query.bias False
pretrained_model.encoder.layer.18.attention.self.key.weight False
pretrained_model.encoder.layer.18.attention.self.key.bias False
pretrained_model.encoder.layer.18.attention.self.value.weight False
pretrained_model.encoder.layer.18.attention.self.value.bias False
pretrained_model.encoder.layer.18.attention.output.dense.weight False
pretrained_model.encoder.layer.18.attention.output.dense.bias False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.intermediate.dense.weight False
pretrained_model.encoder.layer.18.intermediate.dense.bias False
pretrained_model.encoder.layer.18.output.dense.weight False
pretrained_model.encoder.layer.18.output.dense.bias False
pretrained_model.encoder.layer.18.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.attention.self.query.weight False
pretrained_model.encoder.layer.19.attention.self.query.bias False
pretrained_model.encoder.layer.19.attention.self.key.weight False
pretrained_model.encoder.layer.19.attention.self.key.bias False
pretrained_model.encoder.layer.19.attention.self.value.weight False
pretrained_model.encoder.layer.19.attention.self.value.bias False
pretrained_model.encoder.layer.19.attention.output.dense.weight False
pretrained_model.encoder.layer.19.attention.output.dense.bias False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.intermediate.dense.weight False
pretrained_model.encoder.layer.19.intermediate.dense.bias False
pretrained_model.encoder.layer.19.output.dense.weight False
pretrained_model.encoder.layer.19.output.dense.bias False
pretrained_model.encoder.layer.19.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.attention.self.query.weight False
pretrained_model.encoder.layer.20.attention.self.query.bias False
pretrained_model.encoder.layer.20.attention.self.key.weight False
pretrained_model.encoder.layer.20.attention.self.key.bias False
pretrained_model.encoder.layer.20.attention.self.value.weight False
pretrained_model.encoder.layer.20.attention.self.value.bias False
pretrained_model.encoder.layer.20.attention.output.dense.weight False
pretrained_model.encoder.layer.20.attention.output.dense.bias False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.intermediate.dense.weight False
pretrained_model.encoder.layer.20.intermediate.dense.bias False
pretrained_model.encoder.layer.20.output.dense.weight False
pretrained_model.encoder.layer.20.output.dense.bias False
pretrained_model.encoder.layer.20.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.attention.self.query.weight False
pretrained_model.encoder.layer.21.attention.self.query.bias False
pretrained_model.encoder.layer.21.attention.self.key.weight False
pretrained_model.encoder.layer.21.attention.self.key.bias False
pretrained_model.encoder.layer.21.attention.self.value.weight False
pretrained_model.encoder.layer.21.attention.self.value.bias False
pretrained_model.encoder.layer.21.attention.output.dense.weight False
pretrained_model.encoder.layer.21.attention.output.dense.bias False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.intermediate.dense.weight False
pretrained_model.encoder.layer.21.intermediate.dense.bias False
pretrained_model.encoder.layer.21.output.dense.weight False
pretrained_model.encoder.layer.21.output.dense.bias False
pretrained_model.encoder.layer.21.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.attention.self.query.weight False
pretrained_model.encoder.layer.22.attention.self.query.bias False
pretrained_model.encoder.layer.22.attention.self.key.weight False
pretrained_model.encoder.layer.22.attention.self.key.bias False
pretrained_model.encoder.layer.22.attention.self.value.weight False
pretrained_model.encoder.layer.22.attention.self.value.bias False
pretrained_model.encoder.layer.22.attention.output.dense.weight False
pretrained_model.encoder.layer.22.attention.output.dense.bias False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.intermediate.dense.weight False
pretrained_model.encoder.layer.22.intermediate.dense.bias False
pretrained_model.encoder.layer.22.output.dense.weight False
pretrained_model.encoder.layer.22.output.dense.bias False
pretrained_model.encoder.layer.22.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.attention.self.query.weight False
pretrained_model.encoder.layer.23.attention.self.query.bias False
pretrained_model.encoder.layer.23.attention.self.key.weight False
pretrained_model.encoder.layer.23.attention.self.key.bias False
pretrained_model.encoder.layer.23.attention.self.value.weight False
pretrained_model.encoder.layer.23.attention.self.value.bias False
pretrained_model.encoder.layer.23.attention.output.dense.weight False
pretrained_model.encoder.layer.23.attention.output.dense.bias False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.intermediate.dense.weight False
pretrained_model.encoder.layer.23.intermediate.dense.bias False
pretrained_model.encoder.layer.23.output.dense.weight False
pretrained_model.encoder.layer.23.output.dense.bias False
pretrained_model.encoder.layer.23.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 355s, train loss=2.7541, train acc=9.29%, dev loss=2.5707, dev acc=13.92%
saving, test loss=2.5765, test acc=13.28%
epoch: 2/10000, 345s, train loss=2.6415, train acc=11.29%, dev loss=2.5108, dev acc=16.92%
saving, test loss=2.5165, test acc=16.28%
epoch: 3/10000, 347s, train loss=2.5930, train acc=12.81%, dev loss=2.4775, dev acc=18.40%
saving, test loss=2.4834, test acc=18.09%
epoch: 4/10000, 348s, train loss=2.5571, train acc=13.96%, dev loss=2.4539, dev acc=19.38%
saving, test loss=2.4600, test acc=19.36%
epoch: 5/10000, 327s, train loss=2.5384, train acc=15.11%, dev loss=2.4351, dev acc=20.37%
saving, test loss=2.4408, test acc=20.16%
epoch: 6/10000, 319s, train loss=2.5213, train acc=15.89%, dev loss=2.4249, dev acc=20.86%
saving, test loss=2.4305, test acc=20.76%
epoch: 7/10000, 354s, train loss=2.5097, train acc=16.39%, dev loss=2.4149, dev acc=21.29%
saving, test loss=2.4208, test acc=20.88%
epoch: 8/10000, 354s, train loss=2.5051, train acc=16.66%, dev loss=2.4069, dev acc=21.44%
saving, test loss=2.4128, test acc=21.12%
epoch: 9/10000, 348s, train loss=2.4958, train acc=17.01%, dev loss=2.4013, dev acc=21.76%
saving, test loss=2.4068, test acc=21.45%
epoch: 10/10000, 222s, train loss=2.4938, train acc=17.09%, dev loss=2.3946, dev acc=22.06%
saving, test loss=2.4001, test acc=21.96%
epoch: 11/10000, 223s, train loss=2.4928, train acc=17.39%, dev loss=2.3902, dev acc=22.28%
saving, test loss=2.3964, test acc=21.94%
epoch: 12/10000, 223s, train loss=2.4848, train acc=17.52%, dev loss=2.3862, dev acc=22.25%
epoch: 13/10000, 223s, train loss=2.4884, train acc=17.53%, dev loss=2.3809, dev acc=22.96%
saving, test loss=2.3873, test acc=22.43%
epoch: 14/10000, 223s, train loss=2.4853, train acc=17.71%, dev loss=2.3775, dev acc=22.81%
epoch: 15/10000, 224s, train loss=2.4797, train acc=18.00%, dev loss=2.3793, dev acc=23.18%
saving, test loss=2.3867, test acc=22.39%
epoch: 16/10000, 224s, train loss=2.4781, train acc=18.19%, dev loss=2.3758, dev acc=22.64%
epoch: 17/10000, 224s, train loss=2.4782, train acc=18.17%, dev loss=2.3759, dev acc=22.58%
epoch: 18/10000, 224s, train loss=2.4790, train acc=17.94%, dev loss=2.3713, dev acc=23.01%
epoch: 19/10000, 224s, train loss=2.4795, train acc=17.83%, dev loss=2.3710, dev acc=22.81%
epoch: 20/10000, 224s, train loss=2.4783, train acc=17.88%, dev loss=2.3667, dev acc=23.31%
saving, test loss=2.3739, test acc=23.04%
epoch: 21/10000, 225s, train loss=2.4770, train acc=18.07%, dev loss=2.3678, dev acc=23.93%
saving, test loss=2.3750, test acc=23.09%
epoch: 22/10000, 225s, train loss=2.4741, train acc=18.09%, dev loss=2.3663, dev acc=23.86%
epoch: 23/10000, 225s, train loss=2.4747, train acc=18.23%, dev loss=2.3637, dev acc=23.11%
epoch: 24/10000, 225s, train loss=2.4793, train acc=17.93%, dev loss=2.3663, dev acc=23.67%
epoch: 25/10000, 225s, train loss=2.4775, train acc=17.90%, dev loss=2.3650, dev acc=23.82%
epoch: 26/10000, 225s, train loss=2.4758, train acc=17.88%, dev loss=2.3607, dev acc=24.36%
saving, test loss=2.3675, test acc=23.78%
epoch: 27/10000, 225s, train loss=2.4757, train acc=18.30%, dev loss=2.3630, dev acc=24.42%
saving, test loss=2.3698, test acc=23.96%
epoch: 28/10000, 225s, train loss=2.4792, train acc=17.94%, dev loss=2.3632, dev acc=24.36%
epoch: 29/10000, 225s, train loss=2.4756, train acc=18.17%, dev loss=2.3616, dev acc=24.36%
epoch: 30/10000, 225s, train loss=2.4733, train acc=18.14%, dev loss=2.3614, dev acc=24.04%
epoch: 31/10000, 225s, train loss=2.4732, train acc=17.93%, dev loss=2.3588, dev acc=24.33%
epoch: 32/10000, 225s, train loss=2.4734, train acc=18.23%, dev loss=2.3567, dev acc=24.63%
saving, test loss=2.3619, test acc=24.19%
epoch: 33/10000, 225s, train loss=2.4768, train acc=18.27%, dev loss=2.3580, dev acc=24.27%
epoch: 34/10000, 225s, train loss=2.4769, train acc=18.25%, dev loss=2.3573, dev acc=24.29%
epoch: 35/10000, 225s, train loss=2.4773, train acc=17.98%, dev loss=2.3597, dev acc=24.21%
epoch: 36/10000, 225s, train loss=2.4777, train acc=18.05%, dev loss=2.3597, dev acc=24.14%
epoch: 37/10000, 226s, train loss=2.4779, train acc=18.00%, dev loss=2.3575, dev acc=24.16%
time used=11460.0s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='0', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_pair_large', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_finance': 0, 'news_culture': 1, 'news_tech': 2, 'news_game': 3, 'news_stock': 4, 'news_entertainment': 5, 'news_military': 6, 'news_world': 7, 'news_house': 8, 'news_sports': 9, 'news_agriculture': 10, 'news_edu': 11, 'news_story': 12, 'news_car': 13, 'news_travel': 14}
index_labels_dict={0: 'news_finance', 1: 'news_culture', 2: 'news_tech', 3: 'news_game', 4: 'news_stock', 5: 'news_entertainment', 6: 'news_military', 7: 'news_world', 8: 'news_house', 9: 'news_sports', 10: 'news_agriculture', 11: 'news_edu', 12: 'news_story', 13: 'news_car', 14: 'news_travel'}
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. 
The class this function is called from is 'BertTokenizer'.
max_sent_len=147
147	1
60	1
56	2
55	3
54	1
53	1
52	5
51	6
50	9
49	13
48	6
47	10
46	66
45	16
44	30
43	46
42	119
41	113
40	119
39	136
38	145
37	209
36	226
35	295
34	383
33	893
32	3585
31	2638
30	2365
29	2220
28	2158
27	2131
26	2255
25	2210
24	2285
23	2255
22	2180
21	2183
20	1999
19	2240
18	1818
17	1934
16	1716
15	1570
14	1422
13	1173
12	932
11	776
10	518
9	317
8	144
7	125
6	16
5	1
4	4
max_sent_len=51
max_sent_len=51
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_pair_large were not used when initializing RobertaModel: ['bert.encoder.layer.14.attention.output.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.15.intermediate.dense.bias', 'bert.encoder.layer.14.output.dense.bias', 'bert.encoder.layer.19.attention.self.query.weight', 'bert.encoder.layer.21.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.23.attention.self.value.weight', 'bert.encoder.layer.18.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.16.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.self.value.weight', 'bert.encoder.layer.22.output.dense.bias', 'bert.encoder.layer.13.attention.output.dense.weight', 'bert.encoder.layer.22.output.LayerNorm.bias', 'bert.encoder.layer.12.attention.output.LayerNorm.bias', 'bert.encoder.layer.19.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.12.output.dense.weight', 'bert.encoder.layer.14.intermediate.dense.weight', 'bert.encoder.layer.15.output.LayerNorm.weight', 'bert.encoder.layer.18.output.dense.weight', 'bert.encoder.layer.22.attention.self.query.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.18.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'cls.predictions.decoder.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.23.attention.self.query.bias', 'bert.encoder.layer.23.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.13.output.LayerNorm.bias', 'bert.encoder.layer.18.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.13.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.18.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.23.attention.output.LayerNorm.bias', 'bert.encoder.layer.12.intermediate.dense.bias', 'bert.encoder.layer.23.attention.self.key.weight', 'bert.encoder.layer.17.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.23.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.17.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.17.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.19.output.LayerNorm.weight', 'bert.encoder.layer.21.attention.self.key.bias', 'bert.encoder.layer.20.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.17.output.dense.weight', 'bert.encoder.layer.16.output.dense.weight', 'bert.encoder.layer.21.attention.self.value.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.14.output.dense.weight', 'bert.encoder.layer.19.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.20.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.16.attention.self.query.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.20.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.19.attention.self.key.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.22.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.18.attention.self.query.weight', 'bert.encoder.layer.22.attention.self.value.bias', 'bert.encoder.layer.17.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.12.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.16.attention.self.value.bias', 'bert.encoder.layer.13.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.output.dense.bias', 'bert.encoder.layer.21.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.16.attention.output.LayerNorm.bias', 'bert.encoder.layer.23.attention.self.key.bias', 'bert.encoder.layer.14.attention.self.value.weight', 'bert.encoder.layer.23.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.14.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.20.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.16.attention.self.key.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.13.attention.output.LayerNorm.bias', 'bert.encoder.layer.17.attention.output.LayerNorm.bias', 'bert.encoder.layer.18.attention.self.key.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.17.attention.self.key.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.18.attention.self.value.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.16.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.18.attention.self.key.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.15.attention.self.value.bias', 'bert.encoder.layer.21.attention.self.value.bias', 'bert.encoder.layer.13.attention.self.query.bias', 'bert.encoder.layer.19.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.22.attention.self.key.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.23.attention.self.value.bias', 'bert.encoder.layer.15.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.23.attention.self.query.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.18.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.22.output.dense.weight', 'bert.encoder.layer.15.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.18.output.LayerNorm.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.21.output.LayerNorm.weight', 'bert.encoder.layer.12.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.12.attention.self.query.weight', 'bert.encoder.layer.14.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.15.attention.self.value.weight', 'bert.encoder.layer.15.output.dense.bias', 'bert.encoder.layer.19.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.23.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.20.output.dense.weight', 'bert.encoder.layer.21.attention.self.query.weight', 'bert.encoder.layer.17.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.21.attention.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.18.intermediate.dense.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.20.output.LayerNorm.bias', 'bert.encoder.layer.20.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.20.output.dense.bias', 'bert.encoder.layer.16.output.dense.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.13.attention.self.key.bias', 'bert.encoder.layer.13.output.dense.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.21.output.dense.weight', 'bert.encoder.layer.13.attention.self.query.weight', 'bert.encoder.layer.16.attention.output.dense.weight', 'bert.encoder.layer.12.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.attention.self.query.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.12.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.23.output.dense.bias', 'bert.encoder.layer.19.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.20.attention.self.value.bias', 'bert.encoder.layer.15.attention.self.key.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.16.intermediate.dense.weight', 'bert.encoder.layer.20.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.14.attention.self.query.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.17.attention.self.query.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'cls.seq_relationship.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.16.output.LayerNorm.bias', 'bert.encoder.layer.12.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.15.attention.self.query.bias', 'bert.encoder.layer.12.attention.self.key.bias', 'bert.encoder.layer.22.intermediate.dense.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.19.attention.self.value.bias', 'bert.encoder.layer.23.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.22.attention.output.dense.bias', 'bert.encoder.layer.12.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.21.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.13.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'cls.predictions.bias', 'bert.encoder.layer.17.attention.self.key.bias', 'bert.encoder.layer.13.attention.self.key.weight', 'bert.encoder.layer.16.attention.output.dense.bias', 'bert.encoder.layer.19.attention.self.query.bias', 'bert.encoder.layer.22.attention.self.value.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.19.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.20.attention.self.key.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.13.attention.self.value.weight', 'bert.encoder.layer.19.attention.output.dense.bias', 'bert.encoder.layer.12.output.LayerNorm.bias', 'bert.encoder.layer.19.output.dense.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.21.output.LayerNorm.bias', 'bert.encoder.layer.13.intermediate.dense.weight', 'bert.encoder.layer.18.attention.output.LayerNorm.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.23.output.dense.weight', 'bert.encoder.layer.23.output.LayerNorm.bias', 'bert.encoder.layer.13.attention.self.value.bias', 'bert.encoder.layer.18.attention.self.value.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.19.attention.output.LayerNorm.bias', 'bert.encoder.layer.15.intermediate.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.12.attention.output.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.attention.self.value.weight', 'bert.encoder.layer.17.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.20.attention.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.20.attention.self.key.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.self.query.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.20.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.17.intermediate.dense.weight', 'bert.encoder.layer.19.intermediate.dense.bias', 'bert.encoder.layer.14.attention.self.value.bias', 'bert.encoder.layer.20.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.self.key.bias', 'bert.encoder.layer.16.intermediate.dense.bias', 'bert.encoder.layer.21.attention.self.query.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.14.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.18.intermediate.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.17.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.22.attention.output.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.22.attention.self.query.weight', 'bert.encoder.layer.14.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.14.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.21.attention.self.key.weight', 'bert.encoder.layer.21.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.21.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.13.output.dense.weight', 'bert.encoder.layer.19.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.16.attention.self.value.weight', 'bert.encoder.layer.14.attention.output.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.14.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.17.attention.output.dense.bias', 'bert.encoder.layer.22.attention.output.LayerNorm.weight', 'bert.encoder.layer.17.attention.self.query.weight', 'bert.encoder.layer.18.attention.self.query.bias', 'bert.encoder.layer.14.attention.self.key.weight', 'bert.encoder.layer.22.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.22.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.output.LayerNorm.weight', 'bert.encoder.layer.17.attention.self.value.weight', 'bert.encoder.layer.15.attention.output.dense.weight', 'bert.encoder.layer.13.output.LayerNorm.weight', 'bert.encoder.layer.22.attention.self.key.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_pair_large and are newly initialized: ['encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.22.output.dense.bias', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.12.attention.output.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.17.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.13.output.dense.bias', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.16.output.dense.weight', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.15.output.dense.bias', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.23.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.17.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.14.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.18.output.dense.bias', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'pooler.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.23.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.21.output.dense.bias', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.16.intermediate.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.19.output.dense.bias', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.12.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.14.output.dense.weight', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.19.output.dense.weight', 'encoder.layer.21.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.22.attention.self.query.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
CustomModel(
  (pretrained_model): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=1)
      (position_embeddings): Embedding(512, 1024, padding_idx=1)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (12): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (13): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (14): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (15): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (16): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (17): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (18): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (19): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (20): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (21): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (22): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (23): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=1024, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.attention.self.query.weight False
pretrained_model.encoder.layer.12.attention.self.query.bias False
pretrained_model.encoder.layer.12.attention.self.key.weight False
pretrained_model.encoder.layer.12.attention.self.key.bias False
pretrained_model.encoder.layer.12.attention.self.value.weight False
pretrained_model.encoder.layer.12.attention.self.value.bias False
pretrained_model.encoder.layer.12.attention.output.dense.weight False
pretrained_model.encoder.layer.12.attention.output.dense.bias False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.intermediate.dense.weight False
pretrained_model.encoder.layer.12.intermediate.dense.bias False
pretrained_model.encoder.layer.12.output.dense.weight False
pretrained_model.encoder.layer.12.output.dense.bias False
pretrained_model.encoder.layer.12.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.attention.self.query.weight False
pretrained_model.encoder.layer.13.attention.self.query.bias False
pretrained_model.encoder.layer.13.attention.self.key.weight False
pretrained_model.encoder.layer.13.attention.self.key.bias False
pretrained_model.encoder.layer.13.attention.self.value.weight False
pretrained_model.encoder.layer.13.attention.self.value.bias False
pretrained_model.encoder.layer.13.attention.output.dense.weight False
pretrained_model.encoder.layer.13.attention.output.dense.bias False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.intermediate.dense.weight False
pretrained_model.encoder.layer.13.intermediate.dense.bias False
pretrained_model.encoder.layer.13.output.dense.weight False
pretrained_model.encoder.layer.13.output.dense.bias False
pretrained_model.encoder.layer.13.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.attention.self.query.weight False
pretrained_model.encoder.layer.14.attention.self.query.bias False
pretrained_model.encoder.layer.14.attention.self.key.weight False
pretrained_model.encoder.layer.14.attention.self.key.bias False
pretrained_model.encoder.layer.14.attention.self.value.weight False
pretrained_model.encoder.layer.14.attention.self.value.bias False
pretrained_model.encoder.layer.14.attention.output.dense.weight False
pretrained_model.encoder.layer.14.attention.output.dense.bias False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.intermediate.dense.weight False
pretrained_model.encoder.layer.14.intermediate.dense.bias False
pretrained_model.encoder.layer.14.output.dense.weight False
pretrained_model.encoder.layer.14.output.dense.bias False
pretrained_model.encoder.layer.14.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.attention.self.query.weight False
pretrained_model.encoder.layer.15.attention.self.query.bias False
pretrained_model.encoder.layer.15.attention.self.key.weight False
pretrained_model.encoder.layer.15.attention.self.key.bias False
pretrained_model.encoder.layer.15.attention.self.value.weight False
pretrained_model.encoder.layer.15.attention.self.value.bias False
pretrained_model.encoder.layer.15.attention.output.dense.weight False
pretrained_model.encoder.layer.15.attention.output.dense.bias False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.intermediate.dense.weight False
pretrained_model.encoder.layer.15.intermediate.dense.bias False
pretrained_model.encoder.layer.15.output.dense.weight False
pretrained_model.encoder.layer.15.output.dense.bias False
pretrained_model.encoder.layer.15.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.attention.self.query.weight False
pretrained_model.encoder.layer.16.attention.self.query.bias False
pretrained_model.encoder.layer.16.attention.self.key.weight False
pretrained_model.encoder.layer.16.attention.self.key.bias False
pretrained_model.encoder.layer.16.attention.self.value.weight False
pretrained_model.encoder.layer.16.attention.self.value.bias False
pretrained_model.encoder.layer.16.attention.output.dense.weight False
pretrained_model.encoder.layer.16.attention.output.dense.bias False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.intermediate.dense.weight False
pretrained_model.encoder.layer.16.intermediate.dense.bias False
pretrained_model.encoder.layer.16.output.dense.weight False
pretrained_model.encoder.layer.16.output.dense.bias False
pretrained_model.encoder.layer.16.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.attention.self.query.weight False
pretrained_model.encoder.layer.17.attention.self.query.bias False
pretrained_model.encoder.layer.17.attention.self.key.weight False
pretrained_model.encoder.layer.17.attention.self.key.bias False
pretrained_model.encoder.layer.17.attention.self.value.weight False
pretrained_model.encoder.layer.17.attention.self.value.bias False
pretrained_model.encoder.layer.17.attention.output.dense.weight False
pretrained_model.encoder.layer.17.attention.output.dense.bias False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.intermediate.dense.weight False
pretrained_model.encoder.layer.17.intermediate.dense.bias False
pretrained_model.encoder.layer.17.output.dense.weight False
pretrained_model.encoder.layer.17.output.dense.bias False
pretrained_model.encoder.layer.17.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.attention.self.query.weight False
pretrained_model.encoder.layer.18.attention.self.query.bias False
pretrained_model.encoder.layer.18.attention.self.key.weight False
pretrained_model.encoder.layer.18.attention.self.key.bias False
pretrained_model.encoder.layer.18.attention.self.value.weight False
pretrained_model.encoder.layer.18.attention.self.value.bias False
pretrained_model.encoder.layer.18.attention.output.dense.weight False
pretrained_model.encoder.layer.18.attention.output.dense.bias False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.intermediate.dense.weight False
pretrained_model.encoder.layer.18.intermediate.dense.bias False
pretrained_model.encoder.layer.18.output.dense.weight False
pretrained_model.encoder.layer.18.output.dense.bias False
pretrained_model.encoder.layer.18.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.attention.self.query.weight False
pretrained_model.encoder.layer.19.attention.self.query.bias False
pretrained_model.encoder.layer.19.attention.self.key.weight False
pretrained_model.encoder.layer.19.attention.self.key.bias False
pretrained_model.encoder.layer.19.attention.self.value.weight False
pretrained_model.encoder.layer.19.attention.self.value.bias False
pretrained_model.encoder.layer.19.attention.output.dense.weight False
pretrained_model.encoder.layer.19.attention.output.dense.bias False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.intermediate.dense.weight False
pretrained_model.encoder.layer.19.intermediate.dense.bias False
pretrained_model.encoder.layer.19.output.dense.weight False
pretrained_model.encoder.layer.19.output.dense.bias False
pretrained_model.encoder.layer.19.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.attention.self.query.weight False
pretrained_model.encoder.layer.20.attention.self.query.bias False
pretrained_model.encoder.layer.20.attention.self.key.weight False
pretrained_model.encoder.layer.20.attention.self.key.bias False
pretrained_model.encoder.layer.20.attention.self.value.weight False
pretrained_model.encoder.layer.20.attention.self.value.bias False
pretrained_model.encoder.layer.20.attention.output.dense.weight False
pretrained_model.encoder.layer.20.attention.output.dense.bias False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.intermediate.dense.weight False
pretrained_model.encoder.layer.20.intermediate.dense.bias False
pretrained_model.encoder.layer.20.output.dense.weight False
pretrained_model.encoder.layer.20.output.dense.bias False
pretrained_model.encoder.layer.20.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.attention.self.query.weight False
pretrained_model.encoder.layer.21.attention.self.query.bias False
pretrained_model.encoder.layer.21.attention.self.key.weight False
pretrained_model.encoder.layer.21.attention.self.key.bias False
pretrained_model.encoder.layer.21.attention.self.value.weight False
pretrained_model.encoder.layer.21.attention.self.value.bias False
pretrained_model.encoder.layer.21.attention.output.dense.weight False
pretrained_model.encoder.layer.21.attention.output.dense.bias False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.intermediate.dense.weight False
pretrained_model.encoder.layer.21.intermediate.dense.bias False
pretrained_model.encoder.layer.21.output.dense.weight False
pretrained_model.encoder.layer.21.output.dense.bias False
pretrained_model.encoder.layer.21.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.attention.self.query.weight False
pretrained_model.encoder.layer.22.attention.self.query.bias False
pretrained_model.encoder.layer.22.attention.self.key.weight False
pretrained_model.encoder.layer.22.attention.self.key.bias False
pretrained_model.encoder.layer.22.attention.self.value.weight False
pretrained_model.encoder.layer.22.attention.self.value.bias False
pretrained_model.encoder.layer.22.attention.output.dense.weight False
pretrained_model.encoder.layer.22.attention.output.dense.bias False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.intermediate.dense.weight False
pretrained_model.encoder.layer.22.intermediate.dense.bias False
pretrained_model.encoder.layer.22.output.dense.weight False
pretrained_model.encoder.layer.22.output.dense.bias False
pretrained_model.encoder.layer.22.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.attention.self.query.weight False
pretrained_model.encoder.layer.23.attention.self.query.bias False
pretrained_model.encoder.layer.23.attention.self.key.weight False
pretrained_model.encoder.layer.23.attention.self.key.bias False
pretrained_model.encoder.layer.23.attention.self.value.weight False
pretrained_model.encoder.layer.23.attention.self.value.bias False
pretrained_model.encoder.layer.23.attention.output.dense.weight False
pretrained_model.encoder.layer.23.attention.output.dense.bias False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.intermediate.dense.weight False
pretrained_model.encoder.layer.23.intermediate.dense.bias False
pretrained_model.encoder.layer.23.output.dense.weight False
pretrained_model.encoder.layer.23.output.dense.bias False
pretrained_model.encoder.layer.23.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 225s, train loss=2.7659, train acc=9.14%, dev loss=2.5719, dev acc=14.06%
saving, test loss=2.5739, test acc=13.23%
epoch: 2/10000, 227s, train loss=2.6425, train acc=10.99%, dev loss=2.5051, dev acc=17.43%
saving, test loss=2.5104, test acc=17.74%
epoch: 3/10000, 227s, train loss=2.5927, train acc=12.92%, dev loss=2.4689, dev acc=19.75%
saving, test loss=2.4753, test acc=19.42%
epoch: 4/10000, 227s, train loss=2.5626, train acc=14.14%, dev loss=2.4434, dev acc=21.20%
saving, test loss=2.4502, test acc=20.92%
epoch: 5/10000, 227s, train loss=2.5397, train acc=15.24%, dev loss=2.4238, dev acc=22.02%
saving, test loss=2.4297, test acc=22.10%
epoch: 6/10000, 228s, train loss=2.5212, train acc=15.98%, dev loss=2.4093, dev acc=22.77%
saving, test loss=2.4154, test acc=22.71%
epoch: 7/10000, 228s, train loss=2.5059, train acc=16.68%, dev loss=2.3987, dev acc=23.05%
saving, test loss=2.4046, test acc=23.14%
epoch: 8/10000, 228s, train loss=2.5004, train acc=16.95%, dev loss=2.3886, dev acc=23.35%
saving, test loss=2.3942, test acc=23.31%
epoch: 9/10000, 228s, train loss=2.4900, train acc=17.25%, dev loss=2.3812, dev acc=23.74%
saving, test loss=2.3873, test acc=23.99%
epoch: 10/10000, 228s, train loss=2.4859, train acc=17.61%, dev loss=2.3753, dev acc=24.12%
saving, test loss=2.3806, test acc=24.11%
epoch: 11/10000, 228s, train loss=2.4866, train acc=17.53%, dev loss=2.3695, dev acc=24.57%
saving, test loss=2.3747, test acc=24.48%
epoch: 12/10000, 228s, train loss=2.4783, train acc=17.88%, dev loss=2.3658, dev acc=24.53%
epoch: 13/10000, 229s, train loss=2.4802, train acc=17.96%, dev loss=2.3634, dev acc=24.53%
epoch: 14/10000, 229s, train loss=2.4760, train acc=18.33%, dev loss=2.3602, dev acc=24.68%
saving, test loss=2.3661, test acc=24.64%
epoch: 15/10000, 229s, train loss=2.4779, train acc=18.12%, dev loss=2.3571, dev acc=24.57%
epoch: 16/10000, 229s, train loss=2.4739, train acc=18.29%, dev loss=2.3548, dev acc=24.87%
saving, test loss=2.3603, test acc=25.14%
epoch: 17/10000, 229s, train loss=2.4720, train acc=18.42%, dev loss=2.3511, dev acc=25.13%
saving, test loss=2.3554, test acc=25.13%
epoch: 18/10000, 229s, train loss=2.4716, train acc=18.41%, dev loss=2.3496, dev acc=25.15%
saving, test loss=2.3548, test acc=25.23%
epoch: 19/10000, 229s, train loss=2.4728, train acc=18.19%, dev loss=2.3510, dev acc=25.09%
epoch: 20/10000, 229s, train loss=2.4711, train acc=18.49%, dev loss=2.3484, dev acc=24.98%
epoch: 21/10000, 229s, train loss=2.4674, train acc=18.56%, dev loss=2.3474, dev acc=25.26%
saving, test loss=2.3533, test acc=25.03%
epoch: 22/10000, 230s, train loss=2.4710, train acc=18.44%, dev loss=2.3444, dev acc=25.13%
epoch: 23/10000, 230s, train loss=2.4741, train acc=18.35%, dev loss=2.3471, dev acc=24.94%
epoch: 24/10000, 230s, train loss=2.4717, train acc=18.35%, dev loss=2.3452, dev acc=24.64%
epoch: 25/10000, 230s, train loss=2.4704, train acc=18.64%, dev loss=2.3443, dev acc=25.09%
epoch: 26/10000, 230s, train loss=2.4692, train acc=18.56%, dev loss=2.3457, dev acc=25.00%
time used=7380.8s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='0', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_pair_tiny', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_tech': 0, 'news_finance': 1, 'news_world': 2, 'news_story': 3, 'news_culture': 4, 'news_travel': 5, 'news_agriculture': 6, 'news_game': 7, 'news_car': 8, 'news_stock': 9, 'news_entertainment': 10, 'news_sports': 11, 'news_edu': 12, 'news_house': 13, 'news_military': 14}
index_labels_dict={0: 'news_tech', 1: 'news_finance', 2: 'news_world', 3: 'news_story', 4: 'news_culture', 5: 'news_travel', 6: 'news_agriculture', 7: 'news_game', 8: 'news_car', 9: 'news_stock', 10: 'news_entertainment', 11: 'news_sports', 12: 'news_edu', 13: 'news_house', 14: 'news_military'}
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. 
The class this function is called from is 'BertTokenizer'.
max_sent_len=147
147	1
60	1
56	2
55	3
54	1
53	1
52	5
51	6
50	9
49	13
48	6
47	10
46	66
45	16
44	30
43	46
42	119
41	113
40	119
39	136
38	145
37	209
36	226
35	295
34	383
33	893
32	3585
31	2638
30	2365
29	2220
28	2158
27	2131
26	2255
25	2210
24	2285
23	2255
22	2180
21	2183
20	1999
19	2240
18	1818
17	1934
16	1716
15	1570
14	1422
13	1173
12	932
11	776
10	518
9	317
8	144
7	125
6	16
5	1
4	4
max_sent_len=51
max_sent_len=51
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_pair_tiny were not used when initializing RobertaModel: ['bert.embeddings.LayerNorm.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'cls.predictions.decoder.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'cls.seq_relationship.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'cls.predictions.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_roberta_chinese_pair_tiny and are newly initialized: ['encoder.layer.3.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'embeddings.word_embeddings.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.2.attention.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'pooler.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.3.attention.self.key.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
CustomModel(
  (pretrained_model): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(8021, 312, padding_idx=1)
      (position_embeddings): Embedding(512, 312, padding_idx=1)
      (token_type_embeddings): Embedding(2, 312)
      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=312, out_features=312, bias=True)
              (key): Linear(in_features=312, out_features=312, bias=True)
              (value): Linear(in_features=312, out_features=312, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=312, out_features=312, bias=True)
              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=312, out_features=1248, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=1248, out_features=312, bias=True)
            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=312, out_features=312, bias=True)
              (key): Linear(in_features=312, out_features=312, bias=True)
              (value): Linear(in_features=312, out_features=312, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=312, out_features=312, bias=True)
              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=312, out_features=1248, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=1248, out_features=312, bias=True)
            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=312, out_features=312, bias=True)
              (key): Linear(in_features=312, out_features=312, bias=True)
              (value): Linear(in_features=312, out_features=312, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=312, out_features=312, bias=True)
              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=312, out_features=1248, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=1248, out_features=312, bias=True)
            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=312, out_features=312, bias=True)
              (key): Linear(in_features=312, out_features=312, bias=True)
              (value): Linear(in_features=312, out_features=312, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=312, out_features=312, bias=True)
              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=312, out_features=1248, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=1248, out_features=312, bias=True)
            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=312, out_features=312, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=312, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 7s, train loss=2.6910, train acc=9.46%, dev loss=2.5922, dev acc=11.32%
saving, test loss=2.5950, test acc=10.94%
epoch: 2/10000, 7s, train loss=2.6328, train acc=10.11%, dev loss=2.5727, dev acc=12.69%
saving, test loss=2.5759, test acc=12.27%
epoch: 3/10000, 7s, train loss=2.6092, train acc=10.91%, dev loss=2.5637, dev acc=12.84%
saving, test loss=2.5674, test acc=12.74%
epoch: 4/10000, 7s, train loss=2.5951, train acc=11.70%, dev loss=2.5563, dev acc=12.82%
epoch: 5/10000, 7s, train loss=2.5821, train acc=12.26%, dev loss=2.5498, dev acc=13.17%
saving, test loss=2.5530, test acc=13.62%
epoch: 6/10000, 7s, train loss=2.5721, train acc=12.63%, dev loss=2.5433, dev acc=14.02%
saving, test loss=2.5466, test acc=14.24%
epoch: 7/10000, 7s, train loss=2.5632, train acc=13.36%, dev loss=2.5372, dev acc=14.77%
saving, test loss=2.5405, test acc=14.83%
epoch: 8/10000, 7s, train loss=2.5556, train acc=13.73%, dev loss=2.5315, dev acc=16.08%
saving, test loss=2.5349, test acc=15.79%
epoch: 9/10000, 7s, train loss=2.5513, train acc=14.41%, dev loss=2.5263, dev acc=16.12%
saving, test loss=2.5294, test acc=16.16%
epoch: 10/10000, 7s, train loss=2.5454, train acc=14.58%, dev loss=2.5211, dev acc=16.94%
saving, test loss=2.5243, test acc=16.71%
epoch: 11/10000, 7s, train loss=2.5392, train acc=15.06%, dev loss=2.5161, dev acc=17.34%
saving, test loss=2.5190, test acc=17.10%
epoch: 12/10000, 7s, train loss=2.5364, train acc=15.47%, dev loss=2.5116, dev acc=17.93%
saving, test loss=2.5147, test acc=17.58%
epoch: 13/10000, 7s, train loss=2.5304, train acc=15.83%, dev loss=2.5070, dev acc=18.29%
saving, test loss=2.5098, test acc=18.08%
epoch: 14/10000, 7s, train loss=2.5299, train acc=15.68%, dev loss=2.5027, dev acc=18.52%
saving, test loss=2.5056, test acc=18.38%
epoch: 15/10000, 7s, train loss=2.5247, train acc=15.93%, dev loss=2.4986, dev acc=18.63%
saving, test loss=2.5015, test acc=18.54%
epoch: 16/10000, 7s, train loss=2.5229, train acc=16.16%, dev loss=2.4947, dev acc=19.21%
saving, test loss=2.4974, test acc=19.08%
epoch: 17/10000, 7s, train loss=2.5189, train acc=16.52%, dev loss=2.4910, dev acc=19.60%
saving, test loss=2.4938, test acc=19.04%
epoch: 18/10000, 7s, train loss=2.5167, train acc=16.42%, dev loss=2.4874, dev acc=19.98%
saving, test loss=2.4901, test acc=19.32%
epoch: 19/10000, 7s, train loss=2.5128, train acc=16.76%, dev loss=2.4836, dev acc=20.78%
saving, test loss=2.4862, test acc=19.82%
epoch: 20/10000, 7s, train loss=2.5113, train acc=16.71%, dev loss=2.4803, dev acc=20.99%
saving, test loss=2.4830, test acc=20.09%
epoch: 21/10000, 7s, train loss=2.5096, train acc=16.73%, dev loss=2.4772, dev acc=21.21%
saving, test loss=2.4798, test acc=20.35%
epoch: 22/10000, 7s, train loss=2.5066, train acc=16.97%, dev loss=2.4740, dev acc=21.36%
saving, test loss=2.4766, test acc=20.30%
epoch: 23/10000, 7s, train loss=2.5058, train acc=16.87%, dev loss=2.4711, dev acc=21.33%
epoch: 24/10000, 7s, train loss=2.5043, train acc=17.04%, dev loss=2.4683, dev acc=21.72%
saving, test loss=2.4707, test acc=20.72%
epoch: 25/10000, 7s, train loss=2.5018, train acc=17.37%, dev loss=2.4656, dev acc=21.74%
saving, test loss=2.4679, test acc=20.92%
epoch: 26/10000, 7s, train loss=2.5048, train acc=16.82%, dev loss=2.4633, dev acc=21.63%
epoch: 27/10000, 7s, train loss=2.5015, train acc=17.23%, dev loss=2.4608, dev acc=22.02%
saving, test loss=2.4631, test acc=21.04%
epoch: 28/10000, 7s, train loss=2.4977, train acc=17.17%, dev loss=2.4586, dev acc=21.91%
epoch: 29/10000, 7s, train loss=2.4980, train acc=17.32%, dev loss=2.4564, dev acc=22.08%
saving, test loss=2.4585, test acc=21.37%
epoch: 30/10000, 7s, train loss=2.4978, train acc=17.15%, dev loss=2.4544, dev acc=22.17%
saving, test loss=2.4564, test acc=21.47%
epoch: 31/10000, 7s, train loss=2.4968, train acc=17.29%, dev loss=2.4522, dev acc=22.21%
saving, test loss=2.4542, test acc=21.42%
epoch: 32/10000, 7s, train loss=2.4935, train acc=17.17%, dev loss=2.4502, dev acc=22.43%
saving, test loss=2.4520, test acc=21.76%
epoch: 33/10000, 7s, train loss=2.4952, train acc=17.35%, dev loss=2.4483, dev acc=22.17%
epoch: 34/10000, 7s, train loss=2.4925, train acc=17.33%, dev loss=2.4465, dev acc=22.60%
saving, test loss=2.4484, test acc=22.06%
epoch: 35/10000, 7s, train loss=2.4896, train acc=17.86%, dev loss=2.4446, dev acc=22.34%
epoch: 36/10000, 7s, train loss=2.4915, train acc=17.32%, dev loss=2.4429, dev acc=22.60%
epoch: 37/10000, 7s, train loss=2.4907, train acc=17.48%, dev loss=2.4415, dev acc=22.45%
epoch: 38/10000, 7s, train loss=2.4926, train acc=17.48%, dev loss=2.4397, dev acc=22.56%
epoch: 39/10000, 7s, train loss=2.4895, train acc=17.67%, dev loss=2.4381, dev acc=22.60%
time used=429.4s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='0', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_xlnet_chinese_large', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_story': 0, 'news_world': 1, 'news_agriculture': 2, 'news_finance': 3, 'news_travel': 4, 'news_house': 5, 'news_military': 6, 'news_sports': 7, 'news_edu': 8, 'news_culture': 9, 'news_game': 10, 'news_entertainment': 11, 'news_stock': 12, 'news_car': 13, 'news_tech': 14}
index_labels_dict={0: 'news_story', 1: 'news_world', 2: 'news_agriculture', 3: 'news_finance', 4: 'news_travel', 5: 'news_house', 6: 'news_military', 7: 'news_sports', 8: 'news_edu', 9: 'news_culture', 10: 'news_game', 11: 'news_entertainment', 12: 'news_stock', 13: 'news_car', 14: 'news_tech'}
max_sent_len=105
105	1
54	1
47	1
46	3
44	1
43	2
42	3
41	4
40	5
39	14
38	20
37	32
36	96
35	57
34	83
33	145
32	257
31	391
30	625
29	940
28	1251
27	1697
26	1930
25	2244
24	2534
23	2581
22	2778
21	2786
20	2691
19	2937
18	2866
17	2657
16	2646
15	2418
14	2441
13	2154
12	2042
11	1791
10	1280
9	892
8	454
7	195
6	63
5	12
4	2
3	1
max_sent_len=40
max_sent_len=40
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/clue_xlnet_chinese_large were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']
- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): XLNetModel(
    (word_embedding): Embedding(32000, 1024)
    (layer): ModuleList(
      (0): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (6): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (7): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (8): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (9): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (10): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (11): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (12): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (13): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (14): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (15): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (16): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (17): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (18): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (19): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (20): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (21): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (22): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (23): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=1024, out_features=15, bias=True)
)
pretrained_model.mask_emb False
pretrained_model.word_embedding.weight False
pretrained_model.layer.0.rel_attn.q False
pretrained_model.layer.0.rel_attn.k False
pretrained_model.layer.0.rel_attn.v False
pretrained_model.layer.0.rel_attn.o False
pretrained_model.layer.0.rel_attn.r False
pretrained_model.layer.0.rel_attn.r_r_bias False
pretrained_model.layer.0.rel_attn.r_s_bias False
pretrained_model.layer.0.rel_attn.r_w_bias False
pretrained_model.layer.0.rel_attn.seg_embed False
pretrained_model.layer.0.rel_attn.layer_norm.weight False
pretrained_model.layer.0.rel_attn.layer_norm.bias False
pretrained_model.layer.0.ff.layer_norm.weight False
pretrained_model.layer.0.ff.layer_norm.bias False
pretrained_model.layer.0.ff.layer_1.weight False
pretrained_model.layer.0.ff.layer_1.bias False
pretrained_model.layer.0.ff.layer_2.weight False
pretrained_model.layer.0.ff.layer_2.bias False
pretrained_model.layer.1.rel_attn.q False
pretrained_model.layer.1.rel_attn.k False
pretrained_model.layer.1.rel_attn.v False
pretrained_model.layer.1.rel_attn.o False
pretrained_model.layer.1.rel_attn.r False
pretrained_model.layer.1.rel_attn.r_r_bias False
pretrained_model.layer.1.rel_attn.r_s_bias False
pretrained_model.layer.1.rel_attn.r_w_bias False
pretrained_model.layer.1.rel_attn.seg_embed False
pretrained_model.layer.1.rel_attn.layer_norm.weight False
pretrained_model.layer.1.rel_attn.layer_norm.bias False
pretrained_model.layer.1.ff.layer_norm.weight False
pretrained_model.layer.1.ff.layer_norm.bias False
pretrained_model.layer.1.ff.layer_1.weight False
pretrained_model.layer.1.ff.layer_1.bias False
pretrained_model.layer.1.ff.layer_2.weight False
pretrained_model.layer.1.ff.layer_2.bias False
pretrained_model.layer.2.rel_attn.q False
pretrained_model.layer.2.rel_attn.k False
pretrained_model.layer.2.rel_attn.v False
pretrained_model.layer.2.rel_attn.o False
pretrained_model.layer.2.rel_attn.r False
pretrained_model.layer.2.rel_attn.r_r_bias False
pretrained_model.layer.2.rel_attn.r_s_bias False
pretrained_model.layer.2.rel_attn.r_w_bias False
pretrained_model.layer.2.rel_attn.seg_embed False
pretrained_model.layer.2.rel_attn.layer_norm.weight False
pretrained_model.layer.2.rel_attn.layer_norm.bias False
pretrained_model.layer.2.ff.layer_norm.weight False
pretrained_model.layer.2.ff.layer_norm.bias False
pretrained_model.layer.2.ff.layer_1.weight False
pretrained_model.layer.2.ff.layer_1.bias False
pretrained_model.layer.2.ff.layer_2.weight False
pretrained_model.layer.2.ff.layer_2.bias False
pretrained_model.layer.3.rel_attn.q False
pretrained_model.layer.3.rel_attn.k False
pretrained_model.layer.3.rel_attn.v False
pretrained_model.layer.3.rel_attn.o False
pretrained_model.layer.3.rel_attn.r False
pretrained_model.layer.3.rel_attn.r_r_bias False
pretrained_model.layer.3.rel_attn.r_s_bias False
pretrained_model.layer.3.rel_attn.r_w_bias False
pretrained_model.layer.3.rel_attn.seg_embed False
pretrained_model.layer.3.rel_attn.layer_norm.weight False
pretrained_model.layer.3.rel_attn.layer_norm.bias False
pretrained_model.layer.3.ff.layer_norm.weight False
pretrained_model.layer.3.ff.layer_norm.bias False
pretrained_model.layer.3.ff.layer_1.weight False
pretrained_model.layer.3.ff.layer_1.bias False
pretrained_model.layer.3.ff.layer_2.weight False
pretrained_model.layer.3.ff.layer_2.bias False
pretrained_model.layer.4.rel_attn.q False
pretrained_model.layer.4.rel_attn.k False
pretrained_model.layer.4.rel_attn.v False
pretrained_model.layer.4.rel_attn.o False
pretrained_model.layer.4.rel_attn.r False
pretrained_model.layer.4.rel_attn.r_r_bias False
pretrained_model.layer.4.rel_attn.r_s_bias False
pretrained_model.layer.4.rel_attn.r_w_bias False
pretrained_model.layer.4.rel_attn.seg_embed False
pretrained_model.layer.4.rel_attn.layer_norm.weight False
pretrained_model.layer.4.rel_attn.layer_norm.bias False
pretrained_model.layer.4.ff.layer_norm.weight False
pretrained_model.layer.4.ff.layer_norm.bias False
pretrained_model.layer.4.ff.layer_1.weight False
pretrained_model.layer.4.ff.layer_1.bias False
pretrained_model.layer.4.ff.layer_2.weight False
pretrained_model.layer.4.ff.layer_2.bias False
pretrained_model.layer.5.rel_attn.q False
pretrained_model.layer.5.rel_attn.k False
pretrained_model.layer.5.rel_attn.v False
pretrained_model.layer.5.rel_attn.o False
pretrained_model.layer.5.rel_attn.r False
pretrained_model.layer.5.rel_attn.r_r_bias False
pretrained_model.layer.5.rel_attn.r_s_bias False
pretrained_model.layer.5.rel_attn.r_w_bias False
pretrained_model.layer.5.rel_attn.seg_embed False
pretrained_model.layer.5.rel_attn.layer_norm.weight False
pretrained_model.layer.5.rel_attn.layer_norm.bias False
pretrained_model.layer.5.ff.layer_norm.weight False
pretrained_model.layer.5.ff.layer_norm.bias False
pretrained_model.layer.5.ff.layer_1.weight False
pretrained_model.layer.5.ff.layer_1.bias False
pretrained_model.layer.5.ff.layer_2.weight False
pretrained_model.layer.5.ff.layer_2.bias False
pretrained_model.layer.6.rel_attn.q False
pretrained_model.layer.6.rel_attn.k False
pretrained_model.layer.6.rel_attn.v False
pretrained_model.layer.6.rel_attn.o False
pretrained_model.layer.6.rel_attn.r False
pretrained_model.layer.6.rel_attn.r_r_bias False
pretrained_model.layer.6.rel_attn.r_s_bias False
pretrained_model.layer.6.rel_attn.r_w_bias False
pretrained_model.layer.6.rel_attn.seg_embed False
pretrained_model.layer.6.rel_attn.layer_norm.weight False
pretrained_model.layer.6.rel_attn.layer_norm.bias False
pretrained_model.layer.6.ff.layer_norm.weight False
pretrained_model.layer.6.ff.layer_norm.bias False
pretrained_model.layer.6.ff.layer_1.weight False
pretrained_model.layer.6.ff.layer_1.bias False
pretrained_model.layer.6.ff.layer_2.weight False
pretrained_model.layer.6.ff.layer_2.bias False
pretrained_model.layer.7.rel_attn.q False
pretrained_model.layer.7.rel_attn.k False
pretrained_model.layer.7.rel_attn.v False
pretrained_model.layer.7.rel_attn.o False
pretrained_model.layer.7.rel_attn.r False
pretrained_model.layer.7.rel_attn.r_r_bias False
pretrained_model.layer.7.rel_attn.r_s_bias False
pretrained_model.layer.7.rel_attn.r_w_bias False
pretrained_model.layer.7.rel_attn.seg_embed False
pretrained_model.layer.7.rel_attn.layer_norm.weight False
pretrained_model.layer.7.rel_attn.layer_norm.bias False
pretrained_model.layer.7.ff.layer_norm.weight False
pretrained_model.layer.7.ff.layer_norm.bias False
pretrained_model.layer.7.ff.layer_1.weight False
pretrained_model.layer.7.ff.layer_1.bias False
pretrained_model.layer.7.ff.layer_2.weight False
pretrained_model.layer.7.ff.layer_2.bias False
pretrained_model.layer.8.rel_attn.q False
pretrained_model.layer.8.rel_attn.k False
pretrained_model.layer.8.rel_attn.v False
pretrained_model.layer.8.rel_attn.o False
pretrained_model.layer.8.rel_attn.r False
pretrained_model.layer.8.rel_attn.r_r_bias False
pretrained_model.layer.8.rel_attn.r_s_bias False
pretrained_model.layer.8.rel_attn.r_w_bias False
pretrained_model.layer.8.rel_attn.seg_embed False
pretrained_model.layer.8.rel_attn.layer_norm.weight False
pretrained_model.layer.8.rel_attn.layer_norm.bias False
pretrained_model.layer.8.ff.layer_norm.weight False
pretrained_model.layer.8.ff.layer_norm.bias False
pretrained_model.layer.8.ff.layer_1.weight False
pretrained_model.layer.8.ff.layer_1.bias False
pretrained_model.layer.8.ff.layer_2.weight False
pretrained_model.layer.8.ff.layer_2.bias False
pretrained_model.layer.9.rel_attn.q False
pretrained_model.layer.9.rel_attn.k False
pretrained_model.layer.9.rel_attn.v False
pretrained_model.layer.9.rel_attn.o False
pretrained_model.layer.9.rel_attn.r False
pretrained_model.layer.9.rel_attn.r_r_bias False
pretrained_model.layer.9.rel_attn.r_s_bias False
pretrained_model.layer.9.rel_attn.r_w_bias False
pretrained_model.layer.9.rel_attn.seg_embed False
pretrained_model.layer.9.rel_attn.layer_norm.weight False
pretrained_model.layer.9.rel_attn.layer_norm.bias False
pretrained_model.layer.9.ff.layer_norm.weight False
pretrained_model.layer.9.ff.layer_norm.bias False
pretrained_model.layer.9.ff.layer_1.weight False
pretrained_model.layer.9.ff.layer_1.bias False
pretrained_model.layer.9.ff.layer_2.weight False
pretrained_model.layer.9.ff.layer_2.bias False
pretrained_model.layer.10.rel_attn.q False
pretrained_model.layer.10.rel_attn.k False
pretrained_model.layer.10.rel_attn.v False
pretrained_model.layer.10.rel_attn.o False
pretrained_model.layer.10.rel_attn.r False
pretrained_model.layer.10.rel_attn.r_r_bias False
pretrained_model.layer.10.rel_attn.r_s_bias False
pretrained_model.layer.10.rel_attn.r_w_bias False
pretrained_model.layer.10.rel_attn.seg_embed False
pretrained_model.layer.10.rel_attn.layer_norm.weight False
pretrained_model.layer.10.rel_attn.layer_norm.bias False
pretrained_model.layer.10.ff.layer_norm.weight False
pretrained_model.layer.10.ff.layer_norm.bias False
pretrained_model.layer.10.ff.layer_1.weight False
pretrained_model.layer.10.ff.layer_1.bias False
pretrained_model.layer.10.ff.layer_2.weight False
pretrained_model.layer.10.ff.layer_2.bias False
pretrained_model.layer.11.rel_attn.q False
pretrained_model.layer.11.rel_attn.k False
pretrained_model.layer.11.rel_attn.v False
pretrained_model.layer.11.rel_attn.o False
pretrained_model.layer.11.rel_attn.r False
pretrained_model.layer.11.rel_attn.r_r_bias False
pretrained_model.layer.11.rel_attn.r_s_bias False
pretrained_model.layer.11.rel_attn.r_w_bias False
pretrained_model.layer.11.rel_attn.seg_embed False
pretrained_model.layer.11.rel_attn.layer_norm.weight False
pretrained_model.layer.11.rel_attn.layer_norm.bias False
pretrained_model.layer.11.ff.layer_norm.weight False
pretrained_model.layer.11.ff.layer_norm.bias False
pretrained_model.layer.11.ff.layer_1.weight False
pretrained_model.layer.11.ff.layer_1.bias False
pretrained_model.layer.11.ff.layer_2.weight False
pretrained_model.layer.11.ff.layer_2.bias False
pretrained_model.layer.12.rel_attn.q False
pretrained_model.layer.12.rel_attn.k False
pretrained_model.layer.12.rel_attn.v False
pretrained_model.layer.12.rel_attn.o False
pretrained_model.layer.12.rel_attn.r False
pretrained_model.layer.12.rel_attn.r_r_bias False
pretrained_model.layer.12.rel_attn.r_s_bias False
pretrained_model.layer.12.rel_attn.r_w_bias False
pretrained_model.layer.12.rel_attn.seg_embed False
pretrained_model.layer.12.rel_attn.layer_norm.weight False
pretrained_model.layer.12.rel_attn.layer_norm.bias False
pretrained_model.layer.12.ff.layer_norm.weight False
pretrained_model.layer.12.ff.layer_norm.bias False
pretrained_model.layer.12.ff.layer_1.weight False
pretrained_model.layer.12.ff.layer_1.bias False
pretrained_model.layer.12.ff.layer_2.weight False
pretrained_model.layer.12.ff.layer_2.bias False
pretrained_model.layer.13.rel_attn.q False
pretrained_model.layer.13.rel_attn.k False
pretrained_model.layer.13.rel_attn.v False
pretrained_model.layer.13.rel_attn.o False
pretrained_model.layer.13.rel_attn.r False
pretrained_model.layer.13.rel_attn.r_r_bias False
pretrained_model.layer.13.rel_attn.r_s_bias False
pretrained_model.layer.13.rel_attn.r_w_bias False
pretrained_model.layer.13.rel_attn.seg_embed False
pretrained_model.layer.13.rel_attn.layer_norm.weight False
pretrained_model.layer.13.rel_attn.layer_norm.bias False
pretrained_model.layer.13.ff.layer_norm.weight False
pretrained_model.layer.13.ff.layer_norm.bias False
pretrained_model.layer.13.ff.layer_1.weight False
pretrained_model.layer.13.ff.layer_1.bias False
pretrained_model.layer.13.ff.layer_2.weight False
pretrained_model.layer.13.ff.layer_2.bias False
pretrained_model.layer.14.rel_attn.q False
pretrained_model.layer.14.rel_attn.k False
pretrained_model.layer.14.rel_attn.v False
pretrained_model.layer.14.rel_attn.o False
pretrained_model.layer.14.rel_attn.r False
pretrained_model.layer.14.rel_attn.r_r_bias False
pretrained_model.layer.14.rel_attn.r_s_bias False
pretrained_model.layer.14.rel_attn.r_w_bias False
pretrained_model.layer.14.rel_attn.seg_embed False
pretrained_model.layer.14.rel_attn.layer_norm.weight False
pretrained_model.layer.14.rel_attn.layer_norm.bias False
pretrained_model.layer.14.ff.layer_norm.weight False
pretrained_model.layer.14.ff.layer_norm.bias False
pretrained_model.layer.14.ff.layer_1.weight False
pretrained_model.layer.14.ff.layer_1.bias False
pretrained_model.layer.14.ff.layer_2.weight False
pretrained_model.layer.14.ff.layer_2.bias False
pretrained_model.layer.15.rel_attn.q False
pretrained_model.layer.15.rel_attn.k False
pretrained_model.layer.15.rel_attn.v False
pretrained_model.layer.15.rel_attn.o False
pretrained_model.layer.15.rel_attn.r False
pretrained_model.layer.15.rel_attn.r_r_bias False
pretrained_model.layer.15.rel_attn.r_s_bias False
pretrained_model.layer.15.rel_attn.r_w_bias False
pretrained_model.layer.15.rel_attn.seg_embed False
pretrained_model.layer.15.rel_attn.layer_norm.weight False
pretrained_model.layer.15.rel_attn.layer_norm.bias False
pretrained_model.layer.15.ff.layer_norm.weight False
pretrained_model.layer.15.ff.layer_norm.bias False
pretrained_model.layer.15.ff.layer_1.weight False
pretrained_model.layer.15.ff.layer_1.bias False
pretrained_model.layer.15.ff.layer_2.weight False
pretrained_model.layer.15.ff.layer_2.bias False
pretrained_model.layer.16.rel_attn.q False
pretrained_model.layer.16.rel_attn.k False
pretrained_model.layer.16.rel_attn.v False
pretrained_model.layer.16.rel_attn.o False
pretrained_model.layer.16.rel_attn.r False
pretrained_model.layer.16.rel_attn.r_r_bias False
pretrained_model.layer.16.rel_attn.r_s_bias False
pretrained_model.layer.16.rel_attn.r_w_bias False
pretrained_model.layer.16.rel_attn.seg_embed False
pretrained_model.layer.16.rel_attn.layer_norm.weight False
pretrained_model.layer.16.rel_attn.layer_norm.bias False
pretrained_model.layer.16.ff.layer_norm.weight False
pretrained_model.layer.16.ff.layer_norm.bias False
pretrained_model.layer.16.ff.layer_1.weight False
pretrained_model.layer.16.ff.layer_1.bias False
pretrained_model.layer.16.ff.layer_2.weight False
pretrained_model.layer.16.ff.layer_2.bias False
pretrained_model.layer.17.rel_attn.q False
pretrained_model.layer.17.rel_attn.k False
pretrained_model.layer.17.rel_attn.v False
pretrained_model.layer.17.rel_attn.o False
pretrained_model.layer.17.rel_attn.r False
pretrained_model.layer.17.rel_attn.r_r_bias False
pretrained_model.layer.17.rel_attn.r_s_bias False
pretrained_model.layer.17.rel_attn.r_w_bias False
pretrained_model.layer.17.rel_attn.seg_embed False
pretrained_model.layer.17.rel_attn.layer_norm.weight False
pretrained_model.layer.17.rel_attn.layer_norm.bias False
pretrained_model.layer.17.ff.layer_norm.weight False
pretrained_model.layer.17.ff.layer_norm.bias False
pretrained_model.layer.17.ff.layer_1.weight False
pretrained_model.layer.17.ff.layer_1.bias False
pretrained_model.layer.17.ff.layer_2.weight False
pretrained_model.layer.17.ff.layer_2.bias False
pretrained_model.layer.18.rel_attn.q False
pretrained_model.layer.18.rel_attn.k False
pretrained_model.layer.18.rel_attn.v False
pretrained_model.layer.18.rel_attn.o False
pretrained_model.layer.18.rel_attn.r False
pretrained_model.layer.18.rel_attn.r_r_bias False
pretrained_model.layer.18.rel_attn.r_s_bias False
pretrained_model.layer.18.rel_attn.r_w_bias False
pretrained_model.layer.18.rel_attn.seg_embed False
pretrained_model.layer.18.rel_attn.layer_norm.weight False
pretrained_model.layer.18.rel_attn.layer_norm.bias False
pretrained_model.layer.18.ff.layer_norm.weight False
pretrained_model.layer.18.ff.layer_norm.bias False
pretrained_model.layer.18.ff.layer_1.weight False
pretrained_model.layer.18.ff.layer_1.bias False
pretrained_model.layer.18.ff.layer_2.weight False
pretrained_model.layer.18.ff.layer_2.bias False
pretrained_model.layer.19.rel_attn.q False
pretrained_model.layer.19.rel_attn.k False
pretrained_model.layer.19.rel_attn.v False
pretrained_model.layer.19.rel_attn.o False
pretrained_model.layer.19.rel_attn.r False
pretrained_model.layer.19.rel_attn.r_r_bias False
pretrained_model.layer.19.rel_attn.r_s_bias False
pretrained_model.layer.19.rel_attn.r_w_bias False
pretrained_model.layer.19.rel_attn.seg_embed False
pretrained_model.layer.19.rel_attn.layer_norm.weight False
pretrained_model.layer.19.rel_attn.layer_norm.bias False
pretrained_model.layer.19.ff.layer_norm.weight False
pretrained_model.layer.19.ff.layer_norm.bias False
pretrained_model.layer.19.ff.layer_1.weight False
pretrained_model.layer.19.ff.layer_1.bias False
pretrained_model.layer.19.ff.layer_2.weight False
pretrained_model.layer.19.ff.layer_2.bias False
pretrained_model.layer.20.rel_attn.q False
pretrained_model.layer.20.rel_attn.k False
pretrained_model.layer.20.rel_attn.v False
pretrained_model.layer.20.rel_attn.o False
pretrained_model.layer.20.rel_attn.r False
pretrained_model.layer.20.rel_attn.r_r_bias False
pretrained_model.layer.20.rel_attn.r_s_bias False
pretrained_model.layer.20.rel_attn.r_w_bias False
pretrained_model.layer.20.rel_attn.seg_embed False
pretrained_model.layer.20.rel_attn.layer_norm.weight False
pretrained_model.layer.20.rel_attn.layer_norm.bias False
pretrained_model.layer.20.ff.layer_norm.weight False
pretrained_model.layer.20.ff.layer_norm.bias False
pretrained_model.layer.20.ff.layer_1.weight False
pretrained_model.layer.20.ff.layer_1.bias False
pretrained_model.layer.20.ff.layer_2.weight False
pretrained_model.layer.20.ff.layer_2.bias False
pretrained_model.layer.21.rel_attn.q False
pretrained_model.layer.21.rel_attn.k False
pretrained_model.layer.21.rel_attn.v False
pretrained_model.layer.21.rel_attn.o False
pretrained_model.layer.21.rel_attn.r False
pretrained_model.layer.21.rel_attn.r_r_bias False
pretrained_model.layer.21.rel_attn.r_s_bias False
pretrained_model.layer.21.rel_attn.r_w_bias False
pretrained_model.layer.21.rel_attn.seg_embed False
pretrained_model.layer.21.rel_attn.layer_norm.weight False
pretrained_model.layer.21.rel_attn.layer_norm.bias False
pretrained_model.layer.21.ff.layer_norm.weight False
pretrained_model.layer.21.ff.layer_norm.bias False
pretrained_model.layer.21.ff.layer_1.weight False
pretrained_model.layer.21.ff.layer_1.bias False
pretrained_model.layer.21.ff.layer_2.weight False
pretrained_model.layer.21.ff.layer_2.bias False
pretrained_model.layer.22.rel_attn.q False
pretrained_model.layer.22.rel_attn.k False
pretrained_model.layer.22.rel_attn.v False
pretrained_model.layer.22.rel_attn.o False
pretrained_model.layer.22.rel_attn.r False
pretrained_model.layer.22.rel_attn.r_r_bias False
pretrained_model.layer.22.rel_attn.r_s_bias False
pretrained_model.layer.22.rel_attn.r_w_bias False
pretrained_model.layer.22.rel_attn.seg_embed False
pretrained_model.layer.22.rel_attn.layer_norm.weight False
pretrained_model.layer.22.rel_attn.layer_norm.bias False
pretrained_model.layer.22.ff.layer_norm.weight False
pretrained_model.layer.22.ff.layer_norm.bias False
pretrained_model.layer.22.ff.layer_1.weight False
pretrained_model.layer.22.ff.layer_1.bias False
pretrained_model.layer.22.ff.layer_2.weight False
pretrained_model.layer.22.ff.layer_2.bias False
pretrained_model.layer.23.rel_attn.q False
pretrained_model.layer.23.rel_attn.k False
pretrained_model.layer.23.rel_attn.v False
pretrained_model.layer.23.rel_attn.o False
pretrained_model.layer.23.rel_attn.r False
pretrained_model.layer.23.rel_attn.r_r_bias False
pretrained_model.layer.23.rel_attn.r_s_bias False
pretrained_model.layer.23.rel_attn.r_w_bias False
pretrained_model.layer.23.rel_attn.seg_embed False
pretrained_model.layer.23.rel_attn.layer_norm.weight False
pretrained_model.layer.23.rel_attn.layer_norm.bias False
pretrained_model.layer.23.ff.layer_norm.weight False
pretrained_model.layer.23.ff.layer_norm.bias False
pretrained_model.layer.23.ff.layer_1.weight False
pretrained_model.layer.23.ff.layer_1.bias False
pretrained_model.layer.23.ff.layer_2.weight False
pretrained_model.layer.23.ff.layer_2.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 238s, train loss=2.4935, train acc=17.71%, dev loss=2.1669, dev acc=32.95%
saving, test loss=2.1775, test acc=32.19%
epoch: 2/10000, 240s, train loss=2.1593, train acc=30.64%, dev loss=1.9403, dev acc=38.47%
saving, test loss=1.9488, test acc=39.31%
epoch: 3/10000, 240s, train loss=2.0198, train acc=35.22%, dev loss=1.8452, dev acc=40.82%
saving, test loss=1.8514, test acc=41.64%
epoch: 4/10000, 241s, train loss=1.9551, train acc=37.14%, dev loss=1.7967, dev acc=41.96%
saving, test loss=1.8022, test acc=42.73%
epoch: 5/10000, 240s, train loss=1.9162, train acc=38.12%, dev loss=1.7614, dev acc=42.84%
saving, test loss=1.7679, test acc=43.62%
epoch: 6/10000, 241s, train loss=1.8940, train acc=39.21%, dev loss=1.7363, dev acc=43.46%
saving, test loss=1.7433, test acc=44.06%
epoch: 7/10000, 241s, train loss=1.8794, train acc=39.55%, dev loss=1.7137, dev acc=44.00%
saving, test loss=1.7198, test acc=45.11%
epoch: 8/10000, 241s, train loss=1.8683, train acc=39.62%, dev loss=1.7023, dev acc=44.10%
saving, test loss=1.7078, test acc=45.06%
epoch: 9/10000, 241s, train loss=1.8560, train acc=40.24%, dev loss=1.6980, dev acc=44.40%
saving, test loss=1.7028, test acc=45.08%
epoch: 10/10000, 241s, train loss=1.8471, train acc=40.01%, dev loss=1.6894, dev acc=44.43%
saving, test loss=1.6940, test acc=45.24%
epoch: 11/10000, 241s, train loss=1.8412, train acc=40.41%, dev loss=1.6799, dev acc=45.01%
saving, test loss=1.6863, test acc=45.80%
epoch: 12/10000, 241s, train loss=1.8435, train acc=40.19%, dev loss=1.6760, dev acc=44.94%
epoch: 13/10000, 241s, train loss=1.8368, train acc=40.48%, dev loss=1.6666, dev acc=45.01%
epoch: 14/10000, 241s, train loss=1.8402, train acc=40.35%, dev loss=1.6580, dev acc=45.28%
saving, test loss=1.6658, test acc=46.22%
epoch: 15/10000, 241s, train loss=1.8345, train acc=40.75%, dev loss=1.6621, dev acc=45.41%
saving, test loss=1.6715, test acc=45.92%
epoch: 16/10000, 241s, train loss=1.8299, train acc=40.74%, dev loss=1.6593, dev acc=45.13%
epoch: 17/10000, 241s, train loss=1.8287, train acc=40.89%, dev loss=1.6580, dev acc=45.46%
saving, test loss=1.6655, test acc=46.24%
epoch: 18/10000, 241s, train loss=1.8265, train acc=40.81%, dev loss=1.6523, dev acc=45.65%
saving, test loss=1.6590, test acc=46.64%
epoch: 19/10000, 241s, train loss=1.8296, train acc=40.79%, dev loss=1.6497, dev acc=45.99%
saving, test loss=1.6573, test acc=46.34%
epoch: 20/10000, 241s, train loss=1.8302, train acc=40.42%, dev loss=1.6496, dev acc=45.50%
epoch: 21/10000, 241s, train loss=1.8266, train acc=40.82%, dev loss=1.6438, dev acc=45.50%
epoch: 22/10000, 241s, train loss=1.8266, train acc=40.82%, dev loss=1.6453, dev acc=45.91%
epoch: 23/10000, 241s, train loss=1.8262, train acc=40.81%, dev loss=1.6434, dev acc=45.99%
epoch: 24/10000, 241s, train loss=1.8257, train acc=40.72%, dev loss=1.6399, dev acc=45.86%
time used=7194.3s
