nohup: ignoring input
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='1', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-bert-wwm', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_finance': 0, 'news_game': 1, 'news_stock': 2, 'news_agriculture': 3, 'news_culture': 4, 'news_house': 5, 'news_car': 6, 'news_military': 7, 'news_story': 8, 'news_travel': 9, 'news_edu': 10, 'news_entertainment': 11, 'news_world': 12, 'news_tech': 13, 'news_sports': 14}
index_labels_dict={0: 'news_finance', 1: 'news_game', 2: 'news_stock', 3: 'news_agriculture', 4: 'news_culture', 5: 'news_house', 6: 'news_car', 7: 'news_military', 8: 'news_story', 9: 'news_travel', 10: 'news_edu', 11: 'news_entertainment', 12: 'news_world', 13: 'news_tech', 14: 'news_sports'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-bert-wwm were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 69s, train loss=2.3788, train acc=26.12%, dev loss=2.0052, dev acc=46.33%
saving, test loss=2.0147, test acc=45.98%
epoch: 2/10000, 70s, train loss=1.8781, train acc=46.90%, dev loss=1.6680, dev acc=51.97%
saving, test loss=1.6801, test acc=50.97%
epoch: 3/10000, 70s, train loss=1.6512, train acc=50.51%, dev loss=1.5154, dev acc=53.20%
saving, test loss=1.5291, test acc=52.98%
epoch: 4/10000, 70s, train loss=1.5472, train acc=51.47%, dev loss=1.4394, dev acc=53.43%
saving, test loss=1.4539, test acc=53.51%
epoch: 5/10000, 70s, train loss=1.4891, train acc=52.30%, dev loss=1.3972, dev acc=53.56%
saving, test loss=1.4122, test acc=53.86%
epoch: 6/10000, 70s, train loss=1.4581, train acc=52.78%, dev loss=1.3703, dev acc=53.65%
saving, test loss=1.3858, test acc=53.99%
epoch: 7/10000, 70s, train loss=1.4362, train acc=53.00%, dev loss=1.3516, dev acc=53.80%
saving, test loss=1.3673, test acc=54.14%
epoch: 8/10000, 70s, train loss=1.4203, train acc=53.22%, dev loss=1.3387, dev acc=54.14%
saving, test loss=1.3545, test acc=54.34%
epoch: 9/10000, 70s, train loss=1.4129, train acc=53.24%, dev loss=1.3290, dev acc=54.01%
epoch: 10/10000, 70s, train loss=1.3983, train acc=53.52%, dev loss=1.3213, dev acc=54.25%
saving, test loss=1.3379, test acc=54.57%
epoch: 11/10000, 70s, train loss=1.3929, train acc=53.61%, dev loss=1.3150, dev acc=54.35%
saving, test loss=1.3315, test acc=54.71%
epoch: 12/10000, 70s, train loss=1.3914, train acc=53.65%, dev loss=1.3103, dev acc=54.50%
saving, test loss=1.3272, test acc=54.87%
epoch: 13/10000, 70s, train loss=1.3837, train acc=53.70%, dev loss=1.3067, dev acc=54.37%
epoch: 14/10000, 70s, train loss=1.3822, train acc=53.72%, dev loss=1.3032, dev acc=54.42%
epoch: 15/10000, 70s, train loss=1.3780, train acc=53.87%, dev loss=1.3006, dev acc=54.20%
epoch: 16/10000, 70s, train loss=1.3733, train acc=53.96%, dev loss=1.2980, dev acc=54.24%
epoch: 17/10000, 70s, train loss=1.3732, train acc=53.76%, dev loss=1.2954, dev acc=54.37%
time used=1500.8s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='1', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-bert-wwm-ext', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_sports': 0, 'news_travel': 1, 'news_stock': 2, 'news_military': 3, 'news_house': 4, 'news_finance': 5, 'news_game': 6, 'news_car': 7, 'news_agriculture': 8, 'news_entertainment': 9, 'news_edu': 10, 'news_story': 11, 'news_culture': 12, 'news_world': 13, 'news_tech': 14}
index_labels_dict={0: 'news_sports', 1: 'news_travel', 2: 'news_stock', 3: 'news_military', 4: 'news_house', 5: 'news_finance', 6: 'news_game', 7: 'news_car', 8: 'news_agriculture', 9: 'news_entertainment', 10: 'news_edu', 11: 'news_story', 12: 'news_culture', 13: 'news_world', 14: 'news_tech'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-bert-wwm-ext were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 69s, train loss=2.3930, train acc=25.81%, dev loss=2.0642, dev acc=45.43%
saving, test loss=2.0711, test acc=45.26%
epoch: 2/10000, 70s, train loss=1.9287, train acc=46.83%, dev loss=1.7266, dev acc=51.26%
saving, test loss=1.7355, test acc=50.30%
epoch: 3/10000, 70s, train loss=1.6887, train acc=50.43%, dev loss=1.5539, dev acc=52.94%
saving, test loss=1.5644, test acc=52.71%
epoch: 4/10000, 70s, train loss=1.5681, train acc=51.89%, dev loss=1.4627, dev acc=53.67%
saving, test loss=1.4738, test acc=53.65%
epoch: 5/10000, 70s, train loss=1.4998, train acc=52.80%, dev loss=1.4095, dev acc=53.97%
saving, test loss=1.4210, test acc=54.19%
epoch: 6/10000, 70s, train loss=1.4586, train acc=53.32%, dev loss=1.3754, dev acc=54.31%
saving, test loss=1.3875, test acc=54.51%
epoch: 7/10000, 70s, train loss=1.4330, train acc=53.43%, dev loss=1.3520, dev acc=54.65%
saving, test loss=1.3642, test acc=54.96%
epoch: 8/10000, 70s, train loss=1.4135, train acc=53.45%, dev loss=1.3357, dev acc=54.74%
saving, test loss=1.3483, test acc=55.01%
epoch: 9/10000, 70s, train loss=1.4012, train acc=53.78%, dev loss=1.3236, dev acc=55.02%
saving, test loss=1.3367, test acc=55.06%
epoch: 10/10000, 70s, train loss=1.3863, train acc=54.01%, dev loss=1.3144, dev acc=54.93%
epoch: 11/10000, 70s, train loss=1.3815, train acc=54.07%, dev loss=1.3068, dev acc=54.97%
epoch: 12/10000, 70s, train loss=1.3780, train acc=53.92%, dev loss=1.3009, dev acc=55.19%
saving, test loss=1.3142, test acc=55.48%
epoch: 13/10000, 70s, train loss=1.3708, train acc=54.15%, dev loss=1.2958, dev acc=55.21%
saving, test loss=1.3097, test acc=55.42%
epoch: 14/10000, 70s, train loss=1.3665, train acc=54.29%, dev loss=1.2914, dev acc=55.38%
saving, test loss=1.3052, test acc=55.55%
epoch: 15/10000, 70s, train loss=1.3622, train acc=54.41%, dev loss=1.2876, dev acc=55.28%
epoch: 16/10000, 70s, train loss=1.3596, train acc=54.29%, dev loss=1.2844, dev acc=55.32%
epoch: 17/10000, 70s, train loss=1.3554, train acc=54.43%, dev loss=1.2814, dev acc=55.25%
epoch: 18/10000, 70s, train loss=1.3533, train acc=54.53%, dev loss=1.2795, dev acc=55.40%
saving, test loss=1.2939, test acc=55.61%
epoch: 19/10000, 70s, train loss=1.3506, train acc=54.49%, dev loss=1.2771, dev acc=55.43%
saving, test loss=1.2915, test acc=55.60%
epoch: 20/10000, 70s, train loss=1.3519, train acc=54.52%, dev loss=1.2754, dev acc=55.21%
epoch: 21/10000, 70s, train loss=1.3494, train acc=54.72%, dev loss=1.2738, dev acc=55.21%
epoch: 22/10000, 70s, train loss=1.3483, train acc=54.54%, dev loss=1.2721, dev acc=55.34%
epoch: 23/10000, 71s, train loss=1.3433, train acc=54.78%, dev loss=1.2709, dev acc=55.53%
saving, test loss=1.2857, test acc=55.58%
epoch: 24/10000, 71s, train loss=1.3406, train acc=54.82%, dev loss=1.2697, dev acc=55.49%
epoch: 25/10000, 71s, train loss=1.3439, train acc=54.86%, dev loss=1.2692, dev acc=55.49%
epoch: 26/10000, 71s, train loss=1.3402, train acc=55.03%, dev loss=1.2681, dev acc=55.43%
epoch: 27/10000, 71s, train loss=1.3393, train acc=54.80%, dev loss=1.2670, dev acc=55.45%
epoch: 28/10000, 70s, train loss=1.3407, train acc=54.64%, dev loss=1.2664, dev acc=55.40%
time used=2426.2s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='1', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-electra-180g-base-discriminator', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_stock': 0, 'news_entertainment': 1, 'news_tech': 2, 'news_world': 3, 'news_story': 4, 'news_game': 5, 'news_car': 6, 'news_house': 7, 'news_travel': 8, 'news_sports': 9, 'news_finance': 10, 'news_edu': 11, 'news_agriculture': 12, 'news_culture': 13, 'news_military': 14}
index_labels_dict={0: 'news_stock', 1: 'news_entertainment', 2: 'news_tech', 3: 'news_world', 4: 'news_story', 5: 'news_game', 6: 'news_car', 7: 'news_house', 8: 'news_travel', 9: 'news_sports', 10: 'news_finance', 11: 'news_edu', 12: 'news_agriculture', 13: 'news_culture', 14: 'news_military'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-electra-180g-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']
- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 68s, train loss=2.6185, train acc=10.98%, dev loss=2.5410, dev acc=13.96%
saving, test loss=2.5455, test acc=13.59%
epoch: 2/10000, 69s, train loss=2.5302, train acc=16.42%, dev loss=2.4851, dev acc=19.32%
saving, test loss=2.4893, test acc=18.17%
epoch: 3/10000, 69s, train loss=2.4847, train acc=19.09%, dev loss=2.4447, dev acc=20.78%
saving, test loss=2.4489, test acc=19.69%
epoch: 4/10000, 70s, train loss=2.4512, train acc=20.44%, dev loss=2.4118, dev acc=21.87%
saving, test loss=2.4161, test acc=21.01%
epoch: 5/10000, 70s, train loss=2.4228, train acc=21.69%, dev loss=2.3841, dev acc=23.11%
saving, test loss=2.3878, test acc=22.27%
epoch: 6/10000, 70s, train loss=2.3994, train acc=22.56%, dev loss=2.3602, dev acc=24.10%
saving, test loss=2.3636, test acc=23.09%
epoch: 7/10000, 70s, train loss=2.3810, train acc=23.14%, dev loss=2.3388, dev acc=24.87%
saving, test loss=2.3419, test acc=24.06%
epoch: 8/10000, 70s, train loss=2.3606, train acc=23.92%, dev loss=2.3200, dev acc=25.60%
saving, test loss=2.3228, test acc=24.76%
epoch: 9/10000, 70s, train loss=2.3454, train acc=24.46%, dev loss=2.3036, dev acc=26.33%
saving, test loss=2.3057, test acc=25.66%
epoch: 10/10000, 70s, train loss=2.3330, train acc=24.85%, dev loss=2.2884, dev acc=26.65%
saving, test loss=2.2902, test acc=26.19%
epoch: 11/10000, 70s, train loss=2.3207, train acc=25.32%, dev loss=2.2748, dev acc=27.31%
saving, test loss=2.2759, test acc=26.67%
epoch: 12/10000, 70s, train loss=2.3103, train acc=25.50%, dev loss=2.2624, dev acc=27.42%
saving, test loss=2.2635, test acc=27.07%
epoch: 13/10000, 71s, train loss=2.3010, train acc=25.47%, dev loss=2.2512, dev acc=28.07%
saving, test loss=2.2518, test acc=27.44%
epoch: 14/10000, 71s, train loss=2.2928, train acc=26.05%, dev loss=2.2408, dev acc=28.60%
saving, test loss=2.2408, test acc=28.12%
epoch: 15/10000, 71s, train loss=2.2803, train acc=26.51%, dev loss=2.2311, dev acc=28.88%
saving, test loss=2.2310, test acc=28.30%
epoch: 16/10000, 71s, train loss=2.2768, train acc=26.36%, dev loss=2.2220, dev acc=29.18%
saving, test loss=2.2219, test acc=28.69%
epoch: 17/10000, 71s, train loss=2.2679, train acc=26.51%, dev loss=2.2136, dev acc=29.63%
saving, test loss=2.2130, test acc=29.03%
epoch: 18/10000, 71s, train loss=2.2610, train acc=27.05%, dev loss=2.2060, dev acc=29.65%
saving, test loss=2.2052, test acc=29.07%
epoch: 19/10000, 71s, train loss=2.2602, train acc=26.97%, dev loss=2.1987, dev acc=29.89%
saving, test loss=2.1983, test acc=29.39%
epoch: 20/10000, 71s, train loss=2.2530, train acc=27.28%, dev loss=2.1911, dev acc=30.38%
saving, test loss=2.1903, test acc=29.61%
epoch: 21/10000, 71s, train loss=2.2470, train acc=27.18%, dev loss=2.1850, dev acc=30.70%
saving, test loss=2.1843, test acc=29.90%
epoch: 22/10000, 71s, train loss=2.2430, train acc=27.55%, dev loss=2.1791, dev acc=30.85%
saving, test loss=2.1784, test acc=30.13%
epoch: 23/10000, 71s, train loss=2.2404, train acc=27.66%, dev loss=2.1732, dev acc=30.98%
saving, test loss=2.1727, test acc=30.52%
epoch: 24/10000, 71s, train loss=2.2341, train acc=27.75%, dev loss=2.1680, dev acc=31.32%
saving, test loss=2.1671, test acc=30.68%
epoch: 25/10000, 71s, train loss=2.2335, train acc=27.99%, dev loss=2.1628, dev acc=31.47%
saving, test loss=2.1617, test acc=30.95%
epoch: 26/10000, 71s, train loss=2.2286, train acc=27.67%, dev loss=2.1586, dev acc=31.47%
epoch: 27/10000, 71s, train loss=2.2253, train acc=28.07%, dev loss=2.1542, dev acc=31.48%
saving, test loss=2.1530, test acc=31.10%
epoch: 28/10000, 71s, train loss=2.2189, train acc=28.19%, dev loss=2.1498, dev acc=31.65%
saving, test loss=2.1482, test acc=31.13%
epoch: 29/10000, 71s, train loss=2.2197, train acc=27.93%, dev loss=2.1458, dev acc=31.78%
saving, test loss=2.1439, test acc=31.54%
epoch: 30/10000, 71s, train loss=2.2167, train acc=28.29%, dev loss=2.1411, dev acc=31.95%
saving, test loss=2.1396, test acc=31.70%
epoch: 31/10000, 71s, train loss=2.2152, train acc=28.25%, dev loss=2.1378, dev acc=31.93%
epoch: 32/10000, 71s, train loss=2.2134, train acc=28.19%, dev loss=2.1340, dev acc=31.93%
epoch: 33/10000, 71s, train loss=2.2132, train acc=28.20%, dev loss=2.1304, dev acc=32.33%
saving, test loss=2.1286, test acc=32.03%
epoch: 34/10000, 71s, train loss=2.2098, train acc=28.59%, dev loss=2.1278, dev acc=32.25%
epoch: 35/10000, 71s, train loss=2.2043, train acc=28.46%, dev loss=2.1242, dev acc=32.35%
saving, test loss=2.1222, test acc=32.45%
epoch: 36/10000, 71s, train loss=2.2063, train acc=28.70%, dev loss=2.1222, dev acc=32.55%
saving, test loss=2.1201, test acc=32.46%
epoch: 37/10000, 71s, train loss=2.2054, train acc=28.34%, dev loss=2.1194, dev acc=32.63%
saving, test loss=2.1169, test acc=32.60%
epoch: 38/10000, 71s, train loss=2.2006, train acc=28.67%, dev loss=2.1161, dev acc=32.76%
saving, test loss=2.1139, test acc=32.54%
epoch: 39/10000, 71s, train loss=2.1978, train acc=28.96%, dev loss=2.1133, dev acc=32.95%
saving, test loss=2.1112, test acc=32.78%
epoch: 40/10000, 71s, train loss=2.2018, train acc=28.66%, dev loss=2.1115, dev acc=32.85%
epoch: 41/10000, 71s, train loss=2.1960, train acc=28.79%, dev loss=2.1087, dev acc=32.76%
epoch: 42/10000, 71s, train loss=2.1958, train acc=28.94%, dev loss=2.1064, dev acc=32.89%
epoch: 43/10000, 71s, train loss=2.1951, train acc=28.94%, dev loss=2.1039, dev acc=33.04%
saving, test loss=2.1014, test acc=33.21%
epoch: 44/10000, 71s, train loss=2.1976, train acc=28.59%, dev loss=2.1009, dev acc=33.34%
saving, test loss=2.0986, test acc=33.44%
epoch: 45/10000, 71s, train loss=2.1907, train acc=28.99%, dev loss=2.1005, dev acc=32.91%
epoch: 46/10000, 71s, train loss=2.1939, train acc=28.92%, dev loss=2.0983, dev acc=33.10%
epoch: 47/10000, 71s, train loss=2.1910, train acc=29.00%, dev loss=2.0960, dev acc=33.25%
epoch: 48/10000, 71s, train loss=2.1911, train acc=28.94%, dev loss=2.0945, dev acc=33.28%
epoch: 49/10000, 71s, train loss=2.1914, train acc=28.86%, dev loss=2.0931, dev acc=33.41%
saving, test loss=2.0897, test acc=33.41%
epoch: 50/10000, 71s, train loss=2.1843, train acc=29.16%, dev loss=2.0914, dev acc=33.25%
epoch: 51/10000, 71s, train loss=2.1833, train acc=29.05%, dev loss=2.0895, dev acc=33.45%
saving, test loss=2.0861, test acc=33.57%
epoch: 52/10000, 71s, train loss=2.1892, train acc=28.93%, dev loss=2.0874, dev acc=33.73%
saving, test loss=2.0848, test acc=33.57%
epoch: 53/10000, 71s, train loss=2.1869, train acc=28.95%, dev loss=2.0858, dev acc=33.86%
saving, test loss=2.0827, test acc=33.71%
epoch: 54/10000, 71s, train loss=2.1868, train acc=29.15%, dev loss=2.0842, dev acc=33.66%
epoch: 55/10000, 71s, train loss=2.1838, train acc=29.34%, dev loss=2.0828, dev acc=33.64%
epoch: 56/10000, 71s, train loss=2.1845, train acc=29.34%, dev loss=2.0817, dev acc=33.85%
epoch: 57/10000, 71s, train loss=2.1827, train acc=29.44%, dev loss=2.0804, dev acc=33.85%
epoch: 58/10000, 71s, train loss=2.1825, train acc=29.19%, dev loss=2.0801, dev acc=33.58%
time used=5173.0s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='1', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-electra-180g-large-discriminator', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_edu': 0, 'news_tech': 1, 'news_agriculture': 2, 'news_sports': 3, 'news_culture': 4, 'news_entertainment': 5, 'news_military': 6, 'news_story': 7, 'news_stock': 8, 'news_house': 9, 'news_world': 10, 'news_finance': 11, 'news_game': 12, 'news_travel': 13, 'news_car': 14}
index_labels_dict={0: 'news_edu', 1: 'news_tech', 2: 'news_agriculture', 3: 'news_sports', 4: 'news_culture', 5: 'news_entertainment', 6: 'news_military', 7: 'news_story', 8: 'news_stock', 9: 'news_house', 10: 'news_world', 11: 'news_finance', 12: 'news_game', 13: 'news_travel', 14: 'news_car'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-electra-180g-large-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']
- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (12): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (13): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (14): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (15): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (16): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (17): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (18): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (19): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (20): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (21): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (22): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (23): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=1024, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.attention.self.query.weight False
pretrained_model.encoder.layer.12.attention.self.query.bias False
pretrained_model.encoder.layer.12.attention.self.key.weight False
pretrained_model.encoder.layer.12.attention.self.key.bias False
pretrained_model.encoder.layer.12.attention.self.value.weight False
pretrained_model.encoder.layer.12.attention.self.value.bias False
pretrained_model.encoder.layer.12.attention.output.dense.weight False
pretrained_model.encoder.layer.12.attention.output.dense.bias False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.intermediate.dense.weight False
pretrained_model.encoder.layer.12.intermediate.dense.bias False
pretrained_model.encoder.layer.12.output.dense.weight False
pretrained_model.encoder.layer.12.output.dense.bias False
pretrained_model.encoder.layer.12.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.attention.self.query.weight False
pretrained_model.encoder.layer.13.attention.self.query.bias False
pretrained_model.encoder.layer.13.attention.self.key.weight False
pretrained_model.encoder.layer.13.attention.self.key.bias False
pretrained_model.encoder.layer.13.attention.self.value.weight False
pretrained_model.encoder.layer.13.attention.self.value.bias False
pretrained_model.encoder.layer.13.attention.output.dense.weight False
pretrained_model.encoder.layer.13.attention.output.dense.bias False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.intermediate.dense.weight False
pretrained_model.encoder.layer.13.intermediate.dense.bias False
pretrained_model.encoder.layer.13.output.dense.weight False
pretrained_model.encoder.layer.13.output.dense.bias False
pretrained_model.encoder.layer.13.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.attention.self.query.weight False
pretrained_model.encoder.layer.14.attention.self.query.bias False
pretrained_model.encoder.layer.14.attention.self.key.weight False
pretrained_model.encoder.layer.14.attention.self.key.bias False
pretrained_model.encoder.layer.14.attention.self.value.weight False
pretrained_model.encoder.layer.14.attention.self.value.bias False
pretrained_model.encoder.layer.14.attention.output.dense.weight False
pretrained_model.encoder.layer.14.attention.output.dense.bias False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.intermediate.dense.weight False
pretrained_model.encoder.layer.14.intermediate.dense.bias False
pretrained_model.encoder.layer.14.output.dense.weight False
pretrained_model.encoder.layer.14.output.dense.bias False
pretrained_model.encoder.layer.14.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.attention.self.query.weight False
pretrained_model.encoder.layer.15.attention.self.query.bias False
pretrained_model.encoder.layer.15.attention.self.key.weight False
pretrained_model.encoder.layer.15.attention.self.key.bias False
pretrained_model.encoder.layer.15.attention.self.value.weight False
pretrained_model.encoder.layer.15.attention.self.value.bias False
pretrained_model.encoder.layer.15.attention.output.dense.weight False
pretrained_model.encoder.layer.15.attention.output.dense.bias False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.intermediate.dense.weight False
pretrained_model.encoder.layer.15.intermediate.dense.bias False
pretrained_model.encoder.layer.15.output.dense.weight False
pretrained_model.encoder.layer.15.output.dense.bias False
pretrained_model.encoder.layer.15.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.attention.self.query.weight False
pretrained_model.encoder.layer.16.attention.self.query.bias False
pretrained_model.encoder.layer.16.attention.self.key.weight False
pretrained_model.encoder.layer.16.attention.self.key.bias False
pretrained_model.encoder.layer.16.attention.self.value.weight False
pretrained_model.encoder.layer.16.attention.self.value.bias False
pretrained_model.encoder.layer.16.attention.output.dense.weight False
pretrained_model.encoder.layer.16.attention.output.dense.bias False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.intermediate.dense.weight False
pretrained_model.encoder.layer.16.intermediate.dense.bias False
pretrained_model.encoder.layer.16.output.dense.weight False
pretrained_model.encoder.layer.16.output.dense.bias False
pretrained_model.encoder.layer.16.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.attention.self.query.weight False
pretrained_model.encoder.layer.17.attention.self.query.bias False
pretrained_model.encoder.layer.17.attention.self.key.weight False
pretrained_model.encoder.layer.17.attention.self.key.bias False
pretrained_model.encoder.layer.17.attention.self.value.weight False
pretrained_model.encoder.layer.17.attention.self.value.bias False
pretrained_model.encoder.layer.17.attention.output.dense.weight False
pretrained_model.encoder.layer.17.attention.output.dense.bias False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.intermediate.dense.weight False
pretrained_model.encoder.layer.17.intermediate.dense.bias False
pretrained_model.encoder.layer.17.output.dense.weight False
pretrained_model.encoder.layer.17.output.dense.bias False
pretrained_model.encoder.layer.17.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.attention.self.query.weight False
pretrained_model.encoder.layer.18.attention.self.query.bias False
pretrained_model.encoder.layer.18.attention.self.key.weight False
pretrained_model.encoder.layer.18.attention.self.key.bias False
pretrained_model.encoder.layer.18.attention.self.value.weight False
pretrained_model.encoder.layer.18.attention.self.value.bias False
pretrained_model.encoder.layer.18.attention.output.dense.weight False
pretrained_model.encoder.layer.18.attention.output.dense.bias False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.intermediate.dense.weight False
pretrained_model.encoder.layer.18.intermediate.dense.bias False
pretrained_model.encoder.layer.18.output.dense.weight False
pretrained_model.encoder.layer.18.output.dense.bias False
pretrained_model.encoder.layer.18.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.attention.self.query.weight False
pretrained_model.encoder.layer.19.attention.self.query.bias False
pretrained_model.encoder.layer.19.attention.self.key.weight False
pretrained_model.encoder.layer.19.attention.self.key.bias False
pretrained_model.encoder.layer.19.attention.self.value.weight False
pretrained_model.encoder.layer.19.attention.self.value.bias False
pretrained_model.encoder.layer.19.attention.output.dense.weight False
pretrained_model.encoder.layer.19.attention.output.dense.bias False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.intermediate.dense.weight False
pretrained_model.encoder.layer.19.intermediate.dense.bias False
pretrained_model.encoder.layer.19.output.dense.weight False
pretrained_model.encoder.layer.19.output.dense.bias False
pretrained_model.encoder.layer.19.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.attention.self.query.weight False
pretrained_model.encoder.layer.20.attention.self.query.bias False
pretrained_model.encoder.layer.20.attention.self.key.weight False
pretrained_model.encoder.layer.20.attention.self.key.bias False
pretrained_model.encoder.layer.20.attention.self.value.weight False
pretrained_model.encoder.layer.20.attention.self.value.bias False
pretrained_model.encoder.layer.20.attention.output.dense.weight False
pretrained_model.encoder.layer.20.attention.output.dense.bias False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.intermediate.dense.weight False
pretrained_model.encoder.layer.20.intermediate.dense.bias False
pretrained_model.encoder.layer.20.output.dense.weight False
pretrained_model.encoder.layer.20.output.dense.bias False
pretrained_model.encoder.layer.20.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.attention.self.query.weight False
pretrained_model.encoder.layer.21.attention.self.query.bias False
pretrained_model.encoder.layer.21.attention.self.key.weight False
pretrained_model.encoder.layer.21.attention.self.key.bias False
pretrained_model.encoder.layer.21.attention.self.value.weight False
pretrained_model.encoder.layer.21.attention.self.value.bias False
pretrained_model.encoder.layer.21.attention.output.dense.weight False
pretrained_model.encoder.layer.21.attention.output.dense.bias False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.intermediate.dense.weight False
pretrained_model.encoder.layer.21.intermediate.dense.bias False
pretrained_model.encoder.layer.21.output.dense.weight False
pretrained_model.encoder.layer.21.output.dense.bias False
pretrained_model.encoder.layer.21.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.attention.self.query.weight False
pretrained_model.encoder.layer.22.attention.self.query.bias False
pretrained_model.encoder.layer.22.attention.self.key.weight False
pretrained_model.encoder.layer.22.attention.self.key.bias False
pretrained_model.encoder.layer.22.attention.self.value.weight False
pretrained_model.encoder.layer.22.attention.self.value.bias False
pretrained_model.encoder.layer.22.attention.output.dense.weight False
pretrained_model.encoder.layer.22.attention.output.dense.bias False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.intermediate.dense.weight False
pretrained_model.encoder.layer.22.intermediate.dense.bias False
pretrained_model.encoder.layer.22.output.dense.weight False
pretrained_model.encoder.layer.22.output.dense.bias False
pretrained_model.encoder.layer.22.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.attention.self.query.weight False
pretrained_model.encoder.layer.23.attention.self.query.bias False
pretrained_model.encoder.layer.23.attention.self.key.weight False
pretrained_model.encoder.layer.23.attention.self.key.bias False
pretrained_model.encoder.layer.23.attention.self.value.weight False
pretrained_model.encoder.layer.23.attention.self.value.bias False
pretrained_model.encoder.layer.23.attention.output.dense.weight False
pretrained_model.encoder.layer.23.attention.output.dense.bias False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.intermediate.dense.weight False
pretrained_model.encoder.layer.23.intermediate.dense.bias False
pretrained_model.encoder.layer.23.output.dense.weight False
pretrained_model.encoder.layer.23.output.dense.bias False
pretrained_model.encoder.layer.23.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.output.LayerNorm.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 222s, train loss=2.6337, train acc=12.17%, dev loss=2.4742, dev acc=18.74%
saving, test loss=2.4844, test acc=17.80%
epoch: 2/10000, 223s, train loss=2.4797, train acc=18.28%, dev loss=2.3957, dev acc=21.31%
saving, test loss=2.4081, test acc=20.16%
epoch: 3/10000, 223s, train loss=2.4127, train acc=21.06%, dev loss=2.3399, dev acc=23.76%
saving, test loss=2.3524, test acc=22.96%
epoch: 4/10000, 224s, train loss=2.3665, train acc=22.94%, dev loss=2.3004, dev acc=25.47%
saving, test loss=2.3120, test acc=24.89%
epoch: 5/10000, 223s, train loss=2.3372, train acc=23.92%, dev loss=2.2705, dev acc=27.23%
saving, test loss=2.2810, test acc=26.56%
epoch: 6/10000, 224s, train loss=2.3108, train acc=25.12%, dev loss=2.2457, dev acc=28.84%
saving, test loss=2.2552, test acc=28.04%
epoch: 7/10000, 224s, train loss=2.2949, train acc=25.73%, dev loss=2.2250, dev acc=29.84%
saving, test loss=2.2337, test acc=29.05%
epoch: 8/10000, 224s, train loss=2.2746, train acc=26.02%, dev loss=2.2071, dev acc=30.58%
saving, test loss=2.2154, test acc=29.85%
epoch: 9/10000, 224s, train loss=2.2636, train acc=26.91%, dev loss=2.1927, dev acc=30.94%
saving, test loss=2.2000, test acc=30.06%
epoch: 10/10000, 224s, train loss=2.2507, train acc=27.00%, dev loss=2.1788, dev acc=31.84%
saving, test loss=2.1859, test acc=30.43%
epoch: 11/10000, 224s, train loss=2.2445, train acc=27.21%, dev loss=2.1691, dev acc=32.12%
saving, test loss=2.1755, test acc=31.27%
epoch: 12/10000, 224s, train loss=2.2363, train acc=27.51%, dev loss=2.1581, dev acc=32.35%
saving, test loss=2.1633, test acc=31.77%
epoch: 13/10000, 225s, train loss=2.2280, train acc=28.25%, dev loss=2.1513, dev acc=32.68%
saving, test loss=2.1567, test acc=32.07%
epoch: 14/10000, 225s, train loss=2.2265, train acc=28.03%, dev loss=2.1437, dev acc=32.76%
saving, test loss=2.1489, test acc=32.04%
epoch: 15/10000, 225s, train loss=2.2205, train acc=27.98%, dev loss=2.1357, dev acc=32.74%
epoch: 16/10000, 225s, train loss=2.2159, train acc=28.44%, dev loss=2.1303, dev acc=33.06%
saving, test loss=2.1336, test acc=32.69%
epoch: 17/10000, 225s, train loss=2.2141, train acc=28.35%, dev loss=2.1224, dev acc=33.60%
saving, test loss=2.1252, test acc=32.86%
epoch: 18/10000, 224s, train loss=2.2125, train acc=28.52%, dev loss=2.1178, dev acc=33.68%
saving, test loss=2.1201, test acc=32.98%
epoch: 19/10000, 224s, train loss=2.2080, train acc=28.71%, dev loss=2.1142, dev acc=33.64%
epoch: 20/10000, 224s, train loss=2.2050, train acc=28.47%, dev loss=2.1081, dev acc=33.81%
saving, test loss=2.1103, test acc=33.35%
epoch: 21/10000, 224s, train loss=2.2023, train acc=28.61%, dev loss=2.1039, dev acc=33.86%
saving, test loss=2.1062, test acc=33.47%
epoch: 22/10000, 225s, train loss=2.2020, train acc=28.82%, dev loss=2.1019, dev acc=34.13%
saving, test loss=2.1038, test acc=33.70%
epoch: 23/10000, 224s, train loss=2.1985, train acc=28.75%, dev loss=2.0980, dev acc=34.30%
saving, test loss=2.1001, test acc=33.73%
epoch: 24/10000, 224s, train loss=2.2008, train acc=28.69%, dev loss=2.0945, dev acc=34.35%
saving, test loss=2.0971, test acc=33.84%
epoch: 25/10000, 224s, train loss=2.2012, train acc=28.72%, dev loss=2.0948, dev acc=34.56%
saving, test loss=2.0973, test acc=33.87%
epoch: 26/10000, 224s, train loss=2.1977, train acc=28.63%, dev loss=2.0906, dev acc=34.58%
saving, test loss=2.0924, test acc=33.80%
epoch: 27/10000, 224s, train loss=2.1938, train acc=29.00%, dev loss=2.0862, dev acc=34.33%
epoch: 28/10000, 225s, train loss=2.1941, train acc=28.85%, dev loss=2.0853, dev acc=34.50%
epoch: 29/10000, 225s, train loss=2.1982, train acc=28.74%, dev loss=2.0822, dev acc=34.52%
epoch: 30/10000, 225s, train loss=2.1893, train acc=28.90%, dev loss=2.0804, dev acc=34.67%
saving, test loss=2.0822, test acc=34.25%
epoch: 31/10000, 225s, train loss=2.1925, train acc=28.76%, dev loss=2.0779, dev acc=34.65%
epoch: 32/10000, 225s, train loss=2.1893, train acc=29.07%, dev loss=2.0762, dev acc=34.67%
epoch: 33/10000, 275s, train loss=2.1962, train acc=28.90%, dev loss=2.0752, dev acc=34.67%
epoch: 34/10000, 262s, train loss=2.1890, train acc=29.13%, dev loss=2.0740, dev acc=34.84%
saving, test loss=2.0756, test acc=34.57%
epoch: 35/10000, 223s, train loss=2.1922, train acc=28.83%, dev loss=2.0714, dev acc=34.78%
epoch: 36/10000, 255s, train loss=2.1939, train acc=29.18%, dev loss=2.0712, dev acc=34.97%
saving, test loss=2.0729, test acc=34.62%
epoch: 37/10000, 294s, train loss=2.1949, train acc=28.96%, dev loss=2.0709, dev acc=35.10%
saving, test loss=2.0716, test acc=34.78%
epoch: 38/10000, 310s, train loss=2.1922, train acc=29.08%, dev loss=2.0676, dev acc=35.03%
epoch: 39/10000, 300s, train loss=2.1897, train acc=28.96%, dev loss=2.0676, dev acc=34.91%
epoch: 40/10000, 302s, train loss=2.1857, train acc=29.11%, dev loss=2.0683, dev acc=34.69%
epoch: 41/10000, 301s, train loss=2.1828, train acc=28.92%, dev loss=2.0662, dev acc=35.01%
epoch: 42/10000, 298s, train loss=2.1883, train acc=29.05%, dev loss=2.0650, dev acc=35.06%
time used=12410.9s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='1', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-electra-180g-small-discriminator', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_edu': 0, 'news_travel': 1, 'news_house': 2, 'news_entertainment': 3, 'news_culture': 4, 'news_car': 5, 'news_finance': 6, 'news_sports': 7, 'news_agriculture': 8, 'news_tech': 9, 'news_world': 10, 'news_stock': 11, 'news_game': 12, 'news_military': 13, 'news_story': 14}
index_labels_dict={0: 'news_edu', 1: 'news_travel', 2: 'news_house', 3: 'news_entertainment', 4: 'news_culture', 5: 'news_car', 6: 'news_finance', 7: 'news_sports', 8: 'news_agriculture', 9: 'news_tech', 10: 'news_world', 11: 'news_stock', 12: 'news_game', 13: 'news_military', 14: 'news_story'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-electra-180g-small-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight']
- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(21128, 128, padding_idx=0)
      (position_embeddings): Embedding(512, 128)
      (token_type_embeddings): Embedding(2, 128)
      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=256, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.embeddings_project.weight False
pretrained_model.embeddings_project.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 21s, train loss=2.6395, train acc=10.14%, dev loss=2.5716, dev acc=11.24%
saving, test loss=2.5783, test acc=11.07%
epoch: 2/10000, 21s, train loss=2.5753, train acc=12.84%, dev loss=2.5334, dev acc=15.22%
saving, test loss=2.5417, test acc=14.51%
epoch: 3/10000, 21s, train loss=2.5451, train acc=15.08%, dev loss=2.5038, dev acc=17.82%
saving, test loss=2.5121, test acc=17.93%
epoch: 4/10000, 21s, train loss=2.5204, train acc=16.67%, dev loss=2.4799, dev acc=19.68%
saving, test loss=2.4890, test acc=19.43%
epoch: 5/10000, 19s, train loss=2.5003, train acc=18.17%, dev loss=2.4600, dev acc=20.71%
saving, test loss=2.4693, test acc=20.10%
epoch: 6/10000, 21s, train loss=2.4836, train acc=18.94%, dev loss=2.4418, dev acc=21.63%
saving, test loss=2.4514, test acc=20.83%
epoch: 7/10000, 21s, train loss=2.4697, train acc=19.52%, dev loss=2.4260, dev acc=22.21%
saving, test loss=2.4359, test acc=21.47%
epoch: 8/10000, 21s, train loss=2.4557, train acc=20.16%, dev loss=2.4117, dev acc=22.77%
saving, test loss=2.4217, test acc=21.86%
epoch: 9/10000, 21s, train loss=2.4477, train acc=20.52%, dev loss=2.3991, dev acc=23.14%
saving, test loss=2.4093, test acc=22.29%
epoch: 10/10000, 21s, train loss=2.4378, train acc=20.64%, dev loss=2.3880, dev acc=23.54%
saving, test loss=2.3984, test acc=22.80%
epoch: 11/10000, 21s, train loss=2.4277, train acc=21.15%, dev loss=2.3772, dev acc=24.01%
saving, test loss=2.3875, test acc=23.27%
epoch: 12/10000, 21s, train loss=2.4221, train acc=21.13%, dev loss=2.3675, dev acc=24.46%
saving, test loss=2.3775, test acc=23.87%
epoch: 13/10000, 21s, train loss=2.4135, train acc=21.22%, dev loss=2.3586, dev acc=24.57%
saving, test loss=2.3686, test acc=24.09%
epoch: 14/10000, 17s, train loss=2.4073, train acc=21.71%, dev loss=2.3500, dev acc=24.91%
saving, test loss=2.3597, test acc=24.40%
epoch: 15/10000, 21s, train loss=2.4048, train acc=21.56%, dev loss=2.3424, dev acc=24.87%
epoch: 16/10000, 19s, train loss=2.3990, train acc=21.86%, dev loss=2.3358, dev acc=25.09%
saving, test loss=2.3453, test acc=24.91%
epoch: 17/10000, 20s, train loss=2.3938, train acc=21.97%, dev loss=2.3286, dev acc=25.37%
saving, test loss=2.3383, test acc=25.16%
epoch: 18/10000, 20s, train loss=2.3928, train acc=21.73%, dev loss=2.3223, dev acc=25.86%
saving, test loss=2.3319, test acc=25.31%
epoch: 19/10000, 20s, train loss=2.3867, train acc=22.00%, dev loss=2.3173, dev acc=26.03%
saving, test loss=2.3267, test acc=25.53%
epoch: 20/10000, 17s, train loss=2.3820, train acc=22.33%, dev loss=2.3117, dev acc=26.20%
saving, test loss=2.3211, test acc=25.64%
epoch: 21/10000, 21s, train loss=2.3789, train acc=22.13%, dev loss=2.3067, dev acc=26.50%
saving, test loss=2.3160, test acc=25.95%
epoch: 22/10000, 20s, train loss=2.3783, train acc=22.26%, dev loss=2.3016, dev acc=26.59%
saving, test loss=2.3107, test acc=26.08%
epoch: 23/10000, 18s, train loss=2.3700, train acc=22.74%, dev loss=2.2971, dev acc=26.89%
saving, test loss=2.3063, test acc=26.18%
epoch: 24/10000, 20s, train loss=2.3695, train acc=22.74%, dev loss=2.2929, dev acc=27.17%
saving, test loss=2.3020, test acc=26.29%
epoch: 25/10000, 21s, train loss=2.3697, train acc=22.54%, dev loss=2.2878, dev acc=27.19%
saving, test loss=2.2968, test acc=26.42%
epoch: 26/10000, 21s, train loss=2.3662, train acc=22.67%, dev loss=2.2847, dev acc=27.08%
epoch: 27/10000, 17s, train loss=2.3658, train acc=22.78%, dev loss=2.2815, dev acc=27.47%
saving, test loss=2.2905, test acc=26.56%
epoch: 28/10000, 20s, train loss=2.3628, train acc=22.97%, dev loss=2.2775, dev acc=27.25%
epoch: 29/10000, 21s, train loss=2.3609, train acc=22.91%, dev loss=2.2750, dev acc=27.32%
epoch: 30/10000, 20s, train loss=2.3605, train acc=22.89%, dev loss=2.2722, dev acc=27.57%
saving, test loss=2.2811, test acc=26.97%
epoch: 31/10000, 20s, train loss=2.3593, train acc=22.98%, dev loss=2.2690, dev acc=27.72%
saving, test loss=2.2781, test acc=26.90%
epoch: 32/10000, 21s, train loss=2.3568, train acc=23.07%, dev loss=2.2662, dev acc=27.79%
saving, test loss=2.2752, test acc=27.13%
epoch: 33/10000, 20s, train loss=2.3562, train acc=23.12%, dev loss=2.2639, dev acc=27.75%
epoch: 34/10000, 17s, train loss=2.3546, train acc=23.25%, dev loss=2.2611, dev acc=27.81%
saving, test loss=2.2700, test acc=27.31%
epoch: 35/10000, 20s, train loss=2.3527, train acc=23.38%, dev loss=2.2596, dev acc=27.83%
saving, test loss=2.2683, test acc=27.15%
epoch: 36/10000, 20s, train loss=2.3548, train acc=23.38%, dev loss=2.2573, dev acc=27.94%
saving, test loss=2.2659, test acc=27.31%
epoch: 37/10000, 20s, train loss=2.3505, train acc=23.26%, dev loss=2.2550, dev acc=28.00%
saving, test loss=2.2635, test acc=27.44%
epoch: 38/10000, 21s, train loss=2.3531, train acc=23.15%, dev loss=2.2531, dev acc=28.11%
saving, test loss=2.2614, test acc=27.36%
epoch: 39/10000, 18s, train loss=2.3507, train acc=23.18%, dev loss=2.2513, dev acc=28.15%
saving, test loss=2.2599, test acc=27.40%
epoch: 40/10000, 21s, train loss=2.3487, train acc=23.20%, dev loss=2.2493, dev acc=28.24%
saving, test loss=2.2578, test acc=27.42%
epoch: 41/10000, 21s, train loss=2.3461, train acc=23.42%, dev loss=2.2479, dev acc=28.11%
epoch: 42/10000, 21s, train loss=2.3455, train acc=23.27%, dev loss=2.2456, dev acc=28.26%
saving, test loss=2.2540, test acc=27.54%
epoch: 43/10000, 21s, train loss=2.3418, train acc=23.57%, dev loss=2.2449, dev acc=28.35%
saving, test loss=2.2534, test acc=27.58%
epoch: 44/10000, 21s, train loss=2.3462, train acc=23.15%, dev loss=2.2437, dev acc=28.39%
saving, test loss=2.2522, test acc=27.64%
epoch: 45/10000, 21s, train loss=2.3446, train acc=23.47%, dev loss=2.2418, dev acc=28.54%
saving, test loss=2.2503, test acc=27.65%
epoch: 46/10000, 18s, train loss=2.3447, train acc=23.32%, dev loss=2.2406, dev acc=28.56%
saving, test loss=2.2490, test acc=27.70%
epoch: 47/10000, 21s, train loss=2.3481, train acc=23.20%, dev loss=2.2394, dev acc=28.35%
epoch: 48/10000, 21s, train loss=2.3444, train acc=23.52%, dev loss=2.2381, dev acc=28.69%
saving, test loss=2.2465, test acc=27.85%
epoch: 49/10000, 21s, train loss=2.3429, train acc=23.56%, dev loss=2.2362, dev acc=28.73%
saving, test loss=2.2447, test acc=27.77%
epoch: 50/10000, 21s, train loss=2.3414, train acc=23.34%, dev loss=2.2348, dev acc=28.80%
saving, test loss=2.2434, test acc=27.88%
epoch: 51/10000, 21s, train loss=2.3439, train acc=23.19%, dev loss=2.2339, dev acc=28.86%
saving, test loss=2.2427, test acc=27.87%
epoch: 52/10000, 21s, train loss=2.3420, train acc=23.48%, dev loss=2.2325, dev acc=28.94%
saving, test loss=2.2416, test acc=27.95%
epoch: 53/10000, 21s, train loss=2.3436, train acc=23.61%, dev loss=2.2322, dev acc=28.75%
epoch: 54/10000, 21s, train loss=2.3403, train acc=23.47%, dev loss=2.2311, dev acc=28.92%
epoch: 55/10000, 21s, train loss=2.3438, train acc=23.32%, dev loss=2.2304, dev acc=28.90%
epoch: 56/10000, 21s, train loss=2.3376, train acc=23.79%, dev loss=2.2293, dev acc=28.95%
saving, test loss=2.2379, test acc=28.13%
epoch: 57/10000, 21s, train loss=2.3407, train acc=23.52%, dev loss=2.2279, dev acc=29.07%
saving, test loss=2.2365, test acc=28.08%
epoch: 58/10000, 21s, train loss=2.3409, train acc=23.26%, dev loss=2.2275, dev acc=28.97%
epoch: 59/10000, 21s, train loss=2.3419, train acc=23.27%, dev loss=2.2265, dev acc=29.14%
saving, test loss=2.2351, test acc=28.10%
epoch: 60/10000, 21s, train loss=2.3405, train acc=23.50%, dev loss=2.2255, dev acc=28.90%
epoch: 61/10000, 21s, train loss=2.3369, train acc=23.48%, dev loss=2.2255, dev acc=29.03%
epoch: 62/10000, 21s, train loss=2.3421, train acc=23.63%, dev loss=2.2246, dev acc=29.14%
epoch: 63/10000, 21s, train loss=2.3391, train acc=23.75%, dev loss=2.2241, dev acc=29.22%
saving, test loss=2.2323, test acc=28.28%
epoch: 64/10000, 21s, train loss=2.3396, train acc=23.52%, dev loss=2.2238, dev acc=29.27%
saving, test loss=2.2320, test acc=28.34%
epoch: 65/10000, 21s, train loss=2.3375, train acc=23.55%, dev loss=2.2225, dev acc=29.22%
epoch: 66/10000, 21s, train loss=2.3344, train acc=23.58%, dev loss=2.2218, dev acc=29.29%
saving, test loss=2.2303, test acc=28.45%
epoch: 67/10000, 21s, train loss=2.3395, train acc=23.44%, dev loss=2.2212, dev acc=29.18%
epoch: 68/10000, 21s, train loss=2.3412, train acc=23.79%, dev loss=2.2201, dev acc=29.33%
saving, test loss=2.2281, test acc=28.51%
epoch: 69/10000, 21s, train loss=2.3366, train acc=23.75%, dev loss=2.2200, dev acc=29.39%
saving, test loss=2.2282, test acc=28.36%
epoch: 70/10000, 21s, train loss=2.3418, train acc=23.42%, dev loss=2.2199, dev acc=29.44%
saving, test loss=2.2281, test acc=28.42%
epoch: 71/10000, 21s, train loss=2.3381, train acc=23.51%, dev loss=2.2191, dev acc=29.29%
epoch: 72/10000, 21s, train loss=2.3391, train acc=23.46%, dev loss=2.2189, dev acc=29.35%
epoch: 73/10000, 17s, train loss=2.3377, train acc=23.54%, dev loss=2.2179, dev acc=29.54%
saving, test loss=2.2262, test acc=28.38%
epoch: 74/10000, 21s, train loss=2.3393, train acc=23.57%, dev loss=2.2180, dev acc=29.50%
epoch: 75/10000, 21s, train loss=2.3359, train acc=23.74%, dev loss=2.2172, dev acc=29.54%
epoch: 76/10000, 21s, train loss=2.3397, train acc=23.47%, dev loss=2.2167, dev acc=29.59%
saving, test loss=2.2252, test acc=28.53%
epoch: 77/10000, 21s, train loss=2.3364, train acc=23.61%, dev loss=2.2160, dev acc=29.39%
epoch: 78/10000, 17s, train loss=2.3343, train acc=23.47%, dev loss=2.2162, dev acc=29.31%
epoch: 79/10000, 21s, train loss=2.3353, train acc=23.52%, dev loss=2.2158, dev acc=29.42%
epoch: 80/10000, 21s, train loss=2.3358, train acc=23.68%, dev loss=2.2152, dev acc=29.40%
epoch: 81/10000, 21s, train loss=2.3366, train acc=23.78%, dev loss=2.2153, dev acc=29.46%
time used=2111.3s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='1', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-electra-180g-small-ex-discriminator', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_entertainment': 0, 'news_game': 1, 'news_car': 2, 'news_agriculture': 3, 'news_tech': 4, 'news_story': 5, 'news_finance': 6, 'news_stock': 7, 'news_culture': 8, 'news_world': 9, 'news_edu': 10, 'news_military': 11, 'news_sports': 12, 'news_travel': 13, 'news_house': 14}
index_labels_dict={0: 'news_entertainment', 1: 'news_game', 2: 'news_car', 3: 'news_agriculture', 4: 'news_tech', 5: 'news_story', 6: 'news_finance', 7: 'news_stock', 8: 'news_culture', 9: 'news_world', 10: 'news_edu', 11: 'news_military', 12: 'news_sports', 13: 'news_travel', 14: 'news_house'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-electra-180g-small-ex-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight']
- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(21128, 256, padding_idx=0)
      (position_embeddings): Embedding(512, 256)
      (token_type_embeddings): Embedding(2, 256)
      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (12): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (13): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (14): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (15): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (16): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (17): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (18): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (19): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (20): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (21): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (22): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (23): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=256, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.attention.self.query.weight False
pretrained_model.encoder.layer.12.attention.self.query.bias False
pretrained_model.encoder.layer.12.attention.self.key.weight False
pretrained_model.encoder.layer.12.attention.self.key.bias False
pretrained_model.encoder.layer.12.attention.self.value.weight False
pretrained_model.encoder.layer.12.attention.self.value.bias False
pretrained_model.encoder.layer.12.attention.output.dense.weight False
pretrained_model.encoder.layer.12.attention.output.dense.bias False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.intermediate.dense.weight False
pretrained_model.encoder.layer.12.intermediate.dense.bias False
pretrained_model.encoder.layer.12.output.dense.weight False
pretrained_model.encoder.layer.12.output.dense.bias False
pretrained_model.encoder.layer.12.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.attention.self.query.weight False
pretrained_model.encoder.layer.13.attention.self.query.bias False
pretrained_model.encoder.layer.13.attention.self.key.weight False
pretrained_model.encoder.layer.13.attention.self.key.bias False
pretrained_model.encoder.layer.13.attention.self.value.weight False
pretrained_model.encoder.layer.13.attention.self.value.bias False
pretrained_model.encoder.layer.13.attention.output.dense.weight False
pretrained_model.encoder.layer.13.attention.output.dense.bias False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.intermediate.dense.weight False
pretrained_model.encoder.layer.13.intermediate.dense.bias False
pretrained_model.encoder.layer.13.output.dense.weight False
pretrained_model.encoder.layer.13.output.dense.bias False
pretrained_model.encoder.layer.13.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.attention.self.query.weight False
pretrained_model.encoder.layer.14.attention.self.query.bias False
pretrained_model.encoder.layer.14.attention.self.key.weight False
pretrained_model.encoder.layer.14.attention.self.key.bias False
pretrained_model.encoder.layer.14.attention.self.value.weight False
pretrained_model.encoder.layer.14.attention.self.value.bias False
pretrained_model.encoder.layer.14.attention.output.dense.weight False
pretrained_model.encoder.layer.14.attention.output.dense.bias False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.intermediate.dense.weight False
pretrained_model.encoder.layer.14.intermediate.dense.bias False
pretrained_model.encoder.layer.14.output.dense.weight False
pretrained_model.encoder.layer.14.output.dense.bias False
pretrained_model.encoder.layer.14.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.attention.self.query.weight False
pretrained_model.encoder.layer.15.attention.self.query.bias False
pretrained_model.encoder.layer.15.attention.self.key.weight False
pretrained_model.encoder.layer.15.attention.self.key.bias False
pretrained_model.encoder.layer.15.attention.self.value.weight False
pretrained_model.encoder.layer.15.attention.self.value.bias False
pretrained_model.encoder.layer.15.attention.output.dense.weight False
pretrained_model.encoder.layer.15.attention.output.dense.bias False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.intermediate.dense.weight False
pretrained_model.encoder.layer.15.intermediate.dense.bias False
pretrained_model.encoder.layer.15.output.dense.weight False
pretrained_model.encoder.layer.15.output.dense.bias False
pretrained_model.encoder.layer.15.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.attention.self.query.weight False
pretrained_model.encoder.layer.16.attention.self.query.bias False
pretrained_model.encoder.layer.16.attention.self.key.weight False
pretrained_model.encoder.layer.16.attention.self.key.bias False
pretrained_model.encoder.layer.16.attention.self.value.weight False
pretrained_model.encoder.layer.16.attention.self.value.bias False
pretrained_model.encoder.layer.16.attention.output.dense.weight False
pretrained_model.encoder.layer.16.attention.output.dense.bias False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.intermediate.dense.weight False
pretrained_model.encoder.layer.16.intermediate.dense.bias False
pretrained_model.encoder.layer.16.output.dense.weight False
pretrained_model.encoder.layer.16.output.dense.bias False
pretrained_model.encoder.layer.16.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.attention.self.query.weight False
pretrained_model.encoder.layer.17.attention.self.query.bias False
pretrained_model.encoder.layer.17.attention.self.key.weight False
pretrained_model.encoder.layer.17.attention.self.key.bias False
pretrained_model.encoder.layer.17.attention.self.value.weight False
pretrained_model.encoder.layer.17.attention.self.value.bias False
pretrained_model.encoder.layer.17.attention.output.dense.weight False
pretrained_model.encoder.layer.17.attention.output.dense.bias False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.intermediate.dense.weight False
pretrained_model.encoder.layer.17.intermediate.dense.bias False
pretrained_model.encoder.layer.17.output.dense.weight False
pretrained_model.encoder.layer.17.output.dense.bias False
pretrained_model.encoder.layer.17.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.attention.self.query.weight False
pretrained_model.encoder.layer.18.attention.self.query.bias False
pretrained_model.encoder.layer.18.attention.self.key.weight False
pretrained_model.encoder.layer.18.attention.self.key.bias False
pretrained_model.encoder.layer.18.attention.self.value.weight False
pretrained_model.encoder.layer.18.attention.self.value.bias False
pretrained_model.encoder.layer.18.attention.output.dense.weight False
pretrained_model.encoder.layer.18.attention.output.dense.bias False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.intermediate.dense.weight False
pretrained_model.encoder.layer.18.intermediate.dense.bias False
pretrained_model.encoder.layer.18.output.dense.weight False
pretrained_model.encoder.layer.18.output.dense.bias False
pretrained_model.encoder.layer.18.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.attention.self.query.weight False
pretrained_model.encoder.layer.19.attention.self.query.bias False
pretrained_model.encoder.layer.19.attention.self.key.weight False
pretrained_model.encoder.layer.19.attention.self.key.bias False
pretrained_model.encoder.layer.19.attention.self.value.weight False
pretrained_model.encoder.layer.19.attention.self.value.bias False
pretrained_model.encoder.layer.19.attention.output.dense.weight False
pretrained_model.encoder.layer.19.attention.output.dense.bias False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.intermediate.dense.weight False
pretrained_model.encoder.layer.19.intermediate.dense.bias False
pretrained_model.encoder.layer.19.output.dense.weight False
pretrained_model.encoder.layer.19.output.dense.bias False
pretrained_model.encoder.layer.19.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.attention.self.query.weight False
pretrained_model.encoder.layer.20.attention.self.query.bias False
pretrained_model.encoder.layer.20.attention.self.key.weight False
pretrained_model.encoder.layer.20.attention.self.key.bias False
pretrained_model.encoder.layer.20.attention.self.value.weight False
pretrained_model.encoder.layer.20.attention.self.value.bias False
pretrained_model.encoder.layer.20.attention.output.dense.weight False
pretrained_model.encoder.layer.20.attention.output.dense.bias False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.intermediate.dense.weight False
pretrained_model.encoder.layer.20.intermediate.dense.bias False
pretrained_model.encoder.layer.20.output.dense.weight False
pretrained_model.encoder.layer.20.output.dense.bias False
pretrained_model.encoder.layer.20.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.attention.self.query.weight False
pretrained_model.encoder.layer.21.attention.self.query.bias False
pretrained_model.encoder.layer.21.attention.self.key.weight False
pretrained_model.encoder.layer.21.attention.self.key.bias False
pretrained_model.encoder.layer.21.attention.self.value.weight False
pretrained_model.encoder.layer.21.attention.self.value.bias False
pretrained_model.encoder.layer.21.attention.output.dense.weight False
pretrained_model.encoder.layer.21.attention.output.dense.bias False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.intermediate.dense.weight False
pretrained_model.encoder.layer.21.intermediate.dense.bias False
pretrained_model.encoder.layer.21.output.dense.weight False
pretrained_model.encoder.layer.21.output.dense.bias False
pretrained_model.encoder.layer.21.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.attention.self.query.weight False
pretrained_model.encoder.layer.22.attention.self.query.bias False
pretrained_model.encoder.layer.22.attention.self.key.weight False
pretrained_model.encoder.layer.22.attention.self.key.bias False
pretrained_model.encoder.layer.22.attention.self.value.weight False
pretrained_model.encoder.layer.22.attention.self.value.bias False
pretrained_model.encoder.layer.22.attention.output.dense.weight False
pretrained_model.encoder.layer.22.attention.output.dense.bias False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.intermediate.dense.weight False
pretrained_model.encoder.layer.22.intermediate.dense.bias False
pretrained_model.encoder.layer.22.output.dense.weight False
pretrained_model.encoder.layer.22.output.dense.bias False
pretrained_model.encoder.layer.22.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.attention.self.query.weight False
pretrained_model.encoder.layer.23.attention.self.query.bias False
pretrained_model.encoder.layer.23.attention.self.key.weight False
pretrained_model.encoder.layer.23.attention.self.key.bias False
pretrained_model.encoder.layer.23.attention.self.value.weight False
pretrained_model.encoder.layer.23.attention.self.value.bias False
pretrained_model.encoder.layer.23.attention.output.dense.weight False
pretrained_model.encoder.layer.23.attention.output.dense.bias False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.intermediate.dense.weight False
pretrained_model.encoder.layer.23.intermediate.dense.bias False
pretrained_model.encoder.layer.23.output.dense.weight False
pretrained_model.encoder.layer.23.output.dense.bias False
pretrained_model.encoder.layer.23.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.output.LayerNorm.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 39s, train loss=2.6544, train acc=10.32%, dev loss=2.5609, dev acc=13.38%
saving, test loss=2.5655, test acc=13.18%
epoch: 2/10000, 42s, train loss=2.5758, train acc=12.98%, dev loss=2.5227, dev acc=15.72%
saving, test loss=2.5271, test acc=15.68%
epoch: 3/10000, 41s, train loss=2.5353, train acc=15.47%, dev loss=2.4961, dev acc=17.52%
saving, test loss=2.5004, test acc=17.10%
epoch: 4/10000, 42s, train loss=2.5093, train acc=17.09%, dev loss=2.4749, dev acc=18.80%
saving, test loss=2.4788, test acc=18.29%
epoch: 5/10000, 42s, train loss=2.4874, train acc=18.26%, dev loss=2.4564, dev acc=19.92%
saving, test loss=2.4602, test acc=19.25%
epoch: 6/10000, 31s, train loss=2.4708, train acc=19.16%, dev loss=2.4404, dev acc=20.46%
saving, test loss=2.4438, test acc=19.89%
epoch: 7/10000, 30s, train loss=2.4571, train acc=19.61%, dev loss=2.4260, dev acc=20.91%
saving, test loss=2.4288, test acc=20.45%
epoch: 8/10000, 30s, train loss=2.4428, train acc=20.45%, dev loss=2.4134, dev acc=21.46%
saving, test loss=2.4160, test acc=20.97%
epoch: 9/10000, 30s, train loss=2.4336, train acc=20.70%, dev loss=2.4021, dev acc=21.76%
saving, test loss=2.4046, test acc=21.36%
epoch: 10/10000, 30s, train loss=2.4223, train acc=21.26%, dev loss=2.3924, dev acc=22.26%
saving, test loss=2.3945, test acc=21.58%
epoch: 11/10000, 30s, train loss=2.4171, train acc=21.28%, dev loss=2.3825, dev acc=22.43%
saving, test loss=2.3841, test acc=21.89%
epoch: 12/10000, 30s, train loss=2.4133, train acc=21.45%, dev loss=2.3737, dev acc=22.96%
saving, test loss=2.3754, test acc=22.40%
epoch: 13/10000, 30s, train loss=2.4050, train acc=21.77%, dev loss=2.3667, dev acc=23.31%
saving, test loss=2.3683, test acc=22.62%
epoch: 14/10000, 30s, train loss=2.3977, train acc=21.84%, dev loss=2.3586, dev acc=23.73%
saving, test loss=2.3600, test acc=22.85%
epoch: 15/10000, 30s, train loss=2.3928, train acc=22.15%, dev loss=2.3523, dev acc=23.89%
saving, test loss=2.3539, test acc=23.16%
epoch: 16/10000, 30s, train loss=2.3907, train acc=21.94%, dev loss=2.3466, dev acc=24.12%
saving, test loss=2.3474, test acc=23.26%
epoch: 17/10000, 30s, train loss=2.3852, train acc=22.26%, dev loss=2.3412, dev acc=24.29%
saving, test loss=2.3417, test acc=23.55%
epoch: 18/10000, 30s, train loss=2.3831, train acc=22.42%, dev loss=2.3360, dev acc=24.42%
saving, test loss=2.3361, test acc=23.76%
epoch: 19/10000, 30s, train loss=2.3768, train acc=22.50%, dev loss=2.3313, dev acc=24.49%
saving, test loss=2.3312, test acc=24.05%
epoch: 20/10000, 30s, train loss=2.3782, train acc=22.49%, dev loss=2.3274, dev acc=24.59%
saving, test loss=2.3274, test acc=24.00%
epoch: 21/10000, 30s, train loss=2.3756, train acc=22.56%, dev loss=2.3229, dev acc=24.93%
saving, test loss=2.3222, test acc=24.16%
epoch: 22/10000, 30s, train loss=2.3687, train acc=22.72%, dev loss=2.3190, dev acc=25.19%
saving, test loss=2.3185, test acc=24.31%
epoch: 23/10000, 30s, train loss=2.3715, train acc=22.46%, dev loss=2.3150, dev acc=25.64%
saving, test loss=2.3144, test acc=24.39%
epoch: 24/10000, 30s, train loss=2.3701, train acc=22.82%, dev loss=2.3117, dev acc=25.86%
saving, test loss=2.3111, test acc=24.40%
epoch: 25/10000, 30s, train loss=2.3636, train acc=22.81%, dev loss=2.3090, dev acc=26.01%
saving, test loss=2.3079, test acc=24.80%
epoch: 26/10000, 30s, train loss=2.3596, train acc=22.91%, dev loss=2.3052, dev acc=26.31%
saving, test loss=2.3040, test acc=25.36%
epoch: 27/10000, 30s, train loss=2.3608, train acc=22.96%, dev loss=2.3025, dev acc=26.35%
saving, test loss=2.3011, test acc=25.28%
epoch: 28/10000, 30s, train loss=2.3613, train acc=23.08%, dev loss=2.3006, dev acc=26.35%
epoch: 29/10000, 30s, train loss=2.3572, train acc=22.88%, dev loss=2.2976, dev acc=26.57%
saving, test loss=2.2957, test acc=25.48%
epoch: 30/10000, 30s, train loss=2.3549, train acc=23.09%, dev loss=2.2951, dev acc=26.69%
saving, test loss=2.2931, test acc=25.57%
epoch: 31/10000, 30s, train loss=2.3586, train acc=23.04%, dev loss=2.2928, dev acc=26.69%
epoch: 32/10000, 30s, train loss=2.3570, train acc=23.05%, dev loss=2.2902, dev acc=26.72%
saving, test loss=2.2877, test acc=25.79%
epoch: 33/10000, 30s, train loss=2.3544, train acc=23.43%, dev loss=2.2879, dev acc=26.69%
epoch: 34/10000, 30s, train loss=2.3550, train acc=23.18%, dev loss=2.2875, dev acc=26.87%
saving, test loss=2.2850, test acc=25.96%
epoch: 35/10000, 30s, train loss=2.3543, train acc=22.92%, dev loss=2.2859, dev acc=26.80%
epoch: 36/10000, 30s, train loss=2.3541, train acc=23.14%, dev loss=2.2844, dev acc=26.87%
epoch: 37/10000, 30s, train loss=2.3541, train acc=22.89%, dev loss=2.2833, dev acc=26.95%
saving, test loss=2.2804, test acc=26.18%
epoch: 38/10000, 30s, train loss=2.3517, train acc=23.22%, dev loss=2.2818, dev acc=27.12%
saving, test loss=2.2787, test acc=26.17%
epoch: 39/10000, 30s, train loss=2.3494, train acc=23.23%, dev loss=2.2795, dev acc=27.04%
epoch: 40/10000, 30s, train loss=2.3498, train acc=23.10%, dev loss=2.2784, dev acc=27.01%
epoch: 41/10000, 30s, train loss=2.3524, train acc=23.02%, dev loss=2.2776, dev acc=26.99%
epoch: 42/10000, 30s, train loss=2.3461, train acc=23.43%, dev loss=2.2757, dev acc=27.21%
saving, test loss=2.2723, test acc=26.34%
epoch: 43/10000, 30s, train loss=2.3533, train acc=23.32%, dev loss=2.2739, dev acc=27.44%
saving, test loss=2.2704, test acc=26.64%
epoch: 44/10000, 30s, train loss=2.3458, train acc=23.32%, dev loss=2.2729, dev acc=27.42%
epoch: 45/10000, 30s, train loss=2.3478, train acc=23.49%, dev loss=2.2722, dev acc=27.51%
saving, test loss=2.2689, test acc=26.59%
epoch: 46/10000, 30s, train loss=2.3455, train acc=23.31%, dev loss=2.2715, dev acc=27.34%
epoch: 47/10000, 30s, train loss=2.3455, train acc=23.52%, dev loss=2.2701, dev acc=27.44%
epoch: 48/10000, 30s, train loss=2.3481, train acc=23.52%, dev loss=2.2695, dev acc=27.53%
saving, test loss=2.2658, test acc=26.82%
epoch: 49/10000, 30s, train loss=2.3449, train acc=23.37%, dev loss=2.2674, dev acc=27.51%
epoch: 50/10000, 30s, train loss=2.3450, train acc=23.42%, dev loss=2.2674, dev acc=27.68%
saving, test loss=2.2631, test acc=26.80%
epoch: 51/10000, 30s, train loss=2.3452, train acc=23.28%, dev loss=2.2664, dev acc=27.59%
epoch: 52/10000, 30s, train loss=2.3425, train acc=23.33%, dev loss=2.2662, dev acc=27.64%
epoch: 53/10000, 30s, train loss=2.3438, train acc=23.30%, dev loss=2.2657, dev acc=27.46%
epoch: 54/10000, 30s, train loss=2.3460, train acc=23.39%, dev loss=2.2642, dev acc=27.59%
epoch: 55/10000, 30s, train loss=2.3457, train acc=23.29%, dev loss=2.2638, dev acc=27.70%
saving, test loss=2.2590, test acc=27.20%
epoch: 56/10000, 30s, train loss=2.3451, train acc=23.50%, dev loss=2.2635, dev acc=27.57%
epoch: 57/10000, 30s, train loss=2.3425, train acc=23.26%, dev loss=2.2628, dev acc=27.57%
epoch: 58/10000, 30s, train loss=2.3432, train acc=23.52%, dev loss=2.2625, dev acc=27.49%
epoch: 59/10000, 29s, train loss=2.3466, train acc=23.32%, dev loss=2.2608, dev acc=27.62%
epoch: 60/10000, 30s, train loss=2.3414, train acc=23.40%, dev loss=2.2611, dev acc=27.57%
time used=2317.9s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='1', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-electra-base-discriminator', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_car': 0, 'news_tech': 1, 'news_travel': 2, 'news_agriculture': 3, 'news_world': 4, 'news_stock': 5, 'news_house': 6, 'news_finance': 7, 'news_entertainment': 8, 'news_edu': 9, 'news_culture': 10, 'news_sports': 11, 'news_game': 12, 'news_military': 13, 'news_story': 14}
index_labels_dict={0: 'news_car', 1: 'news_tech', 2: 'news_travel', 3: 'news_agriculture', 4: 'news_world', 5: 'news_stock', 6: 'news_house', 7: 'news_finance', 8: 'news_entertainment', 9: 'news_edu', 10: 'news_culture', 11: 'news_sports', 12: 'news_game', 13: 'news_military', 14: 'news_story'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-electra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight']
- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 70s, train loss=2.6323, train acc=10.74%, dev loss=2.5526, dev acc=12.63%
saving, test loss=2.5566, test acc=12.80%
epoch: 2/10000, 70s, train loss=2.5458, train acc=15.43%, dev loss=2.5073, dev acc=15.82%
saving, test loss=2.5136, test acc=16.22%
epoch: 3/10000, 70s, train loss=2.5066, train acc=17.82%, dev loss=2.4769, dev acc=17.13%
saving, test loss=2.4846, test acc=17.68%
epoch: 4/10000, 70s, train loss=2.4810, train acc=18.91%, dev loss=2.4530, dev acc=18.50%
saving, test loss=2.4613, test acc=18.51%
epoch: 5/10000, 70s, train loss=2.4605, train acc=19.73%, dev loss=2.4319, dev acc=19.02%
saving, test loss=2.4408, test acc=19.18%
epoch: 6/10000, 70s, train loss=2.4400, train acc=20.57%, dev loss=2.4144, dev acc=19.68%
saving, test loss=2.4235, test acc=19.75%
epoch: 7/10000, 70s, train loss=2.4244, train acc=21.16%, dev loss=2.3981, dev acc=20.41%
saving, test loss=2.4074, test acc=20.47%
epoch: 8/10000, 70s, train loss=2.4102, train acc=21.70%, dev loss=2.3843, dev acc=20.86%
saving, test loss=2.3934, test acc=20.85%
epoch: 9/10000, 70s, train loss=2.3981, train acc=21.86%, dev loss=2.3706, dev acc=21.35%
saving, test loss=2.3795, test acc=21.30%
epoch: 10/10000, 70s, train loss=2.3862, train acc=22.44%, dev loss=2.3597, dev acc=21.80%
saving, test loss=2.3688, test acc=21.56%
epoch: 11/10000, 70s, train loss=2.3766, train acc=23.03%, dev loss=2.3490, dev acc=22.28%
saving, test loss=2.3578, test acc=21.97%
epoch: 12/10000, 70s, train loss=2.3674, train acc=23.06%, dev loss=2.3398, dev acc=22.83%
saving, test loss=2.3485, test acc=22.62%
epoch: 13/10000, 70s, train loss=2.3593, train acc=23.53%, dev loss=2.3304, dev acc=23.29%
saving, test loss=2.3390, test acc=22.87%
epoch: 14/10000, 70s, train loss=2.3514, train acc=23.54%, dev loss=2.3228, dev acc=23.54%
saving, test loss=2.3314, test acc=23.12%
epoch: 15/10000, 70s, train loss=2.3443, train acc=23.45%, dev loss=2.3153, dev acc=23.61%
saving, test loss=2.3237, test acc=23.42%
epoch: 16/10000, 70s, train loss=2.3399, train acc=23.77%, dev loss=2.3069, dev acc=23.99%
saving, test loss=2.3148, test acc=23.75%
epoch: 17/10000, 70s, train loss=2.3318, train acc=24.21%, dev loss=2.2998, dev acc=24.27%
saving, test loss=2.3076, test acc=24.07%
epoch: 18/10000, 71s, train loss=2.3265, train acc=24.31%, dev loss=2.2937, dev acc=24.66%
saving, test loss=2.3012, test acc=24.27%
epoch: 19/10000, 70s, train loss=2.3257, train acc=24.14%, dev loss=2.2881, dev acc=24.59%
epoch: 20/10000, 71s, train loss=2.3191, train acc=24.65%, dev loss=2.2818, dev acc=25.09%
saving, test loss=2.2891, test acc=24.68%
epoch: 21/10000, 71s, train loss=2.3125, train acc=24.70%, dev loss=2.2771, dev acc=25.32%
saving, test loss=2.2841, test acc=24.80%
epoch: 22/10000, 71s, train loss=2.3102, train acc=24.95%, dev loss=2.2728, dev acc=25.47%
saving, test loss=2.2798, test acc=24.92%
epoch: 23/10000, 71s, train loss=2.3064, train acc=24.94%, dev loss=2.2679, dev acc=25.52%
saving, test loss=2.2747, test acc=25.14%
epoch: 24/10000, 71s, train loss=2.3032, train acc=25.25%, dev loss=2.2641, dev acc=25.92%
saving, test loss=2.2708, test acc=25.21%
epoch: 25/10000, 71s, train loss=2.3006, train acc=24.92%, dev loss=2.2591, dev acc=25.99%
saving, test loss=2.2657, test acc=25.42%
epoch: 26/10000, 71s, train loss=2.2978, train acc=25.17%, dev loss=2.2558, dev acc=26.31%
saving, test loss=2.2623, test acc=25.66%
epoch: 27/10000, 71s, train loss=2.2945, train acc=25.23%, dev loss=2.2513, dev acc=26.35%
saving, test loss=2.2577, test acc=25.81%
epoch: 28/10000, 71s, train loss=2.2928, train acc=25.27%, dev loss=2.2471, dev acc=26.61%
saving, test loss=2.2534, test acc=25.98%
epoch: 29/10000, 71s, train loss=2.2916, train acc=25.54%, dev loss=2.2439, dev acc=26.61%
epoch: 30/10000, 71s, train loss=2.2843, train acc=25.60%, dev loss=2.2412, dev acc=26.87%
saving, test loss=2.2472, test acc=26.28%
epoch: 31/10000, 71s, train loss=2.2822, train acc=25.62%, dev loss=2.2376, dev acc=26.93%
saving, test loss=2.2435, test acc=26.51%
epoch: 32/10000, 71s, train loss=2.2825, train acc=25.57%, dev loss=2.2341, dev acc=27.25%
saving, test loss=2.2400, test acc=26.56%
epoch: 33/10000, 71s, train loss=2.2828, train acc=25.65%, dev loss=2.2319, dev acc=27.40%
saving, test loss=2.2376, test acc=26.77%
epoch: 34/10000, 71s, train loss=2.2806, train acc=25.79%, dev loss=2.2287, dev acc=27.29%
epoch: 35/10000, 71s, train loss=2.2787, train acc=25.82%, dev loss=2.2261, dev acc=27.40%
epoch: 36/10000, 71s, train loss=2.2737, train acc=26.03%, dev loss=2.2242, dev acc=27.55%
saving, test loss=2.2294, test acc=27.00%
epoch: 37/10000, 71s, train loss=2.2758, train acc=26.13%, dev loss=2.2213, dev acc=27.55%
epoch: 38/10000, 71s, train loss=2.2722, train acc=26.06%, dev loss=2.2172, dev acc=27.87%
saving, test loss=2.2227, test acc=27.51%
epoch: 39/10000, 71s, train loss=2.2732, train acc=26.02%, dev loss=2.2158, dev acc=27.81%
epoch: 40/10000, 71s, train loss=2.2711, train acc=26.05%, dev loss=2.2142, dev acc=27.75%
epoch: 41/10000, 71s, train loss=2.2687, train acc=26.10%, dev loss=2.2120, dev acc=27.87%
epoch: 42/10000, 71s, train loss=2.2651, train acc=26.32%, dev loss=2.2105, dev acc=27.85%
epoch: 43/10000, 71s, train loss=2.2685, train acc=26.07%, dev loss=2.2091, dev acc=27.85%
time used=3863.6s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='1', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-electra-base-generator', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_entertainment': 0, 'news_tech': 1, 'news_world': 2, 'news_culture': 3, 'news_edu': 4, 'news_agriculture': 5, 'news_game': 6, 'news_story': 7, 'news_finance': 8, 'news_stock': 9, 'news_sports': 10, 'news_house': 11, 'news_car': 12, 'news_military': 13, 'news_travel': 14}
index_labels_dict={0: 'news_entertainment', 1: 'news_tech', 2: 'news_world', 3: 'news_culture', 4: 'news_edu', 5: 'news_agriculture', 6: 'news_game', 7: 'news_story', 8: 'news_finance', 9: 'news_stock', 10: 'news_sports', 11: 'news_house', 12: 'news_car', 13: 'news_military', 14: 'news_travel'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-electra-base-generator were not used when initializing ElectraModel: ['generator_predictions.dense.bias', 'generator_predictions.LayerNorm.weight', 'generator_lm_head.bias', 'generator_lm_head.weight', 'generator_predictions.LayerNorm.bias', 'generator_predictions.dense.weight']
- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (embeddings_project): Linear(in_features=768, out_features=192, bias=True)
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=192, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.embeddings_project.weight False
pretrained_model.embeddings_project.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 12s, train loss=2.6410, train acc=10.34%, dev loss=2.5920, dev acc=11.32%
saving, test loss=2.5953, test acc=11.12%
epoch: 2/10000, 12s, train loss=2.5832, train acc=12.15%, dev loss=2.5536, dev acc=11.66%
saving, test loss=2.5572, test acc=11.47%
epoch: 3/10000, 12s, train loss=2.5512, train acc=14.44%, dev loss=2.5239, dev acc=14.39%
saving, test loss=2.5277, test acc=14.35%
epoch: 4/10000, 12s, train loss=2.5248, train acc=16.74%, dev loss=2.4968, dev acc=17.60%
saving, test loss=2.5007, test acc=17.42%
epoch: 5/10000, 12s, train loss=2.4996, train acc=19.09%, dev loss=2.4712, dev acc=20.95%
saving, test loss=2.4750, test acc=20.85%
epoch: 6/10000, 12s, train loss=2.4776, train acc=20.88%, dev loss=2.4468, dev acc=23.54%
saving, test loss=2.4508, test acc=23.29%
epoch: 7/10000, 12s, train loss=2.4582, train acc=22.40%, dev loss=2.4236, dev acc=25.86%
saving, test loss=2.4276, test acc=25.44%
epoch: 8/10000, 12s, train loss=2.4369, train acc=24.04%, dev loss=2.4012, dev acc=27.89%
saving, test loss=2.4053, test acc=27.56%
epoch: 9/10000, 12s, train loss=2.4187, train acc=24.95%, dev loss=2.3798, dev acc=29.07%
saving, test loss=2.3839, test acc=28.89%
epoch: 10/10000, 12s, train loss=2.4002, train acc=25.86%, dev loss=2.3592, dev acc=30.73%
saving, test loss=2.3634, test acc=30.38%
epoch: 11/10000, 12s, train loss=2.3835, train acc=26.59%, dev loss=2.3394, dev acc=31.75%
saving, test loss=2.3435, test acc=31.64%
epoch: 12/10000, 12s, train loss=2.3676, train acc=27.46%, dev loss=2.3204, dev acc=33.26%
saving, test loss=2.3246, test acc=33.03%
epoch: 13/10000, 12s, train loss=2.3544, train acc=27.74%, dev loss=2.3022, dev acc=34.30%
saving, test loss=2.3064, test acc=34.34%
epoch: 14/10000, 12s, train loss=2.3374, train acc=28.52%, dev loss=2.2846, dev acc=35.23%
saving, test loss=2.2889, test acc=34.84%
epoch: 15/10000, 12s, train loss=2.3255, train acc=28.51%, dev loss=2.2678, dev acc=36.00%
saving, test loss=2.2720, test acc=35.87%
epoch: 16/10000, 12s, train loss=2.3115, train acc=28.83%, dev loss=2.2516, dev acc=37.20%
saving, test loss=2.2558, test acc=36.91%
epoch: 17/10000, 12s, train loss=2.3003, train acc=29.49%, dev loss=2.2359, dev acc=37.74%
saving, test loss=2.2401, test acc=37.74%
epoch: 18/10000, 12s, train loss=2.2889, train acc=29.69%, dev loss=2.2209, dev acc=38.02%
saving, test loss=2.2252, test acc=38.19%
epoch: 19/10000, 12s, train loss=2.2793, train acc=29.83%, dev loss=2.2066, dev acc=38.79%
saving, test loss=2.2109, test acc=38.80%
epoch: 20/10000, 12s, train loss=2.2662, train acc=30.13%, dev loss=2.1927, dev acc=39.22%
saving, test loss=2.1969, test acc=39.50%
epoch: 21/10000, 12s, train loss=2.2553, train acc=30.30%, dev loss=2.1792, dev acc=39.86%
saving, test loss=2.1834, test acc=39.96%
epoch: 22/10000, 12s, train loss=2.2462, train acc=30.77%, dev loss=2.1663, dev acc=40.12%
saving, test loss=2.1705, test acc=40.42%
epoch: 23/10000, 12s, train loss=2.2373, train acc=30.90%, dev loss=2.1538, dev acc=40.72%
saving, test loss=2.1579, test acc=40.64%
epoch: 24/10000, 12s, train loss=2.2335, train acc=30.77%, dev loss=2.1419, dev acc=41.06%
saving, test loss=2.1460, test acc=41.14%
epoch: 25/10000, 12s, train loss=2.2204, train acc=31.41%, dev loss=2.1304, dev acc=41.25%
saving, test loss=2.1347, test acc=41.30%
epoch: 26/10000, 12s, train loss=2.2148, train acc=31.35%, dev loss=2.1192, dev acc=41.32%
saving, test loss=2.1233, test acc=41.58%
epoch: 27/10000, 12s, train loss=2.2045, train acc=31.81%, dev loss=2.1084, dev acc=41.85%
saving, test loss=2.1125, test acc=41.93%
epoch: 28/10000, 12s, train loss=2.2020, train acc=31.54%, dev loss=2.0981, dev acc=42.13%
saving, test loss=2.1023, test acc=42.15%
epoch: 29/10000, 12s, train loss=2.1908, train acc=31.78%, dev loss=2.0882, dev acc=42.26%
saving, test loss=2.0923, test acc=42.39%
epoch: 30/10000, 12s, train loss=2.1859, train acc=31.92%, dev loss=2.0786, dev acc=42.73%
saving, test loss=2.0825, test acc=42.80%
epoch: 31/10000, 12s, train loss=2.1779, train acc=32.21%, dev loss=2.0692, dev acc=42.71%
epoch: 32/10000, 12s, train loss=2.1713, train acc=32.27%, dev loss=2.0602, dev acc=42.82%
saving, test loss=2.0641, test acc=43.01%
epoch: 33/10000, 12s, train loss=2.1685, train acc=32.64%, dev loss=2.0515, dev acc=43.08%
saving, test loss=2.0555, test acc=43.25%
epoch: 34/10000, 12s, train loss=2.1617, train acc=32.50%, dev loss=2.0432, dev acc=43.08%
epoch: 35/10000, 12s, train loss=2.1568, train acc=32.42%, dev loss=2.0351, dev acc=43.46%
saving, test loss=2.0388, test acc=43.46%
epoch: 36/10000, 12s, train loss=2.1511, train acc=32.57%, dev loss=2.0272, dev acc=43.65%
saving, test loss=2.0310, test acc=43.71%
epoch: 37/10000, 12s, train loss=2.1444, train acc=32.89%, dev loss=2.0196, dev acc=43.83%
saving, test loss=2.0233, test acc=43.79%
epoch: 38/10000, 12s, train loss=2.1432, train acc=32.63%, dev loss=2.0123, dev acc=43.85%
saving, test loss=2.0160, test acc=43.91%
epoch: 39/10000, 12s, train loss=2.1380, train acc=32.88%, dev loss=2.0052, dev acc=43.95%
saving, test loss=2.0087, test acc=44.14%
epoch: 40/10000, 12s, train loss=2.1347, train acc=32.60%, dev loss=1.9984, dev acc=44.08%
saving, test loss=2.0020, test acc=44.40%
epoch: 41/10000, 12s, train loss=2.1287, train acc=32.90%, dev loss=1.9918, dev acc=44.13%
saving, test loss=1.9952, test acc=44.30%
epoch: 42/10000, 12s, train loss=2.1293, train acc=33.23%, dev loss=1.9855, dev acc=44.19%
saving, test loss=1.9890, test acc=44.53%
epoch: 43/10000, 12s, train loss=2.1157, train acc=33.35%, dev loss=1.9791, dev acc=44.42%
saving, test loss=1.9825, test acc=44.65%
epoch: 44/10000, 12s, train loss=2.1181, train acc=33.28%, dev loss=1.9731, dev acc=44.38%
epoch: 45/10000, 12s, train loss=2.1105, train acc=33.48%, dev loss=1.9671, dev acc=44.62%
saving, test loss=1.9704, test acc=44.90%
epoch: 46/10000, 12s, train loss=2.1092, train acc=33.41%, dev loss=1.9615, dev acc=44.64%
saving, test loss=1.9648, test acc=45.00%
epoch: 47/10000, 12s, train loss=2.1106, train acc=33.17%, dev loss=1.9560, dev acc=44.79%
saving, test loss=1.9592, test acc=45.10%
epoch: 48/10000, 12s, train loss=2.1051, train acc=33.43%, dev loss=1.9509, dev acc=44.81%
saving, test loss=1.9542, test acc=45.23%
epoch: 49/10000, 12s, train loss=2.1000, train acc=33.52%, dev loss=1.9455, dev acc=45.03%
saving, test loss=1.9487, test acc=45.36%
epoch: 50/10000, 12s, train loss=2.1005, train acc=33.52%, dev loss=1.9406, dev acc=45.01%
epoch: 51/10000, 12s, train loss=2.0950, train acc=33.85%, dev loss=1.9356, dev acc=45.05%
saving, test loss=1.9387, test acc=45.53%
epoch: 52/10000, 12s, train loss=2.0900, train acc=33.66%, dev loss=1.9308, dev acc=45.07%
saving, test loss=1.9339, test acc=45.62%
epoch: 53/10000, 12s, train loss=2.0891, train acc=34.05%, dev loss=1.9262, dev acc=45.18%
saving, test loss=1.9293, test acc=45.73%
epoch: 54/10000, 12s, train loss=2.0843, train acc=34.01%, dev loss=1.9217, dev acc=45.11%
epoch: 55/10000, 12s, train loss=2.0873, train acc=33.75%, dev loss=1.9174, dev acc=45.13%
epoch: 56/10000, 12s, train loss=2.0813, train acc=33.77%, dev loss=1.9133, dev acc=45.26%
saving, test loss=1.9163, test acc=45.80%
epoch: 57/10000, 12s, train loss=2.0794, train acc=33.90%, dev loss=1.9091, dev acc=45.31%
saving, test loss=1.9122, test acc=45.85%
epoch: 58/10000, 12s, train loss=2.0775, train acc=33.90%, dev loss=1.9052, dev acc=45.39%
saving, test loss=1.9082, test acc=45.90%
epoch: 59/10000, 12s, train loss=2.0716, train acc=34.39%, dev loss=1.9014, dev acc=45.50%
saving, test loss=1.9043, test acc=46.02%
epoch: 60/10000, 12s, train loss=2.0718, train acc=34.29%, dev loss=1.8974, dev acc=45.52%
saving, test loss=1.9004, test acc=46.04%
epoch: 61/10000, 12s, train loss=2.0669, train acc=34.52%, dev loss=1.8938, dev acc=45.54%
saving, test loss=1.8967, test acc=46.11%
epoch: 62/10000, 12s, train loss=2.0713, train acc=34.28%, dev loss=1.8901, dev acc=45.58%
saving, test loss=1.8930, test acc=46.16%
epoch: 63/10000, 12s, train loss=2.0663, train acc=34.21%, dev loss=1.8868, dev acc=45.54%
epoch: 64/10000, 12s, train loss=2.0689, train acc=34.33%, dev loss=1.8832, dev acc=45.75%
saving, test loss=1.8860, test acc=46.16%
epoch: 65/10000, 12s, train loss=2.0604, train acc=34.42%, dev loss=1.8801, dev acc=45.65%
epoch: 66/10000, 12s, train loss=2.0581, train acc=34.62%, dev loss=1.8767, dev acc=45.88%
saving, test loss=1.8793, test acc=46.17%
epoch: 67/10000, 12s, train loss=2.0606, train acc=34.17%, dev loss=1.8736, dev acc=45.82%
epoch: 68/10000, 12s, train loss=2.0564, train acc=34.55%, dev loss=1.8705, dev acc=45.84%
epoch: 69/10000, 12s, train loss=2.0558, train acc=34.30%, dev loss=1.8675, dev acc=45.93%
saving, test loss=1.8700, test acc=46.23%
epoch: 70/10000, 12s, train loss=2.0539, train acc=34.52%, dev loss=1.8646, dev acc=45.95%
saving, test loss=1.8671, test acc=46.29%
epoch: 71/10000, 12s, train loss=2.0539, train acc=34.36%, dev loss=1.8617, dev acc=45.93%
epoch: 72/10000, 12s, train loss=2.0495, train acc=34.62%, dev loss=1.8590, dev acc=46.01%
saving, test loss=1.8618, test acc=46.35%
epoch: 73/10000, 12s, train loss=2.0561, train acc=34.34%, dev loss=1.8562, dev acc=45.91%
epoch: 74/10000, 12s, train loss=2.0470, train acc=34.95%, dev loss=1.8536, dev acc=45.90%
epoch: 75/10000, 12s, train loss=2.0489, train acc=34.66%, dev loss=1.8509, dev acc=46.01%
epoch: 76/10000, 12s, train loss=2.0436, train acc=34.70%, dev loss=1.8485, dev acc=45.97%
epoch: 77/10000, 12s, train loss=2.0444, train acc=34.64%, dev loss=1.8460, dev acc=46.10%
saving, test loss=1.8483, test acc=46.59%
epoch: 78/10000, 12s, train loss=2.0422, train acc=34.52%, dev loss=1.8438, dev acc=46.08%
epoch: 79/10000, 12s, train loss=2.0412, train acc=34.48%, dev loss=1.8412, dev acc=46.21%
saving, test loss=1.8437, test acc=46.71%
epoch: 80/10000, 12s, train loss=2.0404, train acc=34.79%, dev loss=1.8388, dev acc=46.20%
epoch: 81/10000, 12s, train loss=2.0367, train acc=34.82%, dev loss=1.8370, dev acc=46.25%
saving, test loss=1.8394, test acc=46.66%
epoch: 82/10000, 12s, train loss=2.0329, train acc=35.16%, dev loss=1.8346, dev acc=46.16%
epoch: 83/10000, 12s, train loss=2.0290, train acc=35.50%, dev loss=1.8326, dev acc=46.01%
epoch: 84/10000, 12s, train loss=2.0393, train acc=34.98%, dev loss=1.8303, dev acc=46.27%
saving, test loss=1.8324, test acc=46.73%
epoch: 85/10000, 12s, train loss=2.0344, train acc=35.12%, dev loss=1.8283, dev acc=46.27%
epoch: 86/10000, 12s, train loss=2.0340, train acc=34.90%, dev loss=1.8264, dev acc=46.29%
saving, test loss=1.8287, test acc=46.75%
epoch: 87/10000, 12s, train loss=2.0331, train acc=35.03%, dev loss=1.8243, dev acc=46.35%
saving, test loss=1.8267, test acc=46.78%
epoch: 88/10000, 12s, train loss=2.0352, train acc=34.60%, dev loss=1.8225, dev acc=46.42%
saving, test loss=1.8248, test acc=46.81%
epoch: 89/10000, 12s, train loss=2.0271, train acc=35.29%, dev loss=1.8205, dev acc=46.48%
saving, test loss=1.8228, test acc=46.80%
epoch: 90/10000, 12s, train loss=2.0266, train acc=35.18%, dev loss=1.8187, dev acc=46.35%
epoch: 91/10000, 12s, train loss=2.0322, train acc=34.88%, dev loss=1.8170, dev acc=46.35%
epoch: 92/10000, 12s, train loss=2.0237, train acc=35.35%, dev loss=1.8154, dev acc=46.31%
epoch: 93/10000, 12s, train loss=2.0285, train acc=34.71%, dev loss=1.8139, dev acc=46.25%
epoch: 94/10000, 12s, train loss=2.0279, train acc=34.91%, dev loss=1.8120, dev acc=46.23%
time used=1519.5s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='1', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-electra-large-discriminator', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_travel': 0, 'news_sports': 1, 'news_car': 2, 'news_culture': 3, 'news_entertainment': 4, 'news_military': 5, 'news_world': 6, 'news_agriculture': 7, 'news_tech': 8, 'news_edu': 9, 'news_finance': 10, 'news_story': 11, 'news_stock': 12, 'news_game': 13, 'news_house': 14}
index_labels_dict={0: 'news_travel', 1: 'news_sports', 2: 'news_car', 3: 'news_culture', 4: 'news_entertainment', 5: 'news_military', 6: 'news_world', 7: 'news_agriculture', 8: 'news_tech', 9: 'news_edu', 10: 'news_finance', 11: 'news_story', 12: 'news_stock', 13: 'news_game', 14: 'news_house'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-electra-large-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']
- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (12): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (13): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (14): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (15): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (16): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (17): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (18): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (19): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (20): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (21): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (22): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (23): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=1024, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.attention.self.query.weight False
pretrained_model.encoder.layer.12.attention.self.query.bias False
pretrained_model.encoder.layer.12.attention.self.key.weight False
pretrained_model.encoder.layer.12.attention.self.key.bias False
pretrained_model.encoder.layer.12.attention.self.value.weight False
pretrained_model.encoder.layer.12.attention.self.value.bias False
pretrained_model.encoder.layer.12.attention.output.dense.weight False
pretrained_model.encoder.layer.12.attention.output.dense.bias False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.intermediate.dense.weight False
pretrained_model.encoder.layer.12.intermediate.dense.bias False
pretrained_model.encoder.layer.12.output.dense.weight False
pretrained_model.encoder.layer.12.output.dense.bias False
pretrained_model.encoder.layer.12.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.attention.self.query.weight False
pretrained_model.encoder.layer.13.attention.self.query.bias False
pretrained_model.encoder.layer.13.attention.self.key.weight False
pretrained_model.encoder.layer.13.attention.self.key.bias False
pretrained_model.encoder.layer.13.attention.self.value.weight False
pretrained_model.encoder.layer.13.attention.self.value.bias False
pretrained_model.encoder.layer.13.attention.output.dense.weight False
pretrained_model.encoder.layer.13.attention.output.dense.bias False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.intermediate.dense.weight False
pretrained_model.encoder.layer.13.intermediate.dense.bias False
pretrained_model.encoder.layer.13.output.dense.weight False
pretrained_model.encoder.layer.13.output.dense.bias False
pretrained_model.encoder.layer.13.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.attention.self.query.weight False
pretrained_model.encoder.layer.14.attention.self.query.bias False
pretrained_model.encoder.layer.14.attention.self.key.weight False
pretrained_model.encoder.layer.14.attention.self.key.bias False
pretrained_model.encoder.layer.14.attention.self.value.weight False
pretrained_model.encoder.layer.14.attention.self.value.bias False
pretrained_model.encoder.layer.14.attention.output.dense.weight False
pretrained_model.encoder.layer.14.attention.output.dense.bias False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.intermediate.dense.weight False
pretrained_model.encoder.layer.14.intermediate.dense.bias False
pretrained_model.encoder.layer.14.output.dense.weight False
pretrained_model.encoder.layer.14.output.dense.bias False
pretrained_model.encoder.layer.14.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.attention.self.query.weight False
pretrained_model.encoder.layer.15.attention.self.query.bias False
pretrained_model.encoder.layer.15.attention.self.key.weight False
pretrained_model.encoder.layer.15.attention.self.key.bias False
pretrained_model.encoder.layer.15.attention.self.value.weight False
pretrained_model.encoder.layer.15.attention.self.value.bias False
pretrained_model.encoder.layer.15.attention.output.dense.weight False
pretrained_model.encoder.layer.15.attention.output.dense.bias False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.intermediate.dense.weight False
pretrained_model.encoder.layer.15.intermediate.dense.bias False
pretrained_model.encoder.layer.15.output.dense.weight False
pretrained_model.encoder.layer.15.output.dense.bias False
pretrained_model.encoder.layer.15.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.attention.self.query.weight False
pretrained_model.encoder.layer.16.attention.self.query.bias False
pretrained_model.encoder.layer.16.attention.self.key.weight False
pretrained_model.encoder.layer.16.attention.self.key.bias False
pretrained_model.encoder.layer.16.attention.self.value.weight False
pretrained_model.encoder.layer.16.attention.self.value.bias False
pretrained_model.encoder.layer.16.attention.output.dense.weight False
pretrained_model.encoder.layer.16.attention.output.dense.bias False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.intermediate.dense.weight False
pretrained_model.encoder.layer.16.intermediate.dense.bias False
pretrained_model.encoder.layer.16.output.dense.weight False
pretrained_model.encoder.layer.16.output.dense.bias False
pretrained_model.encoder.layer.16.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.attention.self.query.weight False
pretrained_model.encoder.layer.17.attention.self.query.bias False
pretrained_model.encoder.layer.17.attention.self.key.weight False
pretrained_model.encoder.layer.17.attention.self.key.bias False
pretrained_model.encoder.layer.17.attention.self.value.weight False
pretrained_model.encoder.layer.17.attention.self.value.bias False
pretrained_model.encoder.layer.17.attention.output.dense.weight False
pretrained_model.encoder.layer.17.attention.output.dense.bias False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.intermediate.dense.weight False
pretrained_model.encoder.layer.17.intermediate.dense.bias False
pretrained_model.encoder.layer.17.output.dense.weight False
pretrained_model.encoder.layer.17.output.dense.bias False
pretrained_model.encoder.layer.17.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.attention.self.query.weight False
pretrained_model.encoder.layer.18.attention.self.query.bias False
pretrained_model.encoder.layer.18.attention.self.key.weight False
pretrained_model.encoder.layer.18.attention.self.key.bias False
pretrained_model.encoder.layer.18.attention.self.value.weight False
pretrained_model.encoder.layer.18.attention.self.value.bias False
pretrained_model.encoder.layer.18.attention.output.dense.weight False
pretrained_model.encoder.layer.18.attention.output.dense.bias False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.intermediate.dense.weight False
pretrained_model.encoder.layer.18.intermediate.dense.bias False
pretrained_model.encoder.layer.18.output.dense.weight False
pretrained_model.encoder.layer.18.output.dense.bias False
pretrained_model.encoder.layer.18.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.attention.self.query.weight False
pretrained_model.encoder.layer.19.attention.self.query.bias False
pretrained_model.encoder.layer.19.attention.self.key.weight False
pretrained_model.encoder.layer.19.attention.self.key.bias False
pretrained_model.encoder.layer.19.attention.self.value.weight False
pretrained_model.encoder.layer.19.attention.self.value.bias False
pretrained_model.encoder.layer.19.attention.output.dense.weight False
pretrained_model.encoder.layer.19.attention.output.dense.bias False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.intermediate.dense.weight False
pretrained_model.encoder.layer.19.intermediate.dense.bias False
pretrained_model.encoder.layer.19.output.dense.weight False
pretrained_model.encoder.layer.19.output.dense.bias False
pretrained_model.encoder.layer.19.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.attention.self.query.weight False
pretrained_model.encoder.layer.20.attention.self.query.bias False
pretrained_model.encoder.layer.20.attention.self.key.weight False
pretrained_model.encoder.layer.20.attention.self.key.bias False
pretrained_model.encoder.layer.20.attention.self.value.weight False
pretrained_model.encoder.layer.20.attention.self.value.bias False
pretrained_model.encoder.layer.20.attention.output.dense.weight False
pretrained_model.encoder.layer.20.attention.output.dense.bias False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.intermediate.dense.weight False
pretrained_model.encoder.layer.20.intermediate.dense.bias False
pretrained_model.encoder.layer.20.output.dense.weight False
pretrained_model.encoder.layer.20.output.dense.bias False
pretrained_model.encoder.layer.20.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.attention.self.query.weight False
pretrained_model.encoder.layer.21.attention.self.query.bias False
pretrained_model.encoder.layer.21.attention.self.key.weight False
pretrained_model.encoder.layer.21.attention.self.key.bias False
pretrained_model.encoder.layer.21.attention.self.value.weight False
pretrained_model.encoder.layer.21.attention.self.value.bias False
pretrained_model.encoder.layer.21.attention.output.dense.weight False
pretrained_model.encoder.layer.21.attention.output.dense.bias False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.intermediate.dense.weight False
pretrained_model.encoder.layer.21.intermediate.dense.bias False
pretrained_model.encoder.layer.21.output.dense.weight False
pretrained_model.encoder.layer.21.output.dense.bias False
pretrained_model.encoder.layer.21.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.attention.self.query.weight False
pretrained_model.encoder.layer.22.attention.self.query.bias False
pretrained_model.encoder.layer.22.attention.self.key.weight False
pretrained_model.encoder.layer.22.attention.self.key.bias False
pretrained_model.encoder.layer.22.attention.self.value.weight False
pretrained_model.encoder.layer.22.attention.self.value.bias False
pretrained_model.encoder.layer.22.attention.output.dense.weight False
pretrained_model.encoder.layer.22.attention.output.dense.bias False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.intermediate.dense.weight False
pretrained_model.encoder.layer.22.intermediate.dense.bias False
pretrained_model.encoder.layer.22.output.dense.weight False
pretrained_model.encoder.layer.22.output.dense.bias False
pretrained_model.encoder.layer.22.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.attention.self.query.weight False
pretrained_model.encoder.layer.23.attention.self.query.bias False
pretrained_model.encoder.layer.23.attention.self.key.weight False
pretrained_model.encoder.layer.23.attention.self.key.bias False
pretrained_model.encoder.layer.23.attention.self.value.weight False
pretrained_model.encoder.layer.23.attention.self.value.bias False
pretrained_model.encoder.layer.23.attention.output.dense.weight False
pretrained_model.encoder.layer.23.attention.output.dense.bias False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.intermediate.dense.weight False
pretrained_model.encoder.layer.23.intermediate.dense.bias False
pretrained_model.encoder.layer.23.output.dense.weight False
pretrained_model.encoder.layer.23.output.dense.bias False
pretrained_model.encoder.layer.23.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.output.LayerNorm.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 222s, train loss=2.6525, train acc=10.23%, dev loss=2.5676, dev acc=13.79%
saving, test loss=2.5721, test acc=13.68%
epoch: 2/10000, 223s, train loss=2.5713, train acc=13.64%, dev loss=2.5345, dev acc=15.37%
saving, test loss=2.5385, test acc=15.15%
epoch: 3/10000, 223s, train loss=2.5461, train acc=15.05%, dev loss=2.5165, dev acc=16.47%
saving, test loss=2.5219, test acc=16.35%
epoch: 4/10000, 223s, train loss=2.5299, train acc=15.97%, dev loss=2.5018, dev acc=17.15%
saving, test loss=2.5085, test acc=16.59%
epoch: 5/10000, 223s, train loss=2.5164, train acc=16.47%, dev loss=2.4900, dev acc=17.82%
saving, test loss=2.4969, test acc=17.10%
epoch: 6/10000, 223s, train loss=2.5072, train acc=16.81%, dev loss=2.4798, dev acc=18.40%
saving, test loss=2.4869, test acc=17.51%
epoch: 7/10000, 223s, train loss=2.4960, train acc=17.44%, dev loss=2.4713, dev acc=18.76%
saving, test loss=2.4782, test acc=18.04%
epoch: 8/10000, 224s, train loss=2.4905, train acc=17.41%, dev loss=2.4632, dev acc=19.17%
saving, test loss=2.4699, test acc=18.43%
epoch: 9/10000, 224s, train loss=2.4831, train acc=17.80%, dev loss=2.4569, dev acc=19.36%
saving, test loss=2.4643, test acc=18.77%
epoch: 10/10000, 224s, train loss=2.4785, train acc=17.97%, dev loss=2.4500, dev acc=19.64%
saving, test loss=2.4564, test acc=19.04%
epoch: 11/10000, 224s, train loss=2.4741, train acc=18.25%, dev loss=2.4439, dev acc=19.98%
saving, test loss=2.4511, test acc=19.21%
epoch: 12/10000, 224s, train loss=2.4706, train acc=18.52%, dev loss=2.4386, dev acc=20.16%
saving, test loss=2.4453, test acc=19.62%
epoch: 13/10000, 224s, train loss=2.4664, train acc=18.53%, dev loss=2.4341, dev acc=20.26%
saving, test loss=2.4405, test acc=19.60%
epoch: 14/10000, 224s, train loss=2.4601, train acc=18.79%, dev loss=2.4291, dev acc=20.65%
saving, test loss=2.4358, test acc=19.78%
epoch: 15/10000, 224s, train loss=2.4585, train acc=18.82%, dev loss=2.4251, dev acc=20.78%
saving, test loss=2.4315, test acc=19.99%
epoch: 16/10000, 224s, train loss=2.4541, train acc=19.00%, dev loss=2.4211, dev acc=21.16%
saving, test loss=2.4275, test acc=20.23%
epoch: 17/10000, 225s, train loss=2.4525, train acc=19.03%, dev loss=2.4177, dev acc=21.27%
saving, test loss=2.4238, test acc=20.36%
epoch: 18/10000, 225s, train loss=2.4491, train acc=19.21%, dev loss=2.4130, dev acc=21.65%
saving, test loss=2.4194, test acc=20.68%
epoch: 19/10000, 225s, train loss=2.4482, train acc=19.16%, dev loss=2.4100, dev acc=22.15%
saving, test loss=2.4157, test acc=21.23%
epoch: 20/10000, 225s, train loss=2.4452, train acc=19.33%, dev loss=2.4085, dev acc=22.11%
epoch: 21/10000, 225s, train loss=2.4431, train acc=19.33%, dev loss=2.4054, dev acc=22.10%
epoch: 22/10000, 225s, train loss=2.4405, train acc=19.49%, dev loss=2.4011, dev acc=22.25%
saving, test loss=2.4071, test acc=21.38%
epoch: 23/10000, 225s, train loss=2.4421, train acc=19.40%, dev loss=2.3986, dev acc=22.53%
saving, test loss=2.4041, test acc=21.75%
epoch: 24/10000, 225s, train loss=2.4401, train acc=19.40%, dev loss=2.3971, dev acc=22.56%
saving, test loss=2.4028, test acc=21.60%
epoch: 25/10000, 225s, train loss=2.4394, train acc=19.57%, dev loss=2.3941, dev acc=22.60%
saving, test loss=2.4000, test acc=21.84%
epoch: 26/10000, 226s, train loss=2.4379, train acc=19.58%, dev loss=2.3925, dev acc=22.62%
saving, test loss=2.3983, test acc=21.78%
epoch: 27/10000, 226s, train loss=2.4328, train acc=19.60%, dev loss=2.3900, dev acc=22.69%
saving, test loss=2.3955, test acc=21.87%
epoch: 28/10000, 226s, train loss=2.4350, train acc=19.61%, dev loss=2.3892, dev acc=22.86%
saving, test loss=2.3947, test acc=21.89%
epoch: 29/10000, 226s, train loss=2.4339, train acc=19.68%, dev loss=2.3867, dev acc=22.79%
epoch: 30/10000, 226s, train loss=2.4331, train acc=19.93%, dev loss=2.3842, dev acc=23.11%
saving, test loss=2.3894, test acc=22.04%
epoch: 31/10000, 226s, train loss=2.4314, train acc=19.83%, dev loss=2.3835, dev acc=23.28%
saving, test loss=2.3895, test acc=22.14%
epoch: 32/10000, 226s, train loss=2.4268, train acc=20.00%, dev loss=2.3825, dev acc=23.01%
epoch: 33/10000, 226s, train loss=2.4291, train acc=19.87%, dev loss=2.3796, dev acc=23.20%
epoch: 34/10000, 226s, train loss=2.4263, train acc=19.93%, dev loss=2.3790, dev acc=23.41%
saving, test loss=2.3847, test acc=22.27%
epoch: 35/10000, 226s, train loss=2.4234, train acc=20.20%, dev loss=2.3771, dev acc=23.54%
saving, test loss=2.3829, test acc=22.53%
epoch: 36/10000, 226s, train loss=2.4274, train acc=20.02%, dev loss=2.3752, dev acc=23.46%
epoch: 37/10000, 226s, train loss=2.4259, train acc=20.01%, dev loss=2.3748, dev acc=23.56%
saving, test loss=2.3802, test acc=22.52%
epoch: 38/10000, 226s, train loss=2.4257, train acc=20.19%, dev loss=2.3728, dev acc=23.78%
saving, test loss=2.3781, test acc=22.42%
epoch: 39/10000, 226s, train loss=2.4255, train acc=20.18%, dev loss=2.3727, dev acc=23.65%
epoch: 40/10000, 226s, train loss=2.4227, train acc=20.13%, dev loss=2.3704, dev acc=23.56%
epoch: 41/10000, 226s, train loss=2.4223, train acc=20.16%, dev loss=2.3690, dev acc=23.95%
saving, test loss=2.3742, test acc=22.59%
epoch: 42/10000, 226s, train loss=2.4198, train acc=20.16%, dev loss=2.3684, dev acc=23.76%
epoch: 43/10000, 226s, train loss=2.4206, train acc=19.96%, dev loss=2.3677, dev acc=23.65%
epoch: 44/10000, 226s, train loss=2.4187, train acc=20.42%, dev loss=2.3661, dev acc=23.95%
epoch: 45/10000, 226s, train loss=2.4196, train acc=20.39%, dev loss=2.3638, dev acc=23.93%
epoch: 46/10000, 226s, train loss=2.4184, train acc=20.23%, dev loss=2.3626, dev acc=23.86%
time used=12992.6s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='1', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-electra-large-generator', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_military': 0, 'news_story': 1, 'news_world': 2, 'news_edu': 3, 'news_game': 4, 'news_finance': 5, 'news_entertainment': 6, 'news_sports': 7, 'news_tech': 8, 'news_stock': 9, 'news_agriculture': 10, 'news_house': 11, 'news_culture': 12, 'news_car': 13, 'news_travel': 14}
index_labels_dict={0: 'news_military', 1: 'news_story', 2: 'news_world', 3: 'news_edu', 4: 'news_game', 5: 'news_finance', 6: 'news_entertainment', 7: 'news_sports', 8: 'news_tech', 9: 'news_stock', 10: 'news_agriculture', 11: 'news_house', 12: 'news_culture', 13: 'news_car', 14: 'news_travel'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-electra-large-generator were not used when initializing ElectraModel: ['generator_predictions.dense.bias', 'generator_predictions.dense.weight', 'generator_predictions.LayerNorm.bias', 'generator_lm_head.weight', 'generator_predictions.LayerNorm.weight', 'generator_lm_head.bias']
- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (embeddings_project): Linear(in_features=1024, out_features=256, bias=True)
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (12): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (13): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (14): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (15): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (16): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (17): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (18): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (19): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (20): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (21): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (22): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (23): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=256, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.embeddings_project.weight False
pretrained_model.embeddings_project.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.attention.self.query.weight False
pretrained_model.encoder.layer.12.attention.self.query.bias False
pretrained_model.encoder.layer.12.attention.self.key.weight False
pretrained_model.encoder.layer.12.attention.self.key.bias False
pretrained_model.encoder.layer.12.attention.self.value.weight False
pretrained_model.encoder.layer.12.attention.self.value.bias False
pretrained_model.encoder.layer.12.attention.output.dense.weight False
pretrained_model.encoder.layer.12.attention.output.dense.bias False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.intermediate.dense.weight False
pretrained_model.encoder.layer.12.intermediate.dense.bias False
pretrained_model.encoder.layer.12.output.dense.weight False
pretrained_model.encoder.layer.12.output.dense.bias False
pretrained_model.encoder.layer.12.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.attention.self.query.weight False
pretrained_model.encoder.layer.13.attention.self.query.bias False
pretrained_model.encoder.layer.13.attention.self.key.weight False
pretrained_model.encoder.layer.13.attention.self.key.bias False
pretrained_model.encoder.layer.13.attention.self.value.weight False
pretrained_model.encoder.layer.13.attention.self.value.bias False
pretrained_model.encoder.layer.13.attention.output.dense.weight False
pretrained_model.encoder.layer.13.attention.output.dense.bias False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.intermediate.dense.weight False
pretrained_model.encoder.layer.13.intermediate.dense.bias False
pretrained_model.encoder.layer.13.output.dense.weight False
pretrained_model.encoder.layer.13.output.dense.bias False
pretrained_model.encoder.layer.13.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.attention.self.query.weight False
pretrained_model.encoder.layer.14.attention.self.query.bias False
pretrained_model.encoder.layer.14.attention.self.key.weight False
pretrained_model.encoder.layer.14.attention.self.key.bias False
pretrained_model.encoder.layer.14.attention.self.value.weight False
pretrained_model.encoder.layer.14.attention.self.value.bias False
pretrained_model.encoder.layer.14.attention.output.dense.weight False
pretrained_model.encoder.layer.14.attention.output.dense.bias False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.intermediate.dense.weight False
pretrained_model.encoder.layer.14.intermediate.dense.bias False
pretrained_model.encoder.layer.14.output.dense.weight False
pretrained_model.encoder.layer.14.output.dense.bias False
pretrained_model.encoder.layer.14.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.attention.self.query.weight False
pretrained_model.encoder.layer.15.attention.self.query.bias False
pretrained_model.encoder.layer.15.attention.self.key.weight False
pretrained_model.encoder.layer.15.attention.self.key.bias False
pretrained_model.encoder.layer.15.attention.self.value.weight False
pretrained_model.encoder.layer.15.attention.self.value.bias False
pretrained_model.encoder.layer.15.attention.output.dense.weight False
pretrained_model.encoder.layer.15.attention.output.dense.bias False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.intermediate.dense.weight False
pretrained_model.encoder.layer.15.intermediate.dense.bias False
pretrained_model.encoder.layer.15.output.dense.weight False
pretrained_model.encoder.layer.15.output.dense.bias False
pretrained_model.encoder.layer.15.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.attention.self.query.weight False
pretrained_model.encoder.layer.16.attention.self.query.bias False
pretrained_model.encoder.layer.16.attention.self.key.weight False
pretrained_model.encoder.layer.16.attention.self.key.bias False
pretrained_model.encoder.layer.16.attention.self.value.weight False
pretrained_model.encoder.layer.16.attention.self.value.bias False
pretrained_model.encoder.layer.16.attention.output.dense.weight False
pretrained_model.encoder.layer.16.attention.output.dense.bias False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.intermediate.dense.weight False
pretrained_model.encoder.layer.16.intermediate.dense.bias False
pretrained_model.encoder.layer.16.output.dense.weight False
pretrained_model.encoder.layer.16.output.dense.bias False
pretrained_model.encoder.layer.16.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.attention.self.query.weight False
pretrained_model.encoder.layer.17.attention.self.query.bias False
pretrained_model.encoder.layer.17.attention.self.key.weight False
pretrained_model.encoder.layer.17.attention.self.key.bias False
pretrained_model.encoder.layer.17.attention.self.value.weight False
pretrained_model.encoder.layer.17.attention.self.value.bias False
pretrained_model.encoder.layer.17.attention.output.dense.weight False
pretrained_model.encoder.layer.17.attention.output.dense.bias False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.intermediate.dense.weight False
pretrained_model.encoder.layer.17.intermediate.dense.bias False
pretrained_model.encoder.layer.17.output.dense.weight False
pretrained_model.encoder.layer.17.output.dense.bias False
pretrained_model.encoder.layer.17.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.attention.self.query.weight False
pretrained_model.encoder.layer.18.attention.self.query.bias False
pretrained_model.encoder.layer.18.attention.self.key.weight False
pretrained_model.encoder.layer.18.attention.self.key.bias False
pretrained_model.encoder.layer.18.attention.self.value.weight False
pretrained_model.encoder.layer.18.attention.self.value.bias False
pretrained_model.encoder.layer.18.attention.output.dense.weight False
pretrained_model.encoder.layer.18.attention.output.dense.bias False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.intermediate.dense.weight False
pretrained_model.encoder.layer.18.intermediate.dense.bias False
pretrained_model.encoder.layer.18.output.dense.weight False
pretrained_model.encoder.layer.18.output.dense.bias False
pretrained_model.encoder.layer.18.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.attention.self.query.weight False
pretrained_model.encoder.layer.19.attention.self.query.bias False
pretrained_model.encoder.layer.19.attention.self.key.weight False
pretrained_model.encoder.layer.19.attention.self.key.bias False
pretrained_model.encoder.layer.19.attention.self.value.weight False
pretrained_model.encoder.layer.19.attention.self.value.bias False
pretrained_model.encoder.layer.19.attention.output.dense.weight False
pretrained_model.encoder.layer.19.attention.output.dense.bias False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.intermediate.dense.weight False
pretrained_model.encoder.layer.19.intermediate.dense.bias False
pretrained_model.encoder.layer.19.output.dense.weight False
pretrained_model.encoder.layer.19.output.dense.bias False
pretrained_model.encoder.layer.19.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.attention.self.query.weight False
pretrained_model.encoder.layer.20.attention.self.query.bias False
pretrained_model.encoder.layer.20.attention.self.key.weight False
pretrained_model.encoder.layer.20.attention.self.key.bias False
pretrained_model.encoder.layer.20.attention.self.value.weight False
pretrained_model.encoder.layer.20.attention.self.value.bias False
pretrained_model.encoder.layer.20.attention.output.dense.weight False
pretrained_model.encoder.layer.20.attention.output.dense.bias False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.intermediate.dense.weight False
pretrained_model.encoder.layer.20.intermediate.dense.bias False
pretrained_model.encoder.layer.20.output.dense.weight False
pretrained_model.encoder.layer.20.output.dense.bias False
pretrained_model.encoder.layer.20.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.attention.self.query.weight False
pretrained_model.encoder.layer.21.attention.self.query.bias False
pretrained_model.encoder.layer.21.attention.self.key.weight False
pretrained_model.encoder.layer.21.attention.self.key.bias False
pretrained_model.encoder.layer.21.attention.self.value.weight False
pretrained_model.encoder.layer.21.attention.self.value.bias False
pretrained_model.encoder.layer.21.attention.output.dense.weight False
pretrained_model.encoder.layer.21.attention.output.dense.bias False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.intermediate.dense.weight False
pretrained_model.encoder.layer.21.intermediate.dense.bias False
pretrained_model.encoder.layer.21.output.dense.weight False
pretrained_model.encoder.layer.21.output.dense.bias False
pretrained_model.encoder.layer.21.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.attention.self.query.weight False
pretrained_model.encoder.layer.22.attention.self.query.bias False
pretrained_model.encoder.layer.22.attention.self.key.weight False
pretrained_model.encoder.layer.22.attention.self.key.bias False
pretrained_model.encoder.layer.22.attention.self.value.weight False
pretrained_model.encoder.layer.22.attention.self.value.bias False
pretrained_model.encoder.layer.22.attention.output.dense.weight False
pretrained_model.encoder.layer.22.attention.output.dense.bias False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.intermediate.dense.weight False
pretrained_model.encoder.layer.22.intermediate.dense.bias False
pretrained_model.encoder.layer.22.output.dense.weight False
pretrained_model.encoder.layer.22.output.dense.bias False
pretrained_model.encoder.layer.22.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.attention.self.query.weight False
pretrained_model.encoder.layer.23.attention.self.query.bias False
pretrained_model.encoder.layer.23.attention.self.key.weight False
pretrained_model.encoder.layer.23.attention.self.key.bias False
pretrained_model.encoder.layer.23.attention.self.value.weight False
pretrained_model.encoder.layer.23.attention.self.value.bias False
pretrained_model.encoder.layer.23.attention.output.dense.weight False
pretrained_model.encoder.layer.23.attention.output.dense.bias False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.intermediate.dense.weight False
pretrained_model.encoder.layer.23.intermediate.dense.bias False
pretrained_model.encoder.layer.23.output.dense.weight False
pretrained_model.encoder.layer.23.output.dense.bias False
pretrained_model.encoder.layer.23.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.output.LayerNorm.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 30s, train loss=2.6202, train acc=10.76%, dev loss=2.5029, dev acc=16.36%
saving, test loss=2.5100, test acc=15.59%
epoch: 2/10000, 30s, train loss=2.4914, train acc=18.40%, dev loss=2.3990, dev acc=25.90%
saving, test loss=2.4045, test acc=25.00%
epoch: 3/10000, 30s, train loss=2.4025, train acc=24.52%, dev loss=2.3099, dev acc=31.48%
saving, test loss=2.3161, test acc=30.89%
epoch: 4/10000, 30s, train loss=2.3273, train acc=29.02%, dev loss=2.2315, dev acc=35.53%
saving, test loss=2.2380, test acc=35.11%
epoch: 5/10000, 30s, train loss=2.2605, train acc=31.45%, dev loss=2.1632, dev acc=38.27%
saving, test loss=2.1696, test acc=37.95%
epoch: 6/10000, 30s, train loss=2.2078, train acc=33.07%, dev loss=2.1032, dev acc=40.12%
saving, test loss=2.1098, test acc=40.26%
epoch: 7/10000, 30s, train loss=2.1599, train acc=34.38%, dev loss=2.0513, dev acc=41.66%
saving, test loss=2.0575, test acc=42.06%
epoch: 8/10000, 30s, train loss=2.1209, train acc=35.65%, dev loss=2.0054, dev acc=42.80%
saving, test loss=2.0115, test acc=43.42%
epoch: 9/10000, 30s, train loss=2.0865, train acc=36.12%, dev loss=1.9650, dev acc=43.68%
saving, test loss=1.9708, test acc=44.35%
epoch: 10/10000, 30s, train loss=2.0585, train acc=36.85%, dev loss=1.9298, dev acc=44.25%
saving, test loss=1.9355, test acc=45.04%
epoch: 11/10000, 30s, train loss=2.0335, train acc=37.66%, dev loss=1.8986, dev acc=44.62%
saving, test loss=1.9040, test acc=45.78%
epoch: 12/10000, 30s, train loss=2.0088, train acc=37.96%, dev loss=1.8707, dev acc=44.90%
saving, test loss=1.8759, test acc=46.50%
epoch: 13/10000, 30s, train loss=1.9940, train acc=38.14%, dev loss=1.8459, dev acc=45.52%
saving, test loss=1.8510, test acc=46.94%
epoch: 14/10000, 30s, train loss=1.9745, train acc=38.65%, dev loss=1.8240, dev acc=45.86%
saving, test loss=1.8288, test acc=47.12%
epoch: 15/10000, 30s, train loss=1.9624, train acc=38.92%, dev loss=1.8039, dev acc=46.10%
saving, test loss=1.8086, test acc=47.46%
epoch: 16/10000, 30s, train loss=1.9509, train acc=39.10%, dev loss=1.7864, dev acc=46.38%
saving, test loss=1.7909, test acc=47.59%
epoch: 17/10000, 30s, train loss=1.9380, train acc=39.43%, dev loss=1.7703, dev acc=46.96%
saving, test loss=1.7749, test acc=47.74%
epoch: 18/10000, 30s, train loss=1.9277, train acc=39.49%, dev loss=1.7558, dev acc=47.15%
saving, test loss=1.7602, test acc=47.88%
epoch: 19/10000, 30s, train loss=1.9188, train acc=39.71%, dev loss=1.7429, dev acc=47.41%
saving, test loss=1.7470, test acc=48.23%
epoch: 20/10000, 30s, train loss=1.9132, train acc=39.97%, dev loss=1.7306, dev acc=47.60%
saving, test loss=1.7349, test acc=48.51%
epoch: 21/10000, 30s, train loss=1.9062, train acc=39.87%, dev loss=1.7196, dev acc=47.83%
saving, test loss=1.7237, test acc=48.60%
epoch: 22/10000, 30s, train loss=1.8967, train acc=40.29%, dev loss=1.7099, dev acc=47.83%
epoch: 23/10000, 30s, train loss=1.8905, train acc=40.27%, dev loss=1.7009, dev acc=48.03%
saving, test loss=1.7047, test acc=48.68%
epoch: 24/10000, 30s, train loss=1.8874, train acc=40.24%, dev loss=1.6923, dev acc=48.16%
saving, test loss=1.6960, test acc=48.99%
epoch: 25/10000, 30s, train loss=1.8813, train acc=40.54%, dev loss=1.6850, dev acc=48.20%
saving, test loss=1.6884, test acc=48.99%
epoch: 26/10000, 30s, train loss=1.8726, train acc=40.61%, dev loss=1.6772, dev acc=48.46%
saving, test loss=1.6806, test acc=49.10%
epoch: 27/10000, 30s, train loss=1.8753, train acc=40.53%, dev loss=1.6700, dev acc=48.52%
saving, test loss=1.6737, test acc=49.24%
epoch: 28/10000, 30s, train loss=1.8647, train acc=40.75%, dev loss=1.6639, dev acc=48.61%
saving, test loss=1.6675, test acc=49.36%
epoch: 29/10000, 30s, train loss=1.8607, train acc=40.57%, dev loss=1.6585, dev acc=48.73%
saving, test loss=1.6620, test acc=49.48%
epoch: 30/10000, 30s, train loss=1.8587, train acc=40.80%, dev loss=1.6530, dev acc=48.63%
epoch: 31/10000, 30s, train loss=1.8641, train acc=40.58%, dev loss=1.6474, dev acc=48.88%
saving, test loss=1.6509, test acc=49.47%
epoch: 32/10000, 30s, train loss=1.8583, train acc=40.84%, dev loss=1.6431, dev acc=48.95%
saving, test loss=1.6463, test acc=49.64%
epoch: 33/10000, 30s, train loss=1.8559, train acc=40.90%, dev loss=1.6389, dev acc=48.91%
epoch: 34/10000, 30s, train loss=1.8518, train acc=41.29%, dev loss=1.6347, dev acc=49.04%
saving, test loss=1.6379, test acc=49.61%
epoch: 35/10000, 30s, train loss=1.8535, train acc=41.01%, dev loss=1.6311, dev acc=48.93%
epoch: 36/10000, 30s, train loss=1.8472, train acc=40.93%, dev loss=1.6274, dev acc=48.95%
epoch: 37/10000, 30s, train loss=1.8497, train acc=40.93%, dev loss=1.6242, dev acc=49.12%
saving, test loss=1.6276, test acc=49.80%
epoch: 38/10000, 30s, train loss=1.8494, train acc=40.89%, dev loss=1.6212, dev acc=48.93%
epoch: 39/10000, 30s, train loss=1.8452, train acc=41.03%, dev loss=1.6185, dev acc=48.99%
epoch: 40/10000, 30s, train loss=1.8382, train acc=41.34%, dev loss=1.6159, dev acc=48.99%
epoch: 41/10000, 30s, train loss=1.8370, train acc=41.48%, dev loss=1.6130, dev acc=49.08%
epoch: 42/10000, 30s, train loss=1.8430, train acc=41.26%, dev loss=1.6106, dev acc=48.99%
time used=1630.7s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='1', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-electra-small-ex-discriminator', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_entertainment': 0, 'news_sports': 1, 'news_travel': 2, 'news_world': 3, 'news_stock': 4, 'news_finance': 5, 'news_military': 6, 'news_edu': 7, 'news_game': 8, 'news_story': 9, 'news_tech': 10, 'news_culture': 11, 'news_house': 12, 'news_car': 13, 'news_agriculture': 14}
index_labels_dict={0: 'news_entertainment', 1: 'news_sports', 2: 'news_travel', 3: 'news_world', 4: 'news_stock', 5: 'news_finance', 6: 'news_military', 7: 'news_edu', 8: 'news_game', 9: 'news_story', 10: 'news_tech', 11: 'news_culture', 12: 'news_house', 13: 'news_car', 14: 'news_agriculture'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-electra-small-ex-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']
- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(21128, 256, padding_idx=0)
      (position_embeddings): Embedding(512, 256)
      (token_type_embeddings): Embedding(2, 256)
      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (12): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (13): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (14): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (15): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (16): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (17): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (18): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (19): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (20): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (21): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (22): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (23): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=256, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.attention.self.query.weight False
pretrained_model.encoder.layer.12.attention.self.query.bias False
pretrained_model.encoder.layer.12.attention.self.key.weight False
pretrained_model.encoder.layer.12.attention.self.key.bias False
pretrained_model.encoder.layer.12.attention.self.value.weight False
pretrained_model.encoder.layer.12.attention.self.value.bias False
pretrained_model.encoder.layer.12.attention.output.dense.weight False
pretrained_model.encoder.layer.12.attention.output.dense.bias False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.intermediate.dense.weight False
pretrained_model.encoder.layer.12.intermediate.dense.bias False
pretrained_model.encoder.layer.12.output.dense.weight False
pretrained_model.encoder.layer.12.output.dense.bias False
pretrained_model.encoder.layer.12.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.attention.self.query.weight False
pretrained_model.encoder.layer.13.attention.self.query.bias False
pretrained_model.encoder.layer.13.attention.self.key.weight False
pretrained_model.encoder.layer.13.attention.self.key.bias False
pretrained_model.encoder.layer.13.attention.self.value.weight False
pretrained_model.encoder.layer.13.attention.self.value.bias False
pretrained_model.encoder.layer.13.attention.output.dense.weight False
pretrained_model.encoder.layer.13.attention.output.dense.bias False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.intermediate.dense.weight False
pretrained_model.encoder.layer.13.intermediate.dense.bias False
pretrained_model.encoder.layer.13.output.dense.weight False
pretrained_model.encoder.layer.13.output.dense.bias False
pretrained_model.encoder.layer.13.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.attention.self.query.weight False
pretrained_model.encoder.layer.14.attention.self.query.bias False
pretrained_model.encoder.layer.14.attention.self.key.weight False
pretrained_model.encoder.layer.14.attention.self.key.bias False
pretrained_model.encoder.layer.14.attention.self.value.weight False
pretrained_model.encoder.layer.14.attention.self.value.bias False
pretrained_model.encoder.layer.14.attention.output.dense.weight False
pretrained_model.encoder.layer.14.attention.output.dense.bias False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.intermediate.dense.weight False
pretrained_model.encoder.layer.14.intermediate.dense.bias False
pretrained_model.encoder.layer.14.output.dense.weight False
pretrained_model.encoder.layer.14.output.dense.bias False
pretrained_model.encoder.layer.14.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.attention.self.query.weight False
pretrained_model.encoder.layer.15.attention.self.query.bias False
pretrained_model.encoder.layer.15.attention.self.key.weight False
pretrained_model.encoder.layer.15.attention.self.key.bias False
pretrained_model.encoder.layer.15.attention.self.value.weight False
pretrained_model.encoder.layer.15.attention.self.value.bias False
pretrained_model.encoder.layer.15.attention.output.dense.weight False
pretrained_model.encoder.layer.15.attention.output.dense.bias False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.intermediate.dense.weight False
pretrained_model.encoder.layer.15.intermediate.dense.bias False
pretrained_model.encoder.layer.15.output.dense.weight False
pretrained_model.encoder.layer.15.output.dense.bias False
pretrained_model.encoder.layer.15.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.attention.self.query.weight False
pretrained_model.encoder.layer.16.attention.self.query.bias False
pretrained_model.encoder.layer.16.attention.self.key.weight False
pretrained_model.encoder.layer.16.attention.self.key.bias False
pretrained_model.encoder.layer.16.attention.self.value.weight False
pretrained_model.encoder.layer.16.attention.self.value.bias False
pretrained_model.encoder.layer.16.attention.output.dense.weight False
pretrained_model.encoder.layer.16.attention.output.dense.bias False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.intermediate.dense.weight False
pretrained_model.encoder.layer.16.intermediate.dense.bias False
pretrained_model.encoder.layer.16.output.dense.weight False
pretrained_model.encoder.layer.16.output.dense.bias False
pretrained_model.encoder.layer.16.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.attention.self.query.weight False
pretrained_model.encoder.layer.17.attention.self.query.bias False
pretrained_model.encoder.layer.17.attention.self.key.weight False
pretrained_model.encoder.layer.17.attention.self.key.bias False
pretrained_model.encoder.layer.17.attention.self.value.weight False
pretrained_model.encoder.layer.17.attention.self.value.bias False
pretrained_model.encoder.layer.17.attention.output.dense.weight False
pretrained_model.encoder.layer.17.attention.output.dense.bias False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.intermediate.dense.weight False
pretrained_model.encoder.layer.17.intermediate.dense.bias False
pretrained_model.encoder.layer.17.output.dense.weight False
pretrained_model.encoder.layer.17.output.dense.bias False
pretrained_model.encoder.layer.17.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.attention.self.query.weight False
pretrained_model.encoder.layer.18.attention.self.query.bias False
pretrained_model.encoder.layer.18.attention.self.key.weight False
pretrained_model.encoder.layer.18.attention.self.key.bias False
pretrained_model.encoder.layer.18.attention.self.value.weight False
pretrained_model.encoder.layer.18.attention.self.value.bias False
pretrained_model.encoder.layer.18.attention.output.dense.weight False
pretrained_model.encoder.layer.18.attention.output.dense.bias False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.intermediate.dense.weight False
pretrained_model.encoder.layer.18.intermediate.dense.bias False
pretrained_model.encoder.layer.18.output.dense.weight False
pretrained_model.encoder.layer.18.output.dense.bias False
pretrained_model.encoder.layer.18.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.attention.self.query.weight False
pretrained_model.encoder.layer.19.attention.self.query.bias False
pretrained_model.encoder.layer.19.attention.self.key.weight False
pretrained_model.encoder.layer.19.attention.self.key.bias False
pretrained_model.encoder.layer.19.attention.self.value.weight False
pretrained_model.encoder.layer.19.attention.self.value.bias False
pretrained_model.encoder.layer.19.attention.output.dense.weight False
pretrained_model.encoder.layer.19.attention.output.dense.bias False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.intermediate.dense.weight False
pretrained_model.encoder.layer.19.intermediate.dense.bias False
pretrained_model.encoder.layer.19.output.dense.weight False
pretrained_model.encoder.layer.19.output.dense.bias False
pretrained_model.encoder.layer.19.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.attention.self.query.weight False
pretrained_model.encoder.layer.20.attention.self.query.bias False
pretrained_model.encoder.layer.20.attention.self.key.weight False
pretrained_model.encoder.layer.20.attention.self.key.bias False
pretrained_model.encoder.layer.20.attention.self.value.weight False
pretrained_model.encoder.layer.20.attention.self.value.bias False
pretrained_model.encoder.layer.20.attention.output.dense.weight False
pretrained_model.encoder.layer.20.attention.output.dense.bias False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.intermediate.dense.weight False
pretrained_model.encoder.layer.20.intermediate.dense.bias False
pretrained_model.encoder.layer.20.output.dense.weight False
pretrained_model.encoder.layer.20.output.dense.bias False
pretrained_model.encoder.layer.20.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.attention.self.query.weight False
pretrained_model.encoder.layer.21.attention.self.query.bias False
pretrained_model.encoder.layer.21.attention.self.key.weight False
pretrained_model.encoder.layer.21.attention.self.key.bias False
pretrained_model.encoder.layer.21.attention.self.value.weight False
pretrained_model.encoder.layer.21.attention.self.value.bias False
pretrained_model.encoder.layer.21.attention.output.dense.weight False
pretrained_model.encoder.layer.21.attention.output.dense.bias False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.intermediate.dense.weight False
pretrained_model.encoder.layer.21.intermediate.dense.bias False
pretrained_model.encoder.layer.21.output.dense.weight False
pretrained_model.encoder.layer.21.output.dense.bias False
pretrained_model.encoder.layer.21.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.attention.self.query.weight False
pretrained_model.encoder.layer.22.attention.self.query.bias False
pretrained_model.encoder.layer.22.attention.self.key.weight False
pretrained_model.encoder.layer.22.attention.self.key.bias False
pretrained_model.encoder.layer.22.attention.self.value.weight False
pretrained_model.encoder.layer.22.attention.self.value.bias False
pretrained_model.encoder.layer.22.attention.output.dense.weight False
pretrained_model.encoder.layer.22.attention.output.dense.bias False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.intermediate.dense.weight False
pretrained_model.encoder.layer.22.intermediate.dense.bias False
pretrained_model.encoder.layer.22.output.dense.weight False
pretrained_model.encoder.layer.22.output.dense.bias False
pretrained_model.encoder.layer.22.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.attention.self.query.weight False
pretrained_model.encoder.layer.23.attention.self.query.bias False
pretrained_model.encoder.layer.23.attention.self.key.weight False
pretrained_model.encoder.layer.23.attention.self.key.bias False
pretrained_model.encoder.layer.23.attention.self.value.weight False
pretrained_model.encoder.layer.23.attention.self.value.bias False
pretrained_model.encoder.layer.23.attention.output.dense.weight False
pretrained_model.encoder.layer.23.attention.output.dense.bias False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.intermediate.dense.weight False
pretrained_model.encoder.layer.23.intermediate.dense.bias False
pretrained_model.encoder.layer.23.output.dense.weight False
pretrained_model.encoder.layer.23.output.dense.bias False
pretrained_model.encoder.layer.23.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.output.LayerNorm.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 30s, train loss=2.6797, train acc=9.39%, dev loss=2.5807, dev acc=11.47%
saving, test loss=2.5859, test acc=11.41%
epoch: 2/10000, 30s, train loss=2.5922, train acc=11.98%, dev loss=2.5448, dev acc=15.20%
saving, test loss=2.5502, test acc=14.82%
epoch: 3/10000, 30s, train loss=2.5552, train acc=14.53%, dev loss=2.5216, dev acc=16.30%
saving, test loss=2.5268, test acc=15.66%
epoch: 4/10000, 30s, train loss=2.5324, train acc=16.42%, dev loss=2.5023, dev acc=18.76%
saving, test loss=2.5076, test acc=17.96%
epoch: 5/10000, 30s, train loss=2.5155, train acc=17.48%, dev loss=2.4855, dev acc=19.98%
saving, test loss=2.4909, test acc=18.86%
epoch: 6/10000, 30s, train loss=2.4980, train acc=18.66%, dev loss=2.4703, dev acc=20.39%
saving, test loss=2.4759, test acc=19.53%
epoch: 7/10000, 30s, train loss=2.4877, train acc=19.24%, dev loss=2.4563, dev acc=20.80%
saving, test loss=2.4617, test acc=20.02%
epoch: 8/10000, 30s, train loss=2.4758, train acc=19.68%, dev loss=2.4442, dev acc=21.10%
saving, test loss=2.4493, test acc=20.30%
epoch: 9/10000, 30s, train loss=2.4652, train acc=20.14%, dev loss=2.4328, dev acc=21.50%
saving, test loss=2.4379, test acc=20.41%
epoch: 10/10000, 30s, train loss=2.4575, train acc=20.28%, dev loss=2.4222, dev acc=21.70%
saving, test loss=2.4269, test acc=20.72%
epoch: 11/10000, 30s, train loss=2.4467, train acc=20.46%, dev loss=2.4128, dev acc=21.76%
saving, test loss=2.4173, test acc=20.90%
epoch: 12/10000, 30s, train loss=2.4402, train acc=20.79%, dev loss=2.4039, dev acc=22.36%
saving, test loss=2.4080, test acc=21.09%
epoch: 13/10000, 30s, train loss=2.4324, train acc=21.11%, dev loss=2.3964, dev acc=22.38%
saving, test loss=2.4002, test acc=21.14%
epoch: 14/10000, 30s, train loss=2.4276, train acc=21.18%, dev loss=2.3882, dev acc=22.71%
saving, test loss=2.3917, test acc=21.69%
epoch: 15/10000, 30s, train loss=2.4244, train acc=21.04%, dev loss=2.3822, dev acc=22.62%
epoch: 16/10000, 30s, train loss=2.4164, train acc=21.17%, dev loss=2.3753, dev acc=22.84%
saving, test loss=2.3782, test acc=21.86%
epoch: 17/10000, 30s, train loss=2.4109, train acc=21.77%, dev loss=2.3693, dev acc=23.01%
saving, test loss=2.3722, test acc=22.04%
epoch: 18/10000, 30s, train loss=2.4082, train acc=21.71%, dev loss=2.3638, dev acc=22.94%
epoch: 19/10000, 30s, train loss=2.4036, train acc=21.60%, dev loss=2.3581, dev acc=23.18%
saving, test loss=2.3605, test acc=22.44%
epoch: 20/10000, 30s, train loss=2.4022, train acc=21.73%, dev loss=2.3532, dev acc=23.35%
saving, test loss=2.3556, test acc=22.63%
epoch: 21/10000, 30s, train loss=2.3978, train acc=21.96%, dev loss=2.3487, dev acc=23.56%
saving, test loss=2.3508, test acc=22.78%
epoch: 22/10000, 30s, train loss=2.3929, train acc=22.17%, dev loss=2.3439, dev acc=23.46%
epoch: 23/10000, 30s, train loss=2.3903, train acc=22.12%, dev loss=2.3396, dev acc=23.80%
saving, test loss=2.3411, test acc=23.17%
epoch: 24/10000, 30s, train loss=2.3889, train acc=22.14%, dev loss=2.3360, dev acc=23.58%
epoch: 25/10000, 30s, train loss=2.3847, train acc=22.55%, dev loss=2.3321, dev acc=23.86%
saving, test loss=2.3333, test acc=23.44%
epoch: 26/10000, 30s, train loss=2.3847, train acc=22.26%, dev loss=2.3287, dev acc=24.04%
saving, test loss=2.3300, test acc=23.52%
epoch: 27/10000, 30s, train loss=2.3825, train acc=22.56%, dev loss=2.3249, dev acc=24.08%
saving, test loss=2.3261, test acc=23.71%
epoch: 28/10000, 30s, train loss=2.3782, train acc=22.38%, dev loss=2.3216, dev acc=24.12%
saving, test loss=2.3225, test acc=23.78%
epoch: 29/10000, 30s, train loss=2.3782, train acc=22.44%, dev loss=2.3182, dev acc=24.25%
saving, test loss=2.3189, test acc=23.93%
epoch: 30/10000, 30s, train loss=2.3741, train acc=22.77%, dev loss=2.3158, dev acc=24.36%
saving, test loss=2.3165, test acc=24.03%
epoch: 31/10000, 30s, train loss=2.3722, train acc=22.78%, dev loss=2.3130, dev acc=24.44%
saving, test loss=2.3137, test acc=24.05%
epoch: 32/10000, 30s, train loss=2.3727, train acc=22.86%, dev loss=2.3096, dev acc=24.72%
saving, test loss=2.3098, test acc=24.44%
epoch: 33/10000, 30s, train loss=2.3726, train acc=22.46%, dev loss=2.3071, dev acc=24.61%
epoch: 34/10000, 30s, train loss=2.3680, train acc=22.82%, dev loss=2.3048, dev acc=24.66%
epoch: 35/10000, 30s, train loss=2.3684, train acc=22.83%, dev loss=2.3024, dev acc=24.85%
saving, test loss=2.3021, test acc=24.59%
epoch: 36/10000, 30s, train loss=2.3644, train acc=22.97%, dev loss=2.3007, dev acc=24.79%
epoch: 37/10000, 30s, train loss=2.3678, train acc=22.63%, dev loss=2.2977, dev acc=24.93%
saving, test loss=2.2971, test acc=24.61%
epoch: 38/10000, 30s, train loss=2.3628, train acc=23.09%, dev loss=2.2956, dev acc=25.11%
saving, test loss=2.2948, test acc=24.86%
epoch: 39/10000, 30s, train loss=2.3629, train acc=23.06%, dev loss=2.2942, dev acc=25.07%
epoch: 40/10000, 30s, train loss=2.3622, train acc=23.12%, dev loss=2.2923, dev acc=25.02%
epoch: 41/10000, 30s, train loss=2.3616, train acc=22.77%, dev loss=2.2909, dev acc=25.28%
saving, test loss=2.2901, test acc=24.93%
epoch: 42/10000, 30s, train loss=2.3608, train acc=23.06%, dev loss=2.2891, dev acc=25.11%
epoch: 43/10000, 29s, train loss=2.3600, train acc=23.15%, dev loss=2.2872, dev acc=25.32%
saving, test loss=2.2862, test acc=25.08%
epoch: 44/10000, 30s, train loss=2.3586, train acc=22.96%, dev loss=2.2852, dev acc=25.36%
saving, test loss=2.2840, test acc=25.18%
epoch: 45/10000, 30s, train loss=2.3572, train acc=23.10%, dev loss=2.2836, dev acc=25.51%
saving, test loss=2.2820, test acc=25.30%
epoch: 46/10000, 30s, train loss=2.3565, train acc=23.29%, dev loss=2.2827, dev acc=25.41%
epoch: 47/10000, 30s, train loss=2.3566, train acc=23.20%, dev loss=2.2813, dev acc=25.49%
epoch: 48/10000, 29s, train loss=2.3577, train acc=23.05%, dev loss=2.2798, dev acc=25.71%
saving, test loss=2.2781, test acc=25.44%
epoch: 49/10000, 30s, train loss=2.3554, train acc=23.14%, dev loss=2.2781, dev acc=25.64%
epoch: 50/10000, 29s, train loss=2.3533, train acc=23.16%, dev loss=2.2768, dev acc=25.82%
saving, test loss=2.2747, test acc=25.57%
epoch: 51/10000, 29s, train loss=2.3527, train acc=23.23%, dev loss=2.2759, dev acc=25.81%
epoch: 52/10000, 29s, train loss=2.3492, train acc=23.43%, dev loss=2.2744, dev acc=25.79%
epoch: 53/10000, 29s, train loss=2.3529, train acc=23.19%, dev loss=2.2731, dev acc=25.82%
epoch: 54/10000, 30s, train loss=2.3513, train acc=23.32%, dev loss=2.2720, dev acc=25.84%
saving, test loss=2.2700, test acc=25.63%
epoch: 55/10000, 29s, train loss=2.3506, train acc=23.07%, dev loss=2.2705, dev acc=25.94%
saving, test loss=2.2683, test acc=25.61%
epoch: 56/10000, 30s, train loss=2.3495, train acc=23.27%, dev loss=2.2698, dev acc=26.01%
saving, test loss=2.2675, test acc=25.72%
epoch: 57/10000, 30s, train loss=2.3522, train acc=23.13%, dev loss=2.2683, dev acc=26.11%
saving, test loss=2.2659, test acc=25.71%
epoch: 58/10000, 30s, train loss=2.3497, train acc=23.43%, dev loss=2.2669, dev acc=26.20%
saving, test loss=2.2644, test acc=25.84%
epoch: 59/10000, 30s, train loss=2.3519, train acc=23.27%, dev loss=2.2670, dev acc=26.11%
epoch: 60/10000, 29s, train loss=2.3508, train acc=23.41%, dev loss=2.2659, dev acc=26.14%
epoch: 61/10000, 30s, train loss=2.3481, train acc=23.51%, dev loss=2.2646, dev acc=26.11%
epoch: 62/10000, 30s, train loss=2.3464, train acc=23.32%, dev loss=2.2632, dev acc=26.26%
saving, test loss=2.2607, test acc=25.95%
epoch: 63/10000, 30s, train loss=2.3499, train acc=23.40%, dev loss=2.2617, dev acc=26.42%
saving, test loss=2.2591, test acc=25.97%
epoch: 64/10000, 30s, train loss=2.3479, train acc=23.52%, dev loss=2.2615, dev acc=26.48%
saving, test loss=2.2588, test acc=25.89%
epoch: 65/10000, 30s, train loss=2.3495, train acc=23.25%, dev loss=2.2601, dev acc=26.56%
saving, test loss=2.2573, test acc=25.98%
epoch: 66/10000, 30s, train loss=2.3476, train acc=23.56%, dev loss=2.2597, dev acc=26.41%
epoch: 67/10000, 29s, train loss=2.3450, train acc=23.35%, dev loss=2.2592, dev acc=26.59%
saving, test loss=2.2563, test acc=26.04%
epoch: 68/10000, 29s, train loss=2.3453, train acc=23.42%, dev loss=2.2589, dev acc=26.46%
epoch: 69/10000, 30s, train loss=2.3453, train acc=23.62%, dev loss=2.2587, dev acc=26.52%
epoch: 70/10000, 29s, train loss=2.3471, train acc=23.60%, dev loss=2.2585, dev acc=26.39%
epoch: 71/10000, 29s, train loss=2.3448, train acc=23.57%, dev loss=2.2581, dev acc=26.37%
epoch: 72/10000, 29s, train loss=2.3429, train acc=23.44%, dev loss=2.2574, dev acc=26.41%
time used=2686.6s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='1', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-electra-small-ex-generator', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_story': 0, 'news_tech': 1, 'news_finance': 2, 'news_sports': 3, 'news_edu': 4, 'news_game': 5, 'news_agriculture': 6, 'news_military': 7, 'news_travel': 8, 'news_world': 9, 'news_car': 10, 'news_stock': 11, 'news_culture': 12, 'news_house': 13, 'news_entertainment': 14}
index_labels_dict={0: 'news_story', 1: 'news_tech', 2: 'news_finance', 3: 'news_sports', 4: 'news_edu', 5: 'news_game', 6: 'news_agriculture', 7: 'news_military', 8: 'news_travel', 9: 'news_world', 10: 'news_car', 11: 'news_stock', 12: 'news_culture', 13: 'news_house', 14: 'news_entertainment'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-electra-small-ex-generator were not used when initializing ElectraModel: ['generator_lm_head.bias', 'generator_predictions.dense.weight', 'generator_predictions.LayerNorm.bias', 'generator_predictions.LayerNorm.weight', 'generator_lm_head.weight', 'generator_predictions.dense.bias']
- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(21128, 256, padding_idx=0)
      (position_embeddings): Embedding(512, 256)
      (token_type_embeddings): Embedding(2, 256)
      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (embeddings_project): Linear(in_features=256, out_features=64, bias=True)
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (12): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (13): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (14): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (15): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (16): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (17): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (18): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (19): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (20): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (21): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (22): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (23): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=64, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.embeddings_project.weight False
pretrained_model.embeddings_project.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.attention.self.query.weight False
pretrained_model.encoder.layer.12.attention.self.query.bias False
pretrained_model.encoder.layer.12.attention.self.key.weight False
pretrained_model.encoder.layer.12.attention.self.key.bias False
pretrained_model.encoder.layer.12.attention.self.value.weight False
pretrained_model.encoder.layer.12.attention.self.value.bias False
pretrained_model.encoder.layer.12.attention.output.dense.weight False
pretrained_model.encoder.layer.12.attention.output.dense.bias False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.intermediate.dense.weight False
pretrained_model.encoder.layer.12.intermediate.dense.bias False
pretrained_model.encoder.layer.12.output.dense.weight False
pretrained_model.encoder.layer.12.output.dense.bias False
pretrained_model.encoder.layer.12.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.attention.self.query.weight False
pretrained_model.encoder.layer.13.attention.self.query.bias False
pretrained_model.encoder.layer.13.attention.self.key.weight False
pretrained_model.encoder.layer.13.attention.self.key.bias False
pretrained_model.encoder.layer.13.attention.self.value.weight False
pretrained_model.encoder.layer.13.attention.self.value.bias False
pretrained_model.encoder.layer.13.attention.output.dense.weight False
pretrained_model.encoder.layer.13.attention.output.dense.bias False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.intermediate.dense.weight False
pretrained_model.encoder.layer.13.intermediate.dense.bias False
pretrained_model.encoder.layer.13.output.dense.weight False
pretrained_model.encoder.layer.13.output.dense.bias False
pretrained_model.encoder.layer.13.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.attention.self.query.weight False
pretrained_model.encoder.layer.14.attention.self.query.bias False
pretrained_model.encoder.layer.14.attention.self.key.weight False
pretrained_model.encoder.layer.14.attention.self.key.bias False
pretrained_model.encoder.layer.14.attention.self.value.weight False
pretrained_model.encoder.layer.14.attention.self.value.bias False
pretrained_model.encoder.layer.14.attention.output.dense.weight False
pretrained_model.encoder.layer.14.attention.output.dense.bias False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.intermediate.dense.weight False
pretrained_model.encoder.layer.14.intermediate.dense.bias False
pretrained_model.encoder.layer.14.output.dense.weight False
pretrained_model.encoder.layer.14.output.dense.bias False
pretrained_model.encoder.layer.14.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.attention.self.query.weight False
pretrained_model.encoder.layer.15.attention.self.query.bias False
pretrained_model.encoder.layer.15.attention.self.key.weight False
pretrained_model.encoder.layer.15.attention.self.key.bias False
pretrained_model.encoder.layer.15.attention.self.value.weight False
pretrained_model.encoder.layer.15.attention.self.value.bias False
pretrained_model.encoder.layer.15.attention.output.dense.weight False
pretrained_model.encoder.layer.15.attention.output.dense.bias False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.intermediate.dense.weight False
pretrained_model.encoder.layer.15.intermediate.dense.bias False
pretrained_model.encoder.layer.15.output.dense.weight False
pretrained_model.encoder.layer.15.output.dense.bias False
pretrained_model.encoder.layer.15.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.attention.self.query.weight False
pretrained_model.encoder.layer.16.attention.self.query.bias False
pretrained_model.encoder.layer.16.attention.self.key.weight False
pretrained_model.encoder.layer.16.attention.self.key.bias False
pretrained_model.encoder.layer.16.attention.self.value.weight False
pretrained_model.encoder.layer.16.attention.self.value.bias False
pretrained_model.encoder.layer.16.attention.output.dense.weight False
pretrained_model.encoder.layer.16.attention.output.dense.bias False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.intermediate.dense.weight False
pretrained_model.encoder.layer.16.intermediate.dense.bias False
pretrained_model.encoder.layer.16.output.dense.weight False
pretrained_model.encoder.layer.16.output.dense.bias False
pretrained_model.encoder.layer.16.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.attention.self.query.weight False
pretrained_model.encoder.layer.17.attention.self.query.bias False
pretrained_model.encoder.layer.17.attention.self.key.weight False
pretrained_model.encoder.layer.17.attention.self.key.bias False
pretrained_model.encoder.layer.17.attention.self.value.weight False
pretrained_model.encoder.layer.17.attention.self.value.bias False
pretrained_model.encoder.layer.17.attention.output.dense.weight False
pretrained_model.encoder.layer.17.attention.output.dense.bias False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.intermediate.dense.weight False
pretrained_model.encoder.layer.17.intermediate.dense.bias False
pretrained_model.encoder.layer.17.output.dense.weight False
pretrained_model.encoder.layer.17.output.dense.bias False
pretrained_model.encoder.layer.17.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.attention.self.query.weight False
pretrained_model.encoder.layer.18.attention.self.query.bias False
pretrained_model.encoder.layer.18.attention.self.key.weight False
pretrained_model.encoder.layer.18.attention.self.key.bias False
pretrained_model.encoder.layer.18.attention.self.value.weight False
pretrained_model.encoder.layer.18.attention.self.value.bias False
pretrained_model.encoder.layer.18.attention.output.dense.weight False
pretrained_model.encoder.layer.18.attention.output.dense.bias False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.intermediate.dense.weight False
pretrained_model.encoder.layer.18.intermediate.dense.bias False
pretrained_model.encoder.layer.18.output.dense.weight False
pretrained_model.encoder.layer.18.output.dense.bias False
pretrained_model.encoder.layer.18.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.attention.self.query.weight False
pretrained_model.encoder.layer.19.attention.self.query.bias False
pretrained_model.encoder.layer.19.attention.self.key.weight False
pretrained_model.encoder.layer.19.attention.self.key.bias False
pretrained_model.encoder.layer.19.attention.self.value.weight False
pretrained_model.encoder.layer.19.attention.self.value.bias False
pretrained_model.encoder.layer.19.attention.output.dense.weight False
pretrained_model.encoder.layer.19.attention.output.dense.bias False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.intermediate.dense.weight False
pretrained_model.encoder.layer.19.intermediate.dense.bias False
pretrained_model.encoder.layer.19.output.dense.weight False
pretrained_model.encoder.layer.19.output.dense.bias False
pretrained_model.encoder.layer.19.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.attention.self.query.weight False
pretrained_model.encoder.layer.20.attention.self.query.bias False
pretrained_model.encoder.layer.20.attention.self.key.weight False
pretrained_model.encoder.layer.20.attention.self.key.bias False
pretrained_model.encoder.layer.20.attention.self.value.weight False
pretrained_model.encoder.layer.20.attention.self.value.bias False
pretrained_model.encoder.layer.20.attention.output.dense.weight False
pretrained_model.encoder.layer.20.attention.output.dense.bias False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.intermediate.dense.weight False
pretrained_model.encoder.layer.20.intermediate.dense.bias False
pretrained_model.encoder.layer.20.output.dense.weight False
pretrained_model.encoder.layer.20.output.dense.bias False
pretrained_model.encoder.layer.20.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.attention.self.query.weight False
pretrained_model.encoder.layer.21.attention.self.query.bias False
pretrained_model.encoder.layer.21.attention.self.key.weight False
pretrained_model.encoder.layer.21.attention.self.key.bias False
pretrained_model.encoder.layer.21.attention.self.value.weight False
pretrained_model.encoder.layer.21.attention.self.value.bias False
pretrained_model.encoder.layer.21.attention.output.dense.weight False
pretrained_model.encoder.layer.21.attention.output.dense.bias False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.intermediate.dense.weight False
pretrained_model.encoder.layer.21.intermediate.dense.bias False
pretrained_model.encoder.layer.21.output.dense.weight False
pretrained_model.encoder.layer.21.output.dense.bias False
pretrained_model.encoder.layer.21.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.attention.self.query.weight False
pretrained_model.encoder.layer.22.attention.self.query.bias False
pretrained_model.encoder.layer.22.attention.self.key.weight False
pretrained_model.encoder.layer.22.attention.self.key.bias False
pretrained_model.encoder.layer.22.attention.self.value.weight False
pretrained_model.encoder.layer.22.attention.self.value.bias False
pretrained_model.encoder.layer.22.attention.output.dense.weight False
pretrained_model.encoder.layer.22.attention.output.dense.bias False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.intermediate.dense.weight False
pretrained_model.encoder.layer.22.intermediate.dense.bias False
pretrained_model.encoder.layer.22.output.dense.weight False
pretrained_model.encoder.layer.22.output.dense.bias False
pretrained_model.encoder.layer.22.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.attention.self.query.weight False
pretrained_model.encoder.layer.23.attention.self.query.bias False
pretrained_model.encoder.layer.23.attention.self.key.weight False
pretrained_model.encoder.layer.23.attention.self.key.bias False
pretrained_model.encoder.layer.23.attention.self.value.weight False
pretrained_model.encoder.layer.23.attention.self.value.bias False
pretrained_model.encoder.layer.23.attention.output.dense.weight False
pretrained_model.encoder.layer.23.attention.output.dense.bias False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.intermediate.dense.weight False
pretrained_model.encoder.layer.23.intermediate.dense.bias False
pretrained_model.encoder.layer.23.output.dense.weight False
pretrained_model.encoder.layer.23.output.dense.bias False
pretrained_model.encoder.layer.23.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.output.LayerNorm.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 9s, train loss=2.6978, train acc=8.23%, dev loss=2.6229, dev acc=10.70%
saving, test loss=2.6248, test acc=10.32%
epoch: 2/10000, 9s, train loss=2.6155, train acc=11.83%, dev loss=2.5622, dev acc=12.29%
saving, test loss=2.5654, test acc=11.72%
epoch: 3/10000, 9s, train loss=2.5692, train acc=13.32%, dev loss=2.5197, dev acc=14.77%
saving, test loss=2.5234, test acc=14.99%
epoch: 4/10000, 9s, train loss=2.5332, train acc=15.61%, dev loss=2.4844, dev acc=18.52%
saving, test loss=2.4883, test acc=18.45%
epoch: 5/10000, 9s, train loss=2.5052, train acc=17.18%, dev loss=2.4531, dev acc=22.34%
saving, test loss=2.4573, test acc=21.93%
epoch: 6/10000, 9s, train loss=2.4795, train acc=19.11%, dev loss=2.4248, dev acc=24.51%
saving, test loss=2.4290, test acc=24.09%
epoch: 7/10000, 9s, train loss=2.4552, train acc=20.53%, dev loss=2.3984, dev acc=25.88%
saving, test loss=2.4027, test acc=25.52%
epoch: 8/10000, 9s, train loss=2.4331, train acc=21.78%, dev loss=2.3737, dev acc=27.74%
saving, test loss=2.3780, test acc=27.39%
epoch: 9/10000, 9s, train loss=2.4148, train acc=22.78%, dev loss=2.3510, dev acc=28.94%
saving, test loss=2.3553, test acc=28.41%
epoch: 10/10000, 9s, train loss=2.3996, train acc=23.20%, dev loss=2.3297, dev acc=30.00%
saving, test loss=2.3340, test acc=29.36%
epoch: 11/10000, 9s, train loss=2.3822, train acc=24.09%, dev loss=2.3095, dev acc=31.17%
saving, test loss=2.3138, test acc=30.57%
epoch: 12/10000, 9s, train loss=2.3692, train acc=24.37%, dev loss=2.2907, dev acc=32.08%
saving, test loss=2.2949, test acc=31.42%
epoch: 13/10000, 9s, train loss=2.3558, train acc=25.10%, dev loss=2.2729, dev acc=32.72%
saving, test loss=2.2772, test acc=32.09%
epoch: 14/10000, 9s, train loss=2.3446, train acc=25.41%, dev loss=2.2566, dev acc=33.30%
saving, test loss=2.2608, test acc=32.75%
epoch: 15/10000, 9s, train loss=2.3328, train acc=25.69%, dev loss=2.2410, dev acc=33.51%
saving, test loss=2.2453, test acc=33.26%
epoch: 16/10000, 9s, train loss=2.3232, train acc=25.90%, dev loss=2.2265, dev acc=33.70%
saving, test loss=2.2306, test acc=33.65%
epoch: 17/10000, 9s, train loss=2.3113, train acc=26.41%, dev loss=2.2125, dev acc=34.45%
saving, test loss=2.2167, test acc=34.14%
epoch: 18/10000, 9s, train loss=2.3032, train acc=26.48%, dev loss=2.1993, dev acc=35.04%
saving, test loss=2.2035, test acc=34.68%
epoch: 19/10000, 9s, train loss=2.2967, train acc=26.50%, dev loss=2.1869, dev acc=35.27%
saving, test loss=2.1911, test acc=35.10%
epoch: 20/10000, 9s, train loss=2.2873, train acc=26.83%, dev loss=2.1754, dev acc=35.79%
saving, test loss=2.1796, test acc=35.53%
epoch: 21/10000, 9s, train loss=2.2816, train acc=26.84%, dev loss=2.1645, dev acc=35.94%
saving, test loss=2.1688, test acc=35.70%
epoch: 22/10000, 9s, train loss=2.2750, train acc=27.18%, dev loss=2.1541, dev acc=36.24%
saving, test loss=2.1582, test acc=35.94%
epoch: 23/10000, 9s, train loss=2.2666, train acc=27.10%, dev loss=2.1439, dev acc=36.41%
saving, test loss=2.1480, test acc=36.31%
epoch: 24/10000, 9s, train loss=2.2618, train acc=27.69%, dev loss=2.1346, dev acc=36.62%
saving, test loss=2.1387, test acc=36.37%
epoch: 25/10000, 9s, train loss=2.2553, train acc=27.62%, dev loss=2.1259, dev acc=36.86%
saving, test loss=2.1300, test acc=36.31%
epoch: 26/10000, 9s, train loss=2.2530, train acc=27.75%, dev loss=2.1173, dev acc=36.92%
saving, test loss=2.1213, test acc=36.59%
epoch: 27/10000, 9s, train loss=2.2443, train acc=27.64%, dev loss=2.1090, dev acc=37.16%
saving, test loss=2.1130, test acc=36.85%
epoch: 28/10000, 9s, train loss=2.2398, train acc=27.86%, dev loss=2.1016, dev acc=37.14%
epoch: 29/10000, 9s, train loss=2.2381, train acc=27.89%, dev loss=2.0943, dev acc=37.28%
saving, test loss=2.0982, test acc=37.23%
epoch: 30/10000, 9s, train loss=2.2324, train acc=28.11%, dev loss=2.0875, dev acc=37.35%
saving, test loss=2.0913, test acc=37.36%
epoch: 31/10000, 9s, train loss=2.2303, train acc=28.03%, dev loss=2.0808, dev acc=37.57%
saving, test loss=2.0846, test acc=37.70%
epoch: 32/10000, 9s, train loss=2.2252, train acc=28.06%, dev loss=2.0745, dev acc=37.72%
saving, test loss=2.0784, test acc=37.79%
epoch: 33/10000, 9s, train loss=2.2215, train acc=28.28%, dev loss=2.0686, dev acc=37.91%
saving, test loss=2.0725, test acc=37.80%
epoch: 34/10000, 9s, train loss=2.2206, train acc=28.23%, dev loss=2.0629, dev acc=38.02%
saving, test loss=2.0667, test acc=38.12%
epoch: 35/10000, 9s, train loss=2.2195, train acc=28.25%, dev loss=2.0574, dev acc=38.16%
saving, test loss=2.0613, test acc=38.12%
epoch: 36/10000, 9s, train loss=2.2116, train acc=28.50%, dev loss=2.0523, dev acc=38.21%
saving, test loss=2.0562, test acc=38.32%
epoch: 37/10000, 9s, train loss=2.2100, train acc=28.47%, dev loss=2.0472, dev acc=38.23%
saving, test loss=2.0510, test acc=38.35%
epoch: 38/10000, 9s, train loss=2.2059, train acc=28.51%, dev loss=2.0422, dev acc=38.46%
saving, test loss=2.0459, test acc=38.34%
epoch: 39/10000, 9s, train loss=2.2062, train acc=28.40%, dev loss=2.0379, dev acc=38.36%
epoch: 40/10000, 9s, train loss=2.2046, train acc=28.45%, dev loss=2.0336, dev acc=38.44%
epoch: 41/10000, 9s, train loss=2.2020, train acc=28.79%, dev loss=2.0296, dev acc=38.44%
epoch: 42/10000, 9s, train loss=2.1957, train acc=28.75%, dev loss=2.0253, dev acc=38.51%
saving, test loss=2.0290, test acc=38.65%
epoch: 43/10000, 9s, train loss=2.1987, train acc=28.25%, dev loss=2.0213, dev acc=38.55%
saving, test loss=2.0250, test acc=38.82%
epoch: 44/10000, 9s, train loss=2.1936, train acc=28.99%, dev loss=2.0178, dev acc=38.68%
saving, test loss=2.0215, test acc=38.84%
epoch: 45/10000, 9s, train loss=2.1935, train acc=29.00%, dev loss=2.0143, dev acc=38.77%
saving, test loss=2.0177, test acc=38.92%
epoch: 46/10000, 9s, train loss=2.1886, train acc=28.91%, dev loss=2.0109, dev acc=38.70%
epoch: 47/10000, 9s, train loss=2.1919, train acc=28.94%, dev loss=2.0075, dev acc=38.83%
saving, test loss=2.0109, test acc=39.08%
epoch: 48/10000, 9s, train loss=2.1890, train acc=28.99%, dev loss=2.0044, dev acc=38.74%
epoch: 49/10000, 9s, train loss=2.1918, train acc=28.93%, dev loss=2.0015, dev acc=38.96%
saving, test loss=2.0048, test acc=39.20%
epoch: 50/10000, 9s, train loss=2.1869, train acc=29.00%, dev loss=1.9986, dev acc=38.98%
saving, test loss=2.0021, test acc=39.07%
epoch: 51/10000, 9s, train loss=2.1813, train acc=28.95%, dev loss=1.9960, dev acc=39.02%
saving, test loss=1.9992, test acc=39.19%
epoch: 52/10000, 9s, train loss=2.1807, train acc=29.20%, dev loss=1.9932, dev acc=39.09%
saving, test loss=1.9966, test acc=39.18%
epoch: 53/10000, 9s, train loss=2.1805, train acc=29.38%, dev loss=1.9903, dev acc=39.09%
epoch: 54/10000, 9s, train loss=2.1770, train acc=29.16%, dev loss=1.9876, dev acc=39.07%
epoch: 55/10000, 9s, train loss=2.1787, train acc=29.39%, dev loss=1.9854, dev acc=39.15%
saving, test loss=1.9887, test acc=39.43%
epoch: 56/10000, 9s, train loss=2.1752, train acc=29.25%, dev loss=1.9828, dev acc=39.11%
epoch: 57/10000, 9s, train loss=2.1759, train acc=29.44%, dev loss=1.9806, dev acc=39.07%
epoch: 58/10000, 9s, train loss=2.1796, train acc=29.05%, dev loss=1.9784, dev acc=39.13%
epoch: 59/10000, 9s, train loss=2.1734, train acc=29.26%, dev loss=1.9763, dev acc=39.21%
saving, test loss=1.9797, test acc=39.56%
epoch: 60/10000, 9s, train loss=2.1765, train acc=29.10%, dev loss=1.9742, dev acc=39.28%
saving, test loss=1.9776, test acc=39.55%
epoch: 61/10000, 9s, train loss=2.1683, train acc=29.37%, dev loss=1.9722, dev acc=39.30%
saving, test loss=1.9756, test acc=39.59%
epoch: 62/10000, 9s, train loss=2.1681, train acc=29.71%, dev loss=1.9700, dev acc=39.37%
saving, test loss=1.9734, test acc=39.62%
epoch: 63/10000, 9s, train loss=2.1698, train acc=29.22%, dev loss=1.9683, dev acc=39.45%
saving, test loss=1.9717, test acc=39.74%
epoch: 64/10000, 9s, train loss=2.1675, train acc=29.14%, dev loss=1.9664, dev acc=39.30%
epoch: 65/10000, 9s, train loss=2.1647, train acc=29.73%, dev loss=1.9644, dev acc=39.49%
saving, test loss=1.9679, test acc=39.64%
epoch: 66/10000, 9s, train loss=2.1699, train acc=29.22%, dev loss=1.9626, dev acc=39.54%
saving, test loss=1.9660, test acc=39.87%
epoch: 67/10000, 9s, train loss=2.1664, train acc=29.54%, dev loss=1.9611, dev acc=39.49%
epoch: 68/10000, 9s, train loss=2.1647, train acc=29.60%, dev loss=1.9593, dev acc=39.56%
saving, test loss=1.9628, test acc=39.88%
epoch: 69/10000, 9s, train loss=2.1649, train acc=29.67%, dev loss=1.9579, dev acc=39.60%
saving, test loss=1.9614, test acc=39.92%
epoch: 70/10000, 9s, train loss=2.1639, train acc=29.34%, dev loss=1.9568, dev acc=39.51%
epoch: 71/10000, 9s, train loss=2.1656, train acc=29.56%, dev loss=1.9552, dev acc=39.67%
saving, test loss=1.9585, test acc=39.94%
epoch: 72/10000, 9s, train loss=2.1690, train acc=29.16%, dev loss=1.9537, dev acc=39.81%
saving, test loss=1.9573, test acc=39.94%
epoch: 73/10000, 9s, train loss=2.1650, train acc=29.55%, dev loss=1.9528, dev acc=39.62%
epoch: 74/10000, 9s, train loss=2.1592, train acc=29.72%, dev loss=1.9507, dev acc=39.81%
epoch: 75/10000, 9s, train loss=2.1635, train acc=29.59%, dev loss=1.9499, dev acc=39.96%
saving, test loss=1.9533, test acc=39.93%
epoch: 76/10000, 9s, train loss=2.1636, train acc=29.44%, dev loss=1.9491, dev acc=39.92%
epoch: 77/10000, 9s, train loss=2.1569, train acc=29.79%, dev loss=1.9474, dev acc=39.97%
saving, test loss=1.9508, test acc=39.96%
epoch: 78/10000, 9s, train loss=2.1604, train acc=29.76%, dev loss=1.9463, dev acc=40.05%
saving, test loss=1.9498, test acc=40.01%
epoch: 79/10000, 9s, train loss=2.1603, train acc=29.53%, dev loss=1.9450, dev acc=40.01%
epoch: 80/10000, 9s, train loss=2.1594, train acc=29.45%, dev loss=1.9439, dev acc=40.01%
epoch: 81/10000, 9s, train loss=2.1568, train acc=29.81%, dev loss=1.9430, dev acc=40.01%
epoch: 82/10000, 9s, train loss=2.1603, train acc=29.38%, dev loss=1.9419, dev acc=39.99%
epoch: 83/10000, 9s, train loss=2.1634, train acc=29.51%, dev loss=1.9410, dev acc=39.99%
time used=1037.5s
