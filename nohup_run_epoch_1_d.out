nohup: ignoring input
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/junnyu_roformer_chinese_sim_char_ft_small', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_culture': 0, 'news_world': 1, 'news_story': 2, 'news_travel': 3, 'news_entertainment': 4, 'news_edu': 5, 'news_stock': 6, 'news_agriculture': 7, 'news_military': 8, 'news_finance': 9, 'news_house': 10, 'news_game': 11, 'news_car': 12, 'news_tech': 13, 'news_sports': 14}
index_labels_dict={0: 'news_culture', 1: 'news_world', 2: 'news_story', 3: 'news_travel', 4: 'news_entertainment', 5: 'news_edu', 6: 'news_stock', 7: 'news_agriculture', 8: 'news_military', 9: 'news_finance', 10: 'news_house', 11: 'news_game', 12: 'news_car', 13: 'news_tech', 14: 'news_sports'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	8
49	8
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	231
34	320
33	613
32	3433
31	2672
30	2381
29	2215
28	2259
27	2152
26	2235
25	2195
24	2307
23	2331
22	2203
21	2217
20	2011
19	2273
18	1888
17	1978
16	1765
15	1601
14	1484
13	1197
12	963
11	791
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/junnyu_roformer_chinese_sim_char_ft_small were not used when initializing RoFormerModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'roformer.pooler.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'roformer.pooler.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing RoFormerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RoFormerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RoFormerModel were not initialized from the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/junnyu_roformer_chinese_sim_char_ft_small and are newly initialized: ['roformer.encoder.embed_positions.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
CustomModel(
  (pretrained_model): RoFormerModel(
    (embeddings): RoFormerEmbeddings(
      (word_embeddings): Embedding(12000, 384, padding_idx=0)
      (token_type_embeddings): Embedding(2, 384)
      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RoFormerEncoder(
      (embed_positions): RoFormerSinusoidalPositionalEmbedding(512, 64)
      (layer): ModuleList(
        (0): RoFormerLayer(
          (attention): RoFormerAttention(
            (self): RoFormerSelfAttention(
              (query): Linear(in_features=384, out_features=384, bias=True)
              (key): Linear(in_features=384, out_features=384, bias=True)
              (value): Linear(in_features=384, out_features=384, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RoFormerSelfOutput(
              (dense): Linear(in_features=384, out_features=384, bias=True)
              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RoFormerIntermediate(
            (dense): Linear(in_features=384, out_features=1536, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RoFormerOutput(
            (dense): Linear(in_features=1536, out_features=384, bias=True)
            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RoFormerLayer(
          (attention): RoFormerAttention(
            (self): RoFormerSelfAttention(
              (query): Linear(in_features=384, out_features=384, bias=True)
              (key): Linear(in_features=384, out_features=384, bias=True)
              (value): Linear(in_features=384, out_features=384, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RoFormerSelfOutput(
              (dense): Linear(in_features=384, out_features=384, bias=True)
              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RoFormerIntermediate(
            (dense): Linear(in_features=384, out_features=1536, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RoFormerOutput(
            (dense): Linear(in_features=1536, out_features=384, bias=True)
            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RoFormerLayer(
          (attention): RoFormerAttention(
            (self): RoFormerSelfAttention(
              (query): Linear(in_features=384, out_features=384, bias=True)
              (key): Linear(in_features=384, out_features=384, bias=True)
              (value): Linear(in_features=384, out_features=384, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RoFormerSelfOutput(
              (dense): Linear(in_features=384, out_features=384, bias=True)
              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RoFormerIntermediate(
            (dense): Linear(in_features=384, out_features=1536, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RoFormerOutput(
            (dense): Linear(in_features=1536, out_features=384, bias=True)
            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RoFormerLayer(
          (attention): RoFormerAttention(
            (self): RoFormerSelfAttention(
              (query): Linear(in_features=384, out_features=384, bias=True)
              (key): Linear(in_features=384, out_features=384, bias=True)
              (value): Linear(in_features=384, out_features=384, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RoFormerSelfOutput(
              (dense): Linear(in_features=384, out_features=384, bias=True)
              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RoFormerIntermediate(
            (dense): Linear(in_features=384, out_features=1536, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RoFormerOutput(
            (dense): Linear(in_features=1536, out_features=384, bias=True)
            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RoFormerLayer(
          (attention): RoFormerAttention(
            (self): RoFormerSelfAttention(
              (query): Linear(in_features=384, out_features=384, bias=True)
              (key): Linear(in_features=384, out_features=384, bias=True)
              (value): Linear(in_features=384, out_features=384, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RoFormerSelfOutput(
              (dense): Linear(in_features=384, out_features=384, bias=True)
              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RoFormerIntermediate(
            (dense): Linear(in_features=384, out_features=1536, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RoFormerOutput(
            (dense): Linear(in_features=1536, out_features=384, bias=True)
            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RoFormerLayer(
          (attention): RoFormerAttention(
            (self): RoFormerSelfAttention(
              (query): Linear(in_features=384, out_features=384, bias=True)
              (key): Linear(in_features=384, out_features=384, bias=True)
              (value): Linear(in_features=384, out_features=384, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RoFormerSelfOutput(
              (dense): Linear(in_features=384, out_features=384, bias=True)
              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RoFormerIntermediate(
            (dense): Linear(in_features=384, out_features=1536, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RoFormerOutput(
            (dense): Linear(in_features=1536, out_features=384, bias=True)
            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=384, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.embed_positions.weight False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 15s, train loss=2.6179, train acc=12.04%, dev loss=2.4535, dev acc=20.95%
saving, test loss=2.4566, test acc=20.64%
epoch: 2/10000, 14s, train loss=2.4416, train acc=21.20%, dev loss=2.3016, dev acc=31.95%
saving, test loss=2.3035, test acc=31.96%
epoch: 3/10000, 15s, train loss=2.3213, train acc=27.70%, dev loss=2.1837, dev acc=37.86%
saving, test loss=2.1855, test acc=37.70%
epoch: 4/10000, 15s, train loss=2.2330, train acc=31.70%, dev loss=2.0915, dev acc=40.82%
saving, test loss=2.0933, test acc=40.66%
epoch: 5/10000, 15s, train loss=2.1667, train acc=33.59%, dev loss=2.0188, dev acc=42.63%
saving, test loss=2.0202, test acc=42.79%
epoch: 6/10000, 15s, train loss=2.1170, train acc=34.86%, dev loss=1.9609, dev acc=43.68%
saving, test loss=1.9620, test acc=43.99%
epoch: 7/10000, 15s, train loss=2.0728, train acc=36.15%, dev loss=1.9146, dev acc=44.40%
saving, test loss=1.9151, test acc=44.66%
epoch: 8/10000, 15s, train loss=2.0399, train acc=37.03%, dev loss=1.8772, dev acc=44.92%
saving, test loss=1.8768, test acc=45.17%
epoch: 9/10000, 15s, train loss=2.0164, train acc=37.42%, dev loss=1.8457, dev acc=45.37%
saving, test loss=1.8447, test acc=46.08%
epoch: 10/10000, 15s, train loss=1.9988, train acc=37.52%, dev loss=1.8200, dev acc=45.90%
saving, test loss=1.8185, test acc=46.30%
epoch: 11/10000, 15s, train loss=1.9803, train acc=38.13%, dev loss=1.7983, dev acc=45.91%
saving, test loss=1.7967, test acc=46.73%
epoch: 12/10000, 14s, train loss=1.9685, train acc=38.32%, dev loss=1.7801, dev acc=46.50%
saving, test loss=1.7781, test acc=46.98%
epoch: 13/10000, 15s, train loss=1.9484, train acc=38.82%, dev loss=1.7649, dev acc=46.81%
saving, test loss=1.7623, test acc=47.26%
epoch: 14/10000, 15s, train loss=1.9441, train acc=39.11%, dev loss=1.7512, dev acc=46.83%
saving, test loss=1.7486, test acc=47.50%
epoch: 15/10000, 14s, train loss=1.9402, train acc=39.05%, dev loss=1.7405, dev acc=47.06%
saving, test loss=1.7372, test acc=47.69%
epoch: 16/10000, 15s, train loss=1.9304, train acc=39.16%, dev loss=1.7312, dev acc=47.21%
saving, test loss=1.7274, test acc=47.86%
epoch: 17/10000, 15s, train loss=1.9270, train acc=39.07%, dev loss=1.7216, dev acc=47.11%
epoch: 18/10000, 15s, train loss=1.9264, train acc=39.24%, dev loss=1.7141, dev acc=47.23%
saving, test loss=1.7102, test acc=47.94%
epoch: 19/10000, 15s, train loss=1.9165, train acc=39.48%, dev loss=1.7070, dev acc=47.40%
saving, test loss=1.7033, test acc=48.00%
epoch: 20/10000, 15s, train loss=1.9178, train acc=39.37%, dev loss=1.7009, dev acc=47.43%
saving, test loss=1.6970, test acc=48.16%
epoch: 21/10000, 15s, train loss=1.9134, train acc=39.37%, dev loss=1.6953, dev acc=47.43%
epoch: 22/10000, 14s, train loss=1.9073, train acc=39.52%, dev loss=1.6896, dev acc=47.56%
saving, test loss=1.6863, test acc=48.24%
epoch: 23/10000, 15s, train loss=1.9076, train acc=39.81%, dev loss=1.6854, dev acc=47.58%
saving, test loss=1.6817, test acc=48.33%
epoch: 24/10000, 15s, train loss=1.9104, train acc=39.54%, dev loss=1.6824, dev acc=47.84%
saving, test loss=1.6786, test acc=48.44%
epoch: 25/10000, 15s, train loss=1.9010, train acc=39.70%, dev loss=1.6793, dev acc=47.66%
epoch: 26/10000, 15s, train loss=1.8962, train acc=39.98%, dev loss=1.6750, dev acc=47.73%
epoch: 27/10000, 15s, train loss=1.8994, train acc=39.58%, dev loss=1.6723, dev acc=47.75%
epoch: 28/10000, 15s, train loss=1.8932, train acc=39.90%, dev loss=1.6691, dev acc=47.84%
epoch: 29/10000, 15s, train loss=1.8950, train acc=39.71%, dev loss=1.6668, dev acc=47.92%
saving, test loss=1.6635, test acc=48.65%
epoch: 30/10000, 14s, train loss=1.8934, train acc=39.68%, dev loss=1.6638, dev acc=47.88%
epoch: 31/10000, 15s, train loss=1.9013, train acc=39.68%, dev loss=1.6628, dev acc=47.90%
epoch: 32/10000, 15s, train loss=1.8876, train acc=40.14%, dev loss=1.6602, dev acc=47.94%
saving, test loss=1.6564, test acc=48.76%
epoch: 33/10000, 15s, train loss=1.8975, train acc=39.73%, dev loss=1.6585, dev acc=47.84%
epoch: 34/10000, 15s, train loss=1.8931, train acc=39.64%, dev loss=1.6583, dev acc=47.71%
epoch: 35/10000, 15s, train loss=1.8907, train acc=39.80%, dev loss=1.6566, dev acc=47.84%
epoch: 36/10000, 15s, train loss=1.8841, train acc=40.06%, dev loss=1.6555, dev acc=47.98%
saving, test loss=1.6514, test acc=48.87%
epoch: 37/10000, 15s, train loss=1.8930, train acc=39.92%, dev loss=1.6543, dev acc=47.83%
epoch: 38/10000, 15s, train loss=1.8839, train acc=40.04%, dev loss=1.6530, dev acc=47.84%
epoch: 39/10000, 15s, train loss=1.8880, train acc=39.85%, dev loss=1.6513, dev acc=47.73%
epoch: 40/10000, 15s, train loss=1.8854, train acc=40.25%, dev loss=1.6514, dev acc=47.86%
epoch: 41/10000, 15s, train loss=1.8887, train acc=39.80%, dev loss=1.6500, dev acc=47.81%
time used=786.1s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/luhua_chinese_pretrain_mrc_macbert_large', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_military': 0, 'news_sports': 1, 'news_stock': 2, 'news_finance': 3, 'news_story': 4, 'news_travel': 5, 'news_entertainment': 6, 'news_edu': 7, 'news_game': 8, 'news_tech': 9, 'news_agriculture': 10, 'news_culture': 11, 'news_world': 12, 'news_car': 13, 'news_house': 14}
index_labels_dict={0: 'news_military', 1: 'news_sports', 2: 'news_stock', 3: 'news_finance', 4: 'news_story', 5: 'news_travel', 6: 'news_entertainment', 7: 'news_edu', 8: 'news_game', 9: 'news_tech', 10: 'news_agriculture', 11: 'news_culture', 12: 'news_world', 13: 'news_car', 14: 'news_house'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	65
45	11
44	27
43	41
42	98
41	92
40	133
39	119
38	128
37	169
36	171
35	216
34	294
33	555
32	3351
31	2647
30	2397
29	2194
28	2237
27	2187
26	2247
25	2193
24	2281
23	2352
22	2237
21	2242
20	2031
19	2278
18	1894
17	1980
16	1801
15	1616
14	1491
13	1220
12	970
11	808
10	548
9	340
8	152
7	130
6	20
5	5
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/luhua_chinese_pretrain_mrc_macbert_large were not used when initializing BertModel: ['qa_outputs.weight', 'qa_outputs.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (12): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (13): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (14): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (15): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (16): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (17): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (18): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (19): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (20): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (21): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (22): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (23): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=1024, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.attention.self.query.weight False
pretrained_model.encoder.layer.12.attention.self.query.bias False
pretrained_model.encoder.layer.12.attention.self.key.weight False
pretrained_model.encoder.layer.12.attention.self.key.bias False
pretrained_model.encoder.layer.12.attention.self.value.weight False
pretrained_model.encoder.layer.12.attention.self.value.bias False
pretrained_model.encoder.layer.12.attention.output.dense.weight False
pretrained_model.encoder.layer.12.attention.output.dense.bias False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.intermediate.dense.weight False
pretrained_model.encoder.layer.12.intermediate.dense.bias False
pretrained_model.encoder.layer.12.output.dense.weight False
pretrained_model.encoder.layer.12.output.dense.bias False
pretrained_model.encoder.layer.12.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.attention.self.query.weight False
pretrained_model.encoder.layer.13.attention.self.query.bias False
pretrained_model.encoder.layer.13.attention.self.key.weight False
pretrained_model.encoder.layer.13.attention.self.key.bias False
pretrained_model.encoder.layer.13.attention.self.value.weight False
pretrained_model.encoder.layer.13.attention.self.value.bias False
pretrained_model.encoder.layer.13.attention.output.dense.weight False
pretrained_model.encoder.layer.13.attention.output.dense.bias False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.intermediate.dense.weight False
pretrained_model.encoder.layer.13.intermediate.dense.bias False
pretrained_model.encoder.layer.13.output.dense.weight False
pretrained_model.encoder.layer.13.output.dense.bias False
pretrained_model.encoder.layer.13.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.attention.self.query.weight False
pretrained_model.encoder.layer.14.attention.self.query.bias False
pretrained_model.encoder.layer.14.attention.self.key.weight False
pretrained_model.encoder.layer.14.attention.self.key.bias False
pretrained_model.encoder.layer.14.attention.self.value.weight False
pretrained_model.encoder.layer.14.attention.self.value.bias False
pretrained_model.encoder.layer.14.attention.output.dense.weight False
pretrained_model.encoder.layer.14.attention.output.dense.bias False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.intermediate.dense.weight False
pretrained_model.encoder.layer.14.intermediate.dense.bias False
pretrained_model.encoder.layer.14.output.dense.weight False
pretrained_model.encoder.layer.14.output.dense.bias False
pretrained_model.encoder.layer.14.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.attention.self.query.weight False
pretrained_model.encoder.layer.15.attention.self.query.bias False
pretrained_model.encoder.layer.15.attention.self.key.weight False
pretrained_model.encoder.layer.15.attention.self.key.bias False
pretrained_model.encoder.layer.15.attention.self.value.weight False
pretrained_model.encoder.layer.15.attention.self.value.bias False
pretrained_model.encoder.layer.15.attention.output.dense.weight False
pretrained_model.encoder.layer.15.attention.output.dense.bias False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.intermediate.dense.weight False
pretrained_model.encoder.layer.15.intermediate.dense.bias False
pretrained_model.encoder.layer.15.output.dense.weight False
pretrained_model.encoder.layer.15.output.dense.bias False
pretrained_model.encoder.layer.15.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.attention.self.query.weight False
pretrained_model.encoder.layer.16.attention.self.query.bias False
pretrained_model.encoder.layer.16.attention.self.key.weight False
pretrained_model.encoder.layer.16.attention.self.key.bias False
pretrained_model.encoder.layer.16.attention.self.value.weight False
pretrained_model.encoder.layer.16.attention.self.value.bias False
pretrained_model.encoder.layer.16.attention.output.dense.weight False
pretrained_model.encoder.layer.16.attention.output.dense.bias False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.intermediate.dense.weight False
pretrained_model.encoder.layer.16.intermediate.dense.bias False
pretrained_model.encoder.layer.16.output.dense.weight False
pretrained_model.encoder.layer.16.output.dense.bias False
pretrained_model.encoder.layer.16.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.attention.self.query.weight False
pretrained_model.encoder.layer.17.attention.self.query.bias False
pretrained_model.encoder.layer.17.attention.self.key.weight False
pretrained_model.encoder.layer.17.attention.self.key.bias False
pretrained_model.encoder.layer.17.attention.self.value.weight False
pretrained_model.encoder.layer.17.attention.self.value.bias False
pretrained_model.encoder.layer.17.attention.output.dense.weight False
pretrained_model.encoder.layer.17.attention.output.dense.bias False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.intermediate.dense.weight False
pretrained_model.encoder.layer.17.intermediate.dense.bias False
pretrained_model.encoder.layer.17.output.dense.weight False
pretrained_model.encoder.layer.17.output.dense.bias False
pretrained_model.encoder.layer.17.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.attention.self.query.weight False
pretrained_model.encoder.layer.18.attention.self.query.bias False
pretrained_model.encoder.layer.18.attention.self.key.weight False
pretrained_model.encoder.layer.18.attention.self.key.bias False
pretrained_model.encoder.layer.18.attention.self.value.weight False
pretrained_model.encoder.layer.18.attention.self.value.bias False
pretrained_model.encoder.layer.18.attention.output.dense.weight False
pretrained_model.encoder.layer.18.attention.output.dense.bias False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.intermediate.dense.weight False
pretrained_model.encoder.layer.18.intermediate.dense.bias False
pretrained_model.encoder.layer.18.output.dense.weight False
pretrained_model.encoder.layer.18.output.dense.bias False
pretrained_model.encoder.layer.18.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.attention.self.query.weight False
pretrained_model.encoder.layer.19.attention.self.query.bias False
pretrained_model.encoder.layer.19.attention.self.key.weight False
pretrained_model.encoder.layer.19.attention.self.key.bias False
pretrained_model.encoder.layer.19.attention.self.value.weight False
pretrained_model.encoder.layer.19.attention.self.value.bias False
pretrained_model.encoder.layer.19.attention.output.dense.weight False
pretrained_model.encoder.layer.19.attention.output.dense.bias False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.intermediate.dense.weight False
pretrained_model.encoder.layer.19.intermediate.dense.bias False
pretrained_model.encoder.layer.19.output.dense.weight False
pretrained_model.encoder.layer.19.output.dense.bias False
pretrained_model.encoder.layer.19.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.attention.self.query.weight False
pretrained_model.encoder.layer.20.attention.self.query.bias False
pretrained_model.encoder.layer.20.attention.self.key.weight False
pretrained_model.encoder.layer.20.attention.self.key.bias False
pretrained_model.encoder.layer.20.attention.self.value.weight False
pretrained_model.encoder.layer.20.attention.self.value.bias False
pretrained_model.encoder.layer.20.attention.output.dense.weight False
pretrained_model.encoder.layer.20.attention.output.dense.bias False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.intermediate.dense.weight False
pretrained_model.encoder.layer.20.intermediate.dense.bias False
pretrained_model.encoder.layer.20.output.dense.weight False
pretrained_model.encoder.layer.20.output.dense.bias False
pretrained_model.encoder.layer.20.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.attention.self.query.weight False
pretrained_model.encoder.layer.21.attention.self.query.bias False
pretrained_model.encoder.layer.21.attention.self.key.weight False
pretrained_model.encoder.layer.21.attention.self.key.bias False
pretrained_model.encoder.layer.21.attention.self.value.weight False
pretrained_model.encoder.layer.21.attention.self.value.bias False
pretrained_model.encoder.layer.21.attention.output.dense.weight False
pretrained_model.encoder.layer.21.attention.output.dense.bias False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.intermediate.dense.weight False
pretrained_model.encoder.layer.21.intermediate.dense.bias False
pretrained_model.encoder.layer.21.output.dense.weight False
pretrained_model.encoder.layer.21.output.dense.bias False
pretrained_model.encoder.layer.21.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.attention.self.query.weight False
pretrained_model.encoder.layer.22.attention.self.query.bias False
pretrained_model.encoder.layer.22.attention.self.key.weight False
pretrained_model.encoder.layer.22.attention.self.key.bias False
pretrained_model.encoder.layer.22.attention.self.value.weight False
pretrained_model.encoder.layer.22.attention.self.value.bias False
pretrained_model.encoder.layer.22.attention.output.dense.weight False
pretrained_model.encoder.layer.22.attention.output.dense.bias False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.intermediate.dense.weight False
pretrained_model.encoder.layer.22.intermediate.dense.bias False
pretrained_model.encoder.layer.22.output.dense.weight False
pretrained_model.encoder.layer.22.output.dense.bias False
pretrained_model.encoder.layer.22.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.attention.self.query.weight False
pretrained_model.encoder.layer.23.attention.self.query.bias False
pretrained_model.encoder.layer.23.attention.self.key.weight False
pretrained_model.encoder.layer.23.attention.self.key.bias False
pretrained_model.encoder.layer.23.attention.self.value.weight False
pretrained_model.encoder.layer.23.attention.self.value.bias False
pretrained_model.encoder.layer.23.attention.output.dense.weight False
pretrained_model.encoder.layer.23.attention.output.dense.bias False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.intermediate.dense.weight False
pretrained_model.encoder.layer.23.intermediate.dense.bias False
pretrained_model.encoder.layer.23.output.dense.weight False
pretrained_model.encoder.layer.23.output.dense.bias False
pretrained_model.encoder.layer.23.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 222s, train loss=2.5393, train acc=16.99%, dev loss=2.2148, dev acc=37.13%
saving, test loss=2.2212, test acc=36.79%
epoch: 2/10000, 223s, train loss=2.1641, train acc=32.81%, dev loss=1.9255, dev acc=44.53%
saving, test loss=1.9327, test acc=44.96%
epoch: 3/10000, 224s, train loss=1.9613, train acc=39.87%, dev loss=1.7654, dev acc=48.01%
saving, test loss=1.7728, test acc=48.41%
epoch: 4/10000, 224s, train loss=1.8405, train acc=43.41%, dev loss=1.6690, dev acc=49.87%
saving, test loss=1.6755, test acc=50.16%
epoch: 5/10000, 224s, train loss=1.7715, train acc=45.23%, dev loss=1.6068, dev acc=50.79%
saving, test loss=1.6129, test acc=51.22%
epoch: 6/10000, 224s, train loss=1.7205, train acc=46.68%, dev loss=1.5635, dev acc=51.11%
saving, test loss=1.5700, test acc=51.56%
epoch: 7/10000, 224s, train loss=1.6931, train acc=47.01%, dev loss=1.5348, dev acc=51.44%
saving, test loss=1.5404, test acc=51.92%
epoch: 8/10000, 224s, train loss=1.6657, train acc=47.78%, dev loss=1.5116, dev acc=51.67%
saving, test loss=1.5174, test acc=52.34%
epoch: 9/10000, 224s, train loss=1.6458, train acc=48.13%, dev loss=1.4929, dev acc=51.97%
saving, test loss=1.4989, test acc=52.58%
epoch: 10/10000, 224s, train loss=1.6344, train acc=48.46%, dev loss=1.4784, dev acc=52.08%
saving, test loss=1.4842, test acc=52.81%
epoch: 11/10000, 224s, train loss=1.6225, train acc=48.42%, dev loss=1.4671, dev acc=52.31%
saving, test loss=1.4729, test acc=52.77%
epoch: 12/10000, 224s, train loss=1.6203, train acc=48.41%, dev loss=1.4582, dev acc=52.44%
saving, test loss=1.4637, test acc=52.91%
epoch: 13/10000, 225s, train loss=1.6091, train acc=48.75%, dev loss=1.4499, dev acc=52.59%
saving, test loss=1.4554, test acc=53.04%
epoch: 14/10000, 224s, train loss=1.5963, train acc=49.23%, dev loss=1.4428, dev acc=52.72%
saving, test loss=1.4497, test acc=52.99%
epoch: 15/10000, 225s, train loss=1.5966, train acc=49.00%, dev loss=1.4373, dev acc=52.70%
epoch: 16/10000, 225s, train loss=1.5945, train acc=49.14%, dev loss=1.4343, dev acc=52.85%
saving, test loss=1.4404, test acc=53.21%
epoch: 17/10000, 225s, train loss=1.5893, train acc=49.03%, dev loss=1.4292, dev acc=52.98%
saving, test loss=1.4355, test acc=53.21%
epoch: 18/10000, 225s, train loss=1.5871, train acc=49.31%, dev loss=1.4259, dev acc=52.96%
epoch: 19/10000, 225s, train loss=1.5817, train acc=49.07%, dev loss=1.4233, dev acc=52.74%
epoch: 20/10000, 225s, train loss=1.5810, train acc=49.51%, dev loss=1.4207, dev acc=53.02%
saving, test loss=1.4264, test acc=53.32%
epoch: 21/10000, 225s, train loss=1.5755, train acc=49.38%, dev loss=1.4162, dev acc=53.15%
saving, test loss=1.4229, test acc=53.51%
epoch: 22/10000, 225s, train loss=1.5775, train acc=49.37%, dev loss=1.4144, dev acc=52.98%
epoch: 23/10000, 226s, train loss=1.5733, train acc=49.36%, dev loss=1.4124, dev acc=52.96%
epoch: 24/10000, 226s, train loss=1.5696, train acc=49.51%, dev loss=1.4109, dev acc=53.11%
epoch: 25/10000, 226s, train loss=1.5732, train acc=49.40%, dev loss=1.4079, dev acc=53.07%
epoch: 26/10000, 225s, train loss=1.5705, train acc=49.59%, dev loss=1.4063, dev acc=53.19%
saving, test loss=1.4129, test acc=53.61%
epoch: 27/10000, 225s, train loss=1.5723, train acc=49.41%, dev loss=1.4055, dev acc=53.11%
epoch: 28/10000, 226s, train loss=1.5643, train acc=49.56%, dev loss=1.4049, dev acc=53.28%
saving, test loss=1.4114, test acc=53.56%
epoch: 29/10000, 226s, train loss=1.5696, train acc=49.38%, dev loss=1.4044, dev acc=53.02%
epoch: 30/10000, 225s, train loss=1.5660, train acc=49.50%, dev loss=1.4024, dev acc=53.24%
epoch: 31/10000, 225s, train loss=1.5627, train acc=49.71%, dev loss=1.4018, dev acc=53.11%
epoch: 32/10000, 226s, train loss=1.5681, train acc=49.55%, dev loss=1.4010, dev acc=53.41%
saving, test loss=1.4077, test acc=53.41%
epoch: 33/10000, 225s, train loss=1.5680, train acc=49.59%, dev loss=1.4004, dev acc=53.41%
epoch: 34/10000, 225s, train loss=1.5641, train acc=49.58%, dev loss=1.3997, dev acc=53.45%
saving, test loss=1.4047, test acc=53.52%
epoch: 35/10000, 225s, train loss=1.5585, train acc=49.59%, dev loss=1.4010, dev acc=53.09%
epoch: 36/10000, 226s, train loss=1.5646, train acc=49.70%, dev loss=1.3990, dev acc=53.07%
epoch: 37/10000, 226s, train loss=1.5604, train acc=49.90%, dev loss=1.3967, dev acc=53.41%
epoch: 38/10000, 226s, train loss=1.5591, train acc=49.62%, dev loss=1.3982, dev acc=53.15%
epoch: 39/10000, 226s, train loss=1.5595, train acc=49.81%, dev loss=1.3962, dev acc=53.22%
time used=10752.8s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/luhua_chinese_pretrain_mrc_roberta_wwm_ext_large', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_edu': 0, 'news_finance': 1, 'news_agriculture': 2, 'news_culture': 3, 'news_military': 4, 'news_world': 5, 'news_travel': 6, 'news_tech': 7, 'news_entertainment': 8, 'news_sports': 9, 'news_car': 10, 'news_house': 11, 'news_stock': 12, 'news_game': 13, 'news_story': 14}
index_labels_dict={0: 'news_edu', 1: 'news_finance', 2: 'news_agriculture', 3: 'news_culture', 4: 'news_military', 5: 'news_world', 6: 'news_travel', 7: 'news_tech', 8: 'news_entertainment', 9: 'news_sports', 10: 'news_car', 11: 'news_house', 12: 'news_stock', 13: 'news_game', 14: 'news_story'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/luhua_chinese_pretrain_mrc_roberta_wwm_ext_large were not used when initializing BertModel: ['qa_outputs.weight', 'qa_outputs.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (12): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (13): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (14): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (15): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (16): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (17): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (18): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (19): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (20): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (21): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (22): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (23): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=1024, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.attention.self.query.weight False
pretrained_model.encoder.layer.12.attention.self.query.bias False
pretrained_model.encoder.layer.12.attention.self.key.weight False
pretrained_model.encoder.layer.12.attention.self.key.bias False
pretrained_model.encoder.layer.12.attention.self.value.weight False
pretrained_model.encoder.layer.12.attention.self.value.bias False
pretrained_model.encoder.layer.12.attention.output.dense.weight False
pretrained_model.encoder.layer.12.attention.output.dense.bias False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.intermediate.dense.weight False
pretrained_model.encoder.layer.12.intermediate.dense.bias False
pretrained_model.encoder.layer.12.output.dense.weight False
pretrained_model.encoder.layer.12.output.dense.bias False
pretrained_model.encoder.layer.12.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.attention.self.query.weight False
pretrained_model.encoder.layer.13.attention.self.query.bias False
pretrained_model.encoder.layer.13.attention.self.key.weight False
pretrained_model.encoder.layer.13.attention.self.key.bias False
pretrained_model.encoder.layer.13.attention.self.value.weight False
pretrained_model.encoder.layer.13.attention.self.value.bias False
pretrained_model.encoder.layer.13.attention.output.dense.weight False
pretrained_model.encoder.layer.13.attention.output.dense.bias False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.intermediate.dense.weight False
pretrained_model.encoder.layer.13.intermediate.dense.bias False
pretrained_model.encoder.layer.13.output.dense.weight False
pretrained_model.encoder.layer.13.output.dense.bias False
pretrained_model.encoder.layer.13.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.attention.self.query.weight False
pretrained_model.encoder.layer.14.attention.self.query.bias False
pretrained_model.encoder.layer.14.attention.self.key.weight False
pretrained_model.encoder.layer.14.attention.self.key.bias False
pretrained_model.encoder.layer.14.attention.self.value.weight False
pretrained_model.encoder.layer.14.attention.self.value.bias False
pretrained_model.encoder.layer.14.attention.output.dense.weight False
pretrained_model.encoder.layer.14.attention.output.dense.bias False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.intermediate.dense.weight False
pretrained_model.encoder.layer.14.intermediate.dense.bias False
pretrained_model.encoder.layer.14.output.dense.weight False
pretrained_model.encoder.layer.14.output.dense.bias False
pretrained_model.encoder.layer.14.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.attention.self.query.weight False
pretrained_model.encoder.layer.15.attention.self.query.bias False
pretrained_model.encoder.layer.15.attention.self.key.weight False
pretrained_model.encoder.layer.15.attention.self.key.bias False
pretrained_model.encoder.layer.15.attention.self.value.weight False
pretrained_model.encoder.layer.15.attention.self.value.bias False
pretrained_model.encoder.layer.15.attention.output.dense.weight False
pretrained_model.encoder.layer.15.attention.output.dense.bias False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.intermediate.dense.weight False
pretrained_model.encoder.layer.15.intermediate.dense.bias False
pretrained_model.encoder.layer.15.output.dense.weight False
pretrained_model.encoder.layer.15.output.dense.bias False
pretrained_model.encoder.layer.15.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.attention.self.query.weight False
pretrained_model.encoder.layer.16.attention.self.query.bias False
pretrained_model.encoder.layer.16.attention.self.key.weight False
pretrained_model.encoder.layer.16.attention.self.key.bias False
pretrained_model.encoder.layer.16.attention.self.value.weight False
pretrained_model.encoder.layer.16.attention.self.value.bias False
pretrained_model.encoder.layer.16.attention.output.dense.weight False
pretrained_model.encoder.layer.16.attention.output.dense.bias False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.intermediate.dense.weight False
pretrained_model.encoder.layer.16.intermediate.dense.bias False
pretrained_model.encoder.layer.16.output.dense.weight False
pretrained_model.encoder.layer.16.output.dense.bias False
pretrained_model.encoder.layer.16.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.attention.self.query.weight False
pretrained_model.encoder.layer.17.attention.self.query.bias False
pretrained_model.encoder.layer.17.attention.self.key.weight False
pretrained_model.encoder.layer.17.attention.self.key.bias False
pretrained_model.encoder.layer.17.attention.self.value.weight False
pretrained_model.encoder.layer.17.attention.self.value.bias False
pretrained_model.encoder.layer.17.attention.output.dense.weight False
pretrained_model.encoder.layer.17.attention.output.dense.bias False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.intermediate.dense.weight False
pretrained_model.encoder.layer.17.intermediate.dense.bias False
pretrained_model.encoder.layer.17.output.dense.weight False
pretrained_model.encoder.layer.17.output.dense.bias False
pretrained_model.encoder.layer.17.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.attention.self.query.weight False
pretrained_model.encoder.layer.18.attention.self.query.bias False
pretrained_model.encoder.layer.18.attention.self.key.weight False
pretrained_model.encoder.layer.18.attention.self.key.bias False
pretrained_model.encoder.layer.18.attention.self.value.weight False
pretrained_model.encoder.layer.18.attention.self.value.bias False
pretrained_model.encoder.layer.18.attention.output.dense.weight False
pretrained_model.encoder.layer.18.attention.output.dense.bias False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.intermediate.dense.weight False
pretrained_model.encoder.layer.18.intermediate.dense.bias False
pretrained_model.encoder.layer.18.output.dense.weight False
pretrained_model.encoder.layer.18.output.dense.bias False
pretrained_model.encoder.layer.18.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.attention.self.query.weight False
pretrained_model.encoder.layer.19.attention.self.query.bias False
pretrained_model.encoder.layer.19.attention.self.key.weight False
pretrained_model.encoder.layer.19.attention.self.key.bias False
pretrained_model.encoder.layer.19.attention.self.value.weight False
pretrained_model.encoder.layer.19.attention.self.value.bias False
pretrained_model.encoder.layer.19.attention.output.dense.weight False
pretrained_model.encoder.layer.19.attention.output.dense.bias False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.intermediate.dense.weight False
pretrained_model.encoder.layer.19.intermediate.dense.bias False
pretrained_model.encoder.layer.19.output.dense.weight False
pretrained_model.encoder.layer.19.output.dense.bias False
pretrained_model.encoder.layer.19.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.attention.self.query.weight False
pretrained_model.encoder.layer.20.attention.self.query.bias False
pretrained_model.encoder.layer.20.attention.self.key.weight False
pretrained_model.encoder.layer.20.attention.self.key.bias False
pretrained_model.encoder.layer.20.attention.self.value.weight False
pretrained_model.encoder.layer.20.attention.self.value.bias False
pretrained_model.encoder.layer.20.attention.output.dense.weight False
pretrained_model.encoder.layer.20.attention.output.dense.bias False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.intermediate.dense.weight False
pretrained_model.encoder.layer.20.intermediate.dense.bias False
pretrained_model.encoder.layer.20.output.dense.weight False
pretrained_model.encoder.layer.20.output.dense.bias False
pretrained_model.encoder.layer.20.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.attention.self.query.weight False
pretrained_model.encoder.layer.21.attention.self.query.bias False
pretrained_model.encoder.layer.21.attention.self.key.weight False
pretrained_model.encoder.layer.21.attention.self.key.bias False
pretrained_model.encoder.layer.21.attention.self.value.weight False
pretrained_model.encoder.layer.21.attention.self.value.bias False
pretrained_model.encoder.layer.21.attention.output.dense.weight False
pretrained_model.encoder.layer.21.attention.output.dense.bias False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.intermediate.dense.weight False
pretrained_model.encoder.layer.21.intermediate.dense.bias False
pretrained_model.encoder.layer.21.output.dense.weight False
pretrained_model.encoder.layer.21.output.dense.bias False
pretrained_model.encoder.layer.21.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.attention.self.query.weight False
pretrained_model.encoder.layer.22.attention.self.query.bias False
pretrained_model.encoder.layer.22.attention.self.key.weight False
pretrained_model.encoder.layer.22.attention.self.key.bias False
pretrained_model.encoder.layer.22.attention.self.value.weight False
pretrained_model.encoder.layer.22.attention.self.value.bias False
pretrained_model.encoder.layer.22.attention.output.dense.weight False
pretrained_model.encoder.layer.22.attention.output.dense.bias False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.intermediate.dense.weight False
pretrained_model.encoder.layer.22.intermediate.dense.bias False
pretrained_model.encoder.layer.22.output.dense.weight False
pretrained_model.encoder.layer.22.output.dense.bias False
pretrained_model.encoder.layer.22.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.attention.self.query.weight False
pretrained_model.encoder.layer.23.attention.self.query.bias False
pretrained_model.encoder.layer.23.attention.self.key.weight False
pretrained_model.encoder.layer.23.attention.self.key.bias False
pretrained_model.encoder.layer.23.attention.self.value.weight False
pretrained_model.encoder.layer.23.attention.self.value.bias False
pretrained_model.encoder.layer.23.attention.output.dense.weight False
pretrained_model.encoder.layer.23.attention.output.dense.bias False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.intermediate.dense.weight False
pretrained_model.encoder.layer.23.intermediate.dense.bias False
pretrained_model.encoder.layer.23.output.dense.weight False
pretrained_model.encoder.layer.23.output.dense.bias False
pretrained_model.encoder.layer.23.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 222s, train loss=2.5913, train acc=13.92%, dev loss=2.2949, dev acc=29.05%
saving, test loss=2.3000, test acc=28.31%
epoch: 2/10000, 223s, train loss=2.2958, train acc=25.97%, dev loss=2.0589, dev acc=39.71%
saving, test loss=2.0663, test acc=38.04%
epoch: 3/10000, 223s, train loss=2.1256, train acc=32.51%, dev loss=1.9150, dev acc=43.78%
saving, test loss=1.9247, test acc=41.98%
epoch: 4/10000, 223s, train loss=2.0199, train acc=36.16%, dev loss=1.8221, dev acc=45.28%
saving, test loss=1.8335, test acc=44.31%
epoch: 5/10000, 224s, train loss=1.9606, train acc=38.29%, dev loss=1.7608, dev acc=46.68%
saving, test loss=1.7728, test acc=45.82%
epoch: 6/10000, 224s, train loss=1.9096, train acc=39.85%, dev loss=1.7164, dev acc=47.66%
saving, test loss=1.7302, test acc=46.61%
epoch: 7/10000, 224s, train loss=1.8792, train acc=40.65%, dev loss=1.6842, dev acc=48.44%
saving, test loss=1.6982, test acc=47.10%
epoch: 8/10000, 224s, train loss=1.8625, train acc=41.23%, dev loss=1.6595, dev acc=48.95%
saving, test loss=1.6739, test acc=47.71%
epoch: 9/10000, 224s, train loss=1.8431, train acc=41.73%, dev loss=1.6403, dev acc=49.40%
saving, test loss=1.6549, test acc=48.11%
epoch: 10/10000, 224s, train loss=1.8310, train acc=41.90%, dev loss=1.6253, dev acc=49.64%
saving, test loss=1.6404, test acc=48.35%
epoch: 11/10000, 224s, train loss=1.8149, train acc=42.53%, dev loss=1.6127, dev acc=49.74%
saving, test loss=1.6282, test acc=48.38%
epoch: 12/10000, 225s, train loss=1.8059, train acc=42.68%, dev loss=1.6023, dev acc=50.00%
saving, test loss=1.6184, test acc=48.61%
epoch: 13/10000, 225s, train loss=1.8071, train acc=42.46%, dev loss=1.5938, dev acc=50.06%
saving, test loss=1.6096, test acc=48.78%
epoch: 14/10000, 225s, train loss=1.7995, train acc=42.76%, dev loss=1.5857, dev acc=50.24%
saving, test loss=1.6015, test acc=49.09%
epoch: 15/10000, 225s, train loss=1.7951, train acc=42.82%, dev loss=1.5830, dev acc=50.15%
epoch: 16/10000, 224s, train loss=1.7928, train acc=42.80%, dev loss=1.5751, dev acc=50.21%
epoch: 17/10000, 225s, train loss=1.7859, train acc=42.90%, dev loss=1.5709, dev acc=50.47%
saving, test loss=1.5866, test acc=49.42%
epoch: 18/10000, 225s, train loss=1.7869, train acc=42.95%, dev loss=1.5672, dev acc=50.34%
epoch: 19/10000, 225s, train loss=1.7826, train acc=43.26%, dev loss=1.5631, dev acc=50.86%
saving, test loss=1.5801, test acc=49.13%
epoch: 20/10000, 225s, train loss=1.7837, train acc=43.06%, dev loss=1.5592, dev acc=50.71%
epoch: 21/10000, 225s, train loss=1.7800, train acc=43.58%, dev loss=1.5568, dev acc=50.92%
saving, test loss=1.5743, test acc=49.34%
epoch: 22/10000, 225s, train loss=1.7755, train acc=43.37%, dev loss=1.5560, dev acc=50.90%
epoch: 23/10000, 225s, train loss=1.7745, train acc=43.61%, dev loss=1.5530, dev acc=50.97%
saving, test loss=1.5695, test acc=49.56%
epoch: 24/10000, 225s, train loss=1.7736, train acc=43.51%, dev loss=1.5502, dev acc=50.90%
epoch: 25/10000, 225s, train loss=1.7810, train acc=43.18%, dev loss=1.5487, dev acc=50.77%
epoch: 26/10000, 259s, train loss=1.7716, train acc=43.69%, dev loss=1.5469, dev acc=50.73%
epoch: 27/10000, 283s, train loss=1.7673, train acc=43.48%, dev loss=1.5455, dev acc=50.79%
epoch: 28/10000, 222s, train loss=1.7719, train acc=43.34%, dev loss=1.5443, dev acc=50.71%
time used=7903.9s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/peterchou_nezha-chinese-base', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_culture': 0, 'news_finance': 1, 'news_story': 2, 'news_stock': 3, 'news_travel': 4, 'news_tech': 5, 'news_military': 6, 'news_entertainment': 7, 'news_car': 8, 'news_sports': 9, 'news_agriculture': 10, 'news_edu': 11, 'news_house': 12, 'news_world': 13, 'news_game': 14}
index_labels_dict={0: 'news_culture', 1: 'news_finance', 2: 'news_story', 3: 'news_stock', 4: 'news_travel', 5: 'news_tech', 6: 'news_military', 7: 'news_entertainment', 8: 'news_car', 9: 'news_sports', 10: 'news_agriculture', 11: 'news_edu', 12: 'news_house', 13: 'news_world', 14: 'news_game'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/peterchou_nezha-chinese-base were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/peterchou_nezha-chinese-base and are newly initialized: ['bert.embeddings.position_embeddings.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
CustomModel(
  (pretrained_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 68s, train loss=2.6122, train acc=10.78%, dev loss=2.5563, dev acc=16.02%
saving, test loss=2.5628, test acc=15.08%
epoch: 2/10000, 68s, train loss=2.5640, train acc=13.16%, dev loss=2.5328, dev acc=15.89%
epoch: 3/10000, 79s, train loss=2.5488, train acc=14.04%, dev loss=2.5225, dev acc=15.55%
epoch: 4/10000, 90s, train loss=2.5374, train acc=14.57%, dev loss=2.5134, dev acc=16.19%
saving, test loss=2.5189, test acc=15.36%
epoch: 5/10000, 89s, train loss=2.5325, train acc=14.71%, dev loss=2.5062, dev acc=16.02%
epoch: 6/10000, 90s, train loss=2.5251, train acc=15.34%, dev loss=2.5032, dev acc=15.61%
epoch: 7/10000, 90s, train loss=2.5228, train acc=15.42%, dev loss=2.4992, dev acc=15.55%
epoch: 8/10000, 95s, train loss=2.5193, train acc=15.51%, dev loss=2.4973, dev acc=15.07%
epoch: 9/10000, 92s, train loss=2.5175, train acc=15.67%, dev loss=2.4932, dev acc=15.05%
time used=901.3s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/uer_chinese_roberta_L-4_H-512', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_tech': 0, 'news_story': 1, 'news_stock': 2, 'news_sports': 3, 'news_entertainment': 4, 'news_edu': 5, 'news_game': 6, 'news_military': 7, 'news_culture': 8, 'news_car': 9, 'news_finance': 10, 'news_house': 11, 'news_travel': 12, 'news_world': 13, 'news_agriculture': 14}
index_labels_dict={0: 'news_tech', 1: 'news_story', 2: 'news_stock', 3: 'news_sports', 4: 'news_entertainment', 5: 'news_edu', 6: 'news_game', 7: 'news_military', 8: 'news_culture', 9: 'news_car', 10: 'news_finance', 11: 'news_house', 12: 'news_travel', 13: 'news_world', 14: 'news_agriculture'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	65
45	11
44	27
43	41
42	98
41	92
40	133
39	119
38	128
37	169
36	171
35	216
34	294
33	555
32	3351
31	2647
30	2397
29	2194
28	2237
27	2187
26	2247
25	2193
24	2281
23	2352
22	2237
21	2242
20	2031
19	2278
18	1894
17	1980
16	1801
15	1616
14	1491
13	1220
12	970
11	808
10	548
9	340
8	152
7	130
6	20
5	5
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/uer_chinese_roberta_L-4_H-512 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/uer_chinese_roberta_L-4_H-512 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
CustomModel(
  (pretrained_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 512, padding_idx=0)
      (position_embeddings): Embedding(512, 512)
      (token_type_embeddings): Embedding(2, 512)
      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=512, out_features=512, bias=True)
              (key): Linear(in_features=512, out_features=512, bias=True)
              (value): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=512, out_features=512, bias=True)
              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=512, out_features=2048, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=2048, out_features=512, bias=True)
            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=512, out_features=512, bias=True)
              (key): Linear(in_features=512, out_features=512, bias=True)
              (value): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=512, out_features=512, bias=True)
              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=512, out_features=2048, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=2048, out_features=512, bias=True)
            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=512, out_features=512, bias=True)
              (key): Linear(in_features=512, out_features=512, bias=True)
              (value): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=512, out_features=512, bias=True)
              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=512, out_features=2048, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=2048, out_features=512, bias=True)
            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=512, out_features=512, bias=True)
              (key): Linear(in_features=512, out_features=512, bias=True)
              (value): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=512, out_features=512, bias=True)
              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=512, out_features=2048, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=2048, out_features=512, bias=True)
            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=512, out_features=512, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=512, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 21s, train loss=2.4931, train acc=20.59%, dev loss=2.2470, dev acc=37.65%
saving, test loss=2.2529, test acc=37.61%
epoch: 2/10000, 17s, train loss=2.1678, train acc=38.09%, dev loss=1.9838, dev acc=44.36%
saving, test loss=1.9922, test acc=44.83%
epoch: 3/10000, 17s, train loss=1.9671, train acc=43.36%, dev loss=1.8190, dev acc=46.96%
saving, test loss=1.8266, test acc=47.60%
epoch: 4/10000, 17s, train loss=1.8457, train acc=44.86%, dev loss=1.7166, dev acc=48.03%
saving, test loss=1.7232, test acc=48.52%
epoch: 5/10000, 17s, train loss=1.7665, train acc=46.01%, dev loss=1.6513, dev acc=48.46%
saving, test loss=1.6571, test acc=49.17%
epoch: 6/10000, 17s, train loss=1.7169, train acc=46.49%, dev loss=1.6070, dev acc=48.89%
saving, test loss=1.6119, test acc=49.73%
epoch: 7/10000, 13s, train loss=1.6830, train acc=46.96%, dev loss=1.5759, dev acc=49.12%
saving, test loss=1.5802, test acc=49.93%
epoch: 8/10000, 17s, train loss=1.6561, train acc=47.52%, dev loss=1.5531, dev acc=49.46%
saving, test loss=1.5574, test acc=50.22%
epoch: 9/10000, 17s, train loss=1.6401, train acc=47.56%, dev loss=1.5360, dev acc=49.74%
saving, test loss=1.5399, test acc=50.39%
epoch: 10/10000, 17s, train loss=1.6260, train acc=47.87%, dev loss=1.5228, dev acc=49.78%
saving, test loss=1.5267, test acc=50.56%
epoch: 11/10000, 17s, train loss=1.6123, train acc=48.18%, dev loss=1.5128, dev acc=50.11%
saving, test loss=1.5165, test acc=50.63%
epoch: 12/10000, 17s, train loss=1.6085, train acc=47.69%, dev loss=1.5042, dev acc=50.32%
saving, test loss=1.5080, test acc=50.79%
epoch: 13/10000, 17s, train loss=1.6013, train acc=48.23%, dev loss=1.4974, dev acc=50.24%
epoch: 14/10000, 17s, train loss=1.5912, train acc=48.40%, dev loss=1.4916, dev acc=50.36%
saving, test loss=1.4957, test acc=50.97%
epoch: 15/10000, 17s, train loss=1.5877, train acc=48.29%, dev loss=1.4867, dev acc=50.64%
saving, test loss=1.4906, test acc=51.32%
epoch: 16/10000, 17s, train loss=1.5859, train acc=48.36%, dev loss=1.4825, dev acc=50.82%
saving, test loss=1.4863, test acc=51.34%
epoch: 17/10000, 17s, train loss=1.5793, train acc=48.57%, dev loss=1.4786, dev acc=50.96%
saving, test loss=1.4828, test acc=51.36%
epoch: 18/10000, 13s, train loss=1.5759, train acc=48.45%, dev loss=1.4753, dev acc=51.07%
saving, test loss=1.4799, test acc=51.29%
epoch: 19/10000, 17s, train loss=1.5770, train acc=48.44%, dev loss=1.4726, dev acc=50.92%
epoch: 20/10000, 17s, train loss=1.5732, train acc=48.72%, dev loss=1.4700, dev acc=51.12%
saving, test loss=1.4746, test acc=51.46%
epoch: 21/10000, 17s, train loss=1.5713, train acc=48.51%, dev loss=1.4679, dev acc=50.97%
epoch: 22/10000, 17s, train loss=1.5673, train acc=48.60%, dev loss=1.4664, dev acc=51.03%
epoch: 23/10000, 17s, train loss=1.5665, train acc=48.89%, dev loss=1.4641, dev acc=51.11%
epoch: 24/10000, 14s, train loss=1.5677, train acc=48.66%, dev loss=1.4625, dev acc=51.14%
saving, test loss=1.4673, test acc=51.75%
epoch: 25/10000, 17s, train loss=1.5654, train acc=48.59%, dev loss=1.4603, dev acc=51.35%
saving, test loss=1.4658, test acc=51.70%
epoch: 26/10000, 17s, train loss=1.5634, train acc=49.06%, dev loss=1.4593, dev acc=51.33%
epoch: 27/10000, 17s, train loss=1.5612, train acc=48.75%, dev loss=1.4576, dev acc=51.42%
saving, test loss=1.4633, test acc=51.68%
epoch: 28/10000, 17s, train loss=1.5595, train acc=48.83%, dev loss=1.4567, dev acc=51.35%
epoch: 29/10000, 17s, train loss=1.5638, train acc=48.60%, dev loss=1.4554, dev acc=51.42%
epoch: 30/10000, 16s, train loss=1.5580, train acc=48.99%, dev loss=1.4546, dev acc=51.33%
epoch: 31/10000, 14s, train loss=1.5617, train acc=48.90%, dev loss=1.4543, dev acc=51.54%
saving, test loss=1.4596, test acc=51.74%
epoch: 32/10000, 17s, train loss=1.5589, train acc=48.99%, dev loss=1.4532, dev acc=51.37%
epoch: 33/10000, 17s, train loss=1.5534, train acc=49.11%, dev loss=1.4527, dev acc=51.48%
epoch: 34/10000, 17s, train loss=1.5565, train acc=49.09%, dev loss=1.4518, dev acc=51.42%
epoch: 35/10000, 17s, train loss=1.5572, train acc=49.05%, dev loss=1.4506, dev acc=51.50%
epoch: 36/10000, 17s, train loss=1.5605, train acc=48.81%, dev loss=1.4501, dev acc=51.63%
saving, test loss=1.4561, test acc=51.89%
epoch: 37/10000, 14s, train loss=1.5558, train acc=48.66%, dev loss=1.4497, dev acc=51.57%
epoch: 38/10000, 17s, train loss=1.5585, train acc=48.88%, dev loss=1.4494, dev acc=51.59%
epoch: 39/10000, 17s, train loss=1.5535, train acc=48.93%, dev loss=1.4487, dev acc=51.59%
epoch: 40/10000, 17s, train loss=1.5607, train acc=48.85%, dev loss=1.4486, dev acc=51.61%
epoch: 41/10000, 17s, train loss=1.5554, train acc=49.04%, dev loss=1.4479, dev acc=51.67%
saving, test loss=1.4535, test acc=51.91%
epoch: 42/10000, 17s, train loss=1.5602, train acc=48.73%, dev loss=1.4479, dev acc=51.57%
epoch: 43/10000, 17s, train loss=1.5609, train acc=48.83%, dev loss=1.4476, dev acc=51.54%
epoch: 44/10000, 13s, train loss=1.5577, train acc=48.68%, dev loss=1.4470, dev acc=51.48%
epoch: 45/10000, 17s, train loss=1.5559, train acc=48.78%, dev loss=1.4468, dev acc=51.44%
epoch: 46/10000, 17s, train loss=1.5546, train acc=48.92%, dev loss=1.4467, dev acc=51.44%
time used=973.1s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/uer_roberta-base-finetuned-jd-full-chinese', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_tech': 0, 'news_car': 1, 'news_game': 2, 'news_sports': 3, 'news_house': 4, 'news_military': 5, 'news_agriculture': 6, 'news_finance': 7, 'news_edu': 8, 'news_world': 9, 'news_entertainment': 10, 'news_story': 11, 'news_stock': 12, 'news_culture': 13, 'news_travel': 14}
index_labels_dict={0: 'news_tech', 1: 'news_car', 2: 'news_game', 3: 'news_sports', 4: 'news_house', 5: 'news_military', 6: 'news_agriculture', 7: 'news_finance', 8: 'news_edu', 9: 'news_world', 10: 'news_entertainment', 11: 'news_story', 12: 'news_stock', 13: 'news_culture', 14: 'news_travel'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/uer_roberta-base-finetuned-jd-full-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 92s, train loss=2.5153, train acc=16.72%, dev loss=2.3592, dev acc=25.41%
saving, test loss=2.3685, test acc=23.56%
epoch: 2/10000, 94s, train loss=2.3249, train acc=26.26%, dev loss=2.2143, dev acc=32.08%
saving, test loss=2.2264, test acc=30.76%
epoch: 3/10000, 92s, train loss=2.2183, train acc=31.09%, dev loss=2.1101, dev acc=36.06%
saving, test loss=2.1217, test acc=34.90%
epoch: 4/10000, 95s, train loss=2.1361, train acc=34.31%, dev loss=2.0303, dev acc=38.49%
saving, test loss=2.0421, test acc=37.70%
epoch: 5/10000, 91s, train loss=2.0723, train acc=36.44%, dev loss=1.9692, dev acc=40.40%
saving, test loss=1.9814, test acc=39.79%
epoch: 6/10000, 95s, train loss=2.0286, train acc=37.73%, dev loss=1.9215, dev acc=41.02%
saving, test loss=1.9324, test acc=41.15%
epoch: 7/10000, 91s, train loss=1.9899, train acc=38.86%, dev loss=1.8830, dev acc=42.00%
saving, test loss=1.8936, test acc=42.20%
epoch: 8/10000, 94s, train loss=1.9631, train acc=39.41%, dev loss=1.8511, dev acc=42.58%
saving, test loss=1.8622, test acc=43.01%
epoch: 9/10000, 86s, train loss=1.9391, train acc=40.16%, dev loss=1.8257, dev acc=43.67%
saving, test loss=1.8358, test acc=43.56%
epoch: 10/10000, 89s, train loss=1.9215, train acc=40.41%, dev loss=1.8031, dev acc=43.85%
saving, test loss=1.8130, test acc=44.07%
epoch: 11/10000, 90s, train loss=1.9082, train acc=40.88%, dev loss=1.7862, dev acc=44.00%
saving, test loss=1.7957, test acc=44.38%
epoch: 12/10000, 90s, train loss=1.8922, train acc=41.24%, dev loss=1.7700, dev acc=44.58%
saving, test loss=1.7794, test acc=44.51%
epoch: 13/10000, 89s, train loss=1.8812, train acc=41.36%, dev loss=1.7563, dev acc=44.85%
saving, test loss=1.7660, test acc=44.66%
epoch: 14/10000, 90s, train loss=1.8673, train acc=41.66%, dev loss=1.7436, dev acc=45.11%
saving, test loss=1.7533, test acc=44.97%
epoch: 15/10000, 95s, train loss=1.8618, train acc=41.67%, dev loss=1.7337, dev acc=45.45%
saving, test loss=1.7425, test acc=45.42%
epoch: 16/10000, 93s, train loss=1.8554, train acc=41.74%, dev loss=1.7233, dev acc=45.60%
saving, test loss=1.7329, test acc=45.59%
epoch: 17/10000, 94s, train loss=1.8474, train acc=41.94%, dev loss=1.7163, dev acc=45.75%
saving, test loss=1.7254, test acc=45.66%
epoch: 18/10000, 95s, train loss=1.8430, train acc=42.09%, dev loss=1.7094, dev acc=45.91%
saving, test loss=1.7176, test acc=45.78%
epoch: 19/10000, 95s, train loss=1.8334, train acc=42.21%, dev loss=1.7026, dev acc=46.10%
saving, test loss=1.7112, test acc=46.01%
epoch: 20/10000, 95s, train loss=1.8312, train acc=42.46%, dev loss=1.6962, dev acc=46.20%
saving, test loss=1.7044, test acc=46.02%
epoch: 21/10000, 92s, train loss=1.8261, train acc=42.55%, dev loss=1.6926, dev acc=46.29%
saving, test loss=1.7010, test acc=46.20%
epoch: 22/10000, 92s, train loss=1.8248, train acc=42.52%, dev loss=1.6859, dev acc=46.40%
saving, test loss=1.6950, test acc=46.49%
epoch: 23/10000, 93s, train loss=1.8238, train acc=42.47%, dev loss=1.6816, dev acc=46.38%
epoch: 24/10000, 95s, train loss=1.8159, train acc=42.91%, dev loss=1.6777, dev acc=46.48%
saving, test loss=1.6861, test acc=46.65%
epoch: 25/10000, 95s, train loss=1.8148, train acc=42.82%, dev loss=1.6746, dev acc=46.55%
saving, test loss=1.6828, test acc=46.46%
epoch: 26/10000, 69s, train loss=1.8098, train acc=42.98%, dev loss=1.6705, dev acc=46.93%
saving, test loss=1.6787, test acc=46.66%
epoch: 27/10000, 69s, train loss=1.8117, train acc=42.61%, dev loss=1.6668, dev acc=46.74%
epoch: 28/10000, 69s, train loss=1.8085, train acc=43.21%, dev loss=1.6636, dev acc=46.85%
epoch: 29/10000, 70s, train loss=1.8058, train acc=42.96%, dev loss=1.6612, dev acc=46.81%
epoch: 30/10000, 70s, train loss=1.8059, train acc=42.59%, dev loss=1.6585, dev acc=47.00%
saving, test loss=1.6673, test acc=46.96%
epoch: 31/10000, 70s, train loss=1.7984, train acc=42.99%, dev loss=1.6557, dev acc=46.93%
epoch: 32/10000, 70s, train loss=1.8014, train acc=43.03%, dev loss=1.6533, dev acc=47.00%
epoch: 33/10000, 70s, train loss=1.7949, train acc=43.06%, dev loss=1.6512, dev acc=46.95%
epoch: 34/10000, 70s, train loss=1.7953, train acc=43.16%, dev loss=1.6487, dev acc=47.11%
saving, test loss=1.6577, test acc=47.09%
epoch: 35/10000, 70s, train loss=1.7975, train acc=42.82%, dev loss=1.6482, dev acc=47.13%
saving, test loss=1.6560, test acc=47.09%
epoch: 36/10000, 70s, train loss=1.7926, train acc=43.27%, dev loss=1.6474, dev acc=47.00%
epoch: 37/10000, 70s, train loss=1.7948, train acc=43.17%, dev loss=1.6455, dev acc=47.13%
epoch: 38/10000, 70s, train loss=1.7952, train acc=43.18%, dev loss=1.6439, dev acc=47.40%
saving, test loss=1.6518, test acc=47.27%
epoch: 39/10000, 70s, train loss=1.7873, train acc=43.51%, dev loss=1.6428, dev acc=47.19%
epoch: 40/10000, 70s, train loss=1.7894, train acc=43.14%, dev loss=1.6405, dev acc=47.13%
epoch: 41/10000, 70s, train loss=1.7915, train acc=43.17%, dev loss=1.6387, dev acc=47.36%
epoch: 42/10000, 70s, train loss=1.7897, train acc=43.39%, dev loss=1.6378, dev acc=47.41%
saving, test loss=1.6464, test acc=47.36%
epoch: 43/10000, 70s, train loss=1.7855, train acc=43.30%, dev loss=1.6367, dev acc=47.41%
epoch: 44/10000, 70s, train loss=1.7864, train acc=43.21%, dev loss=1.6351, dev acc=47.64%
saving, test loss=1.6429, test acc=47.30%
epoch: 45/10000, 70s, train loss=1.7822, train acc=43.41%, dev loss=1.6363, dev acc=47.43%
epoch: 46/10000, 70s, train loss=1.7859, train acc=43.09%, dev loss=1.6350, dev acc=47.28%
epoch: 47/10000, 70s, train loss=1.7906, train acc=43.02%, dev loss=1.6325, dev acc=47.38%
epoch: 48/10000, 70s, train loss=1.7890, train acc=43.28%, dev loss=1.6318, dev acc=47.47%
epoch: 49/10000, 70s, train loss=1.7822, train acc=43.51%, dev loss=1.6305, dev acc=47.68%
saving, test loss=1.6394, test acc=47.62%
epoch: 50/10000, 70s, train loss=1.7860, train acc=43.50%, dev loss=1.6302, dev acc=47.60%
epoch: 51/10000, 70s, train loss=1.7836, train acc=43.13%, dev loss=1.6300, dev acc=47.58%
epoch: 52/10000, 70s, train loss=1.7879, train acc=43.27%, dev loss=1.6285, dev acc=47.49%
epoch: 53/10000, 70s, train loss=1.7837, train acc=43.46%, dev loss=1.6283, dev acc=47.58%
epoch: 54/10000, 70s, train loss=1.7853, train acc=43.32%, dev loss=1.6278, dev acc=47.69%
saving, test loss=1.6365, test acc=47.53%
epoch: 55/10000, 70s, train loss=1.7808, train acc=43.56%, dev loss=1.6266, dev acc=47.68%
epoch: 56/10000, 70s, train loss=1.7868, train acc=43.36%, dev loss=1.6254, dev acc=47.69%
epoch: 57/10000, 70s, train loss=1.7868, train acc=43.35%, dev loss=1.6255, dev acc=47.73%
saving, test loss=1.6351, test acc=47.67%
epoch: 58/10000, 70s, train loss=1.7799, train acc=43.64%, dev loss=1.6253, dev acc=47.36%
epoch: 59/10000, 70s, train loss=1.7784, train acc=43.42%, dev loss=1.6252, dev acc=47.69%
epoch: 60/10000, 70s, train loss=1.7814, train acc=43.66%, dev loss=1.6243, dev acc=47.83%
saving, test loss=1.6335, test acc=47.72%
epoch: 61/10000, 70s, train loss=1.7809, train acc=43.56%, dev loss=1.6227, dev acc=47.60%
epoch: 62/10000, 70s, train loss=1.7787, train acc=43.60%, dev loss=1.6222, dev acc=47.68%
epoch: 63/10000, 70s, train loss=1.7796, train acc=43.39%, dev loss=1.6223, dev acc=47.58%
epoch: 64/10000, 70s, train loss=1.7817, train acc=43.25%, dev loss=1.6216, dev acc=47.77%
epoch: 65/10000, 70s, train loss=1.7814, train acc=43.27%, dev loss=1.6218, dev acc=47.84%
saving, test loss=1.6309, test acc=47.65%
epoch: 66/10000, 70s, train loss=1.7821, train acc=43.45%, dev loss=1.6226, dev acc=47.81%
epoch: 67/10000, 70s, train loss=1.7725, train acc=43.65%, dev loss=1.6211, dev acc=47.79%
epoch: 68/10000, 70s, train loss=1.7757, train acc=43.71%, dev loss=1.6206, dev acc=48.01%
saving, test loss=1.6296, test acc=47.87%
epoch: 69/10000, 70s, train loss=1.7741, train acc=43.58%, dev loss=1.6199, dev acc=47.84%
epoch: 70/10000, 70s, train loss=1.7754, train acc=43.68%, dev loss=1.6196, dev acc=48.05%
saving, test loss=1.6288, test acc=47.85%
epoch: 71/10000, 70s, train loss=1.7758, train acc=43.74%, dev loss=1.6184, dev acc=47.81%
epoch: 72/10000, 70s, train loss=1.7776, train acc=43.69%, dev loss=1.6185, dev acc=47.79%
epoch: 73/10000, 70s, train loss=1.7781, train acc=43.56%, dev loss=1.6186, dev acc=47.77%
epoch: 74/10000, 70s, train loss=1.7772, train acc=43.60%, dev loss=1.6177, dev acc=47.73%
epoch: 75/10000, 70s, train loss=1.7799, train acc=43.36%, dev loss=1.6175, dev acc=47.90%
time used=7153.8s
