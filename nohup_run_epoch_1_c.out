nohup: ignoring input
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='2', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-legal-electra-base-generator', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_car': 0, 'news_tech': 1, 'news_world': 2, 'news_finance': 3, 'news_game': 4, 'news_culture': 5, 'news_house': 6, 'news_agriculture': 7, 'news_story': 8, 'news_entertainment': 9, 'news_sports': 10, 'news_military': 11, 'news_edu': 12, 'news_travel': 13, 'news_stock': 14}
index_labels_dict={0: 'news_car', 1: 'news_tech', 2: 'news_world', 3: 'news_finance', 4: 'news_game', 5: 'news_culture', 6: 'news_house', 7: 'news_agriculture', 8: 'news_story', 9: 'news_entertainment', 10: 'news_sports', 11: 'news_military', 12: 'news_edu', 13: 'news_travel', 14: 'news_stock'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-legal-electra-base-generator were not used when initializing ElectraModel: ['generator_predictions.LayerNorm.weight', 'generator_lm_head.weight', 'generator_predictions.dense.weight', 'generator_predictions.LayerNorm.bias', 'generator_predictions.dense.bias', 'generator_lm_head.bias']
- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (embeddings_project): Linear(in_features=768, out_features=192, bias=True)
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=192, out_features=192, bias=True)
              (key): Linear(in_features=192, out_features=192, bias=True)
              (value): Linear(in_features=192, out_features=192, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=192, out_features=192, bias=True)
              (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=192, out_features=768, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=768, out_features=192, bias=True)
            (LayerNorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=192, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.embeddings_project.weight False
pretrained_model.embeddings_project.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 12s, train loss=2.6625, train acc=10.61%, dev loss=2.6037, dev acc=11.15%
saving, test loss=2.6067, test acc=10.90%
epoch: 2/10000, 12s, train loss=2.5988, train acc=12.31%, dev loss=2.5648, dev acc=11.64%
saving, test loss=2.5681, test acc=11.47%
epoch: 3/10000, 12s, train loss=2.5638, train acc=14.24%, dev loss=2.5352, dev acc=13.01%
saving, test loss=2.5391, test acc=13.23%
epoch: 4/10000, 12s, train loss=2.5361, train acc=16.42%, dev loss=2.5088, dev acc=16.21%
saving, test loss=2.5130, test acc=15.65%
epoch: 5/10000, 12s, train loss=2.5122, train acc=18.25%, dev loss=2.4842, dev acc=18.03%
saving, test loss=2.4887, test acc=17.71%
epoch: 6/10000, 12s, train loss=2.4906, train acc=20.09%, dev loss=2.4609, dev acc=20.78%
saving, test loss=2.4656, test acc=20.15%
epoch: 7/10000, 12s, train loss=2.4700, train acc=21.53%, dev loss=2.4387, dev acc=22.84%
saving, test loss=2.4438, test acc=22.22%
epoch: 8/10000, 12s, train loss=2.4504, train acc=22.79%, dev loss=2.4174, dev acc=25.02%
saving, test loss=2.4227, test acc=24.17%
epoch: 9/10000, 12s, train loss=2.4336, train acc=23.60%, dev loss=2.3972, dev acc=26.33%
saving, test loss=2.4027, test acc=25.17%
epoch: 10/10000, 12s, train loss=2.4184, train acc=24.59%, dev loss=2.3778, dev acc=27.72%
saving, test loss=2.3836, test acc=26.70%
epoch: 11/10000, 12s, train loss=2.4018, train acc=25.08%, dev loss=2.3595, dev acc=28.69%
saving, test loss=2.3654, test acc=27.81%
epoch: 12/10000, 12s, train loss=2.3852, train acc=26.01%, dev loss=2.3415, dev acc=30.04%
saving, test loss=2.3475, test acc=29.18%
epoch: 13/10000, 12s, train loss=2.3734, train acc=26.43%, dev loss=2.3244, dev acc=31.11%
saving, test loss=2.3308, test acc=30.18%
epoch: 14/10000, 12s, train loss=2.3576, train acc=27.21%, dev loss=2.3079, dev acc=31.82%
saving, test loss=2.3143, test acc=30.84%
epoch: 15/10000, 12s, train loss=2.3470, train acc=27.10%, dev loss=2.2920, dev acc=32.80%
saving, test loss=2.2985, test acc=32.00%
epoch: 16/10000, 12s, train loss=2.3311, train acc=28.04%, dev loss=2.2765, dev acc=33.45%
saving, test loss=2.2833, test acc=33.08%
epoch: 17/10000, 12s, train loss=2.3218, train acc=28.17%, dev loss=2.2620, dev acc=33.92%
saving, test loss=2.2688, test acc=33.40%
epoch: 18/10000, 12s, train loss=2.3115, train acc=28.69%, dev loss=2.2477, dev acc=34.65%
saving, test loss=2.2547, test acc=34.28%
epoch: 19/10000, 12s, train loss=2.2996, train acc=28.94%, dev loss=2.2341, dev acc=35.18%
saving, test loss=2.2411, test acc=34.75%
epoch: 20/10000, 12s, train loss=2.2882, train acc=29.17%, dev loss=2.2209, dev acc=35.64%
saving, test loss=2.2280, test acc=35.41%
epoch: 21/10000, 12s, train loss=2.2805, train acc=29.17%, dev loss=2.2082, dev acc=35.98%
saving, test loss=2.2154, test acc=35.79%
epoch: 22/10000, 12s, train loss=2.2722, train acc=29.56%, dev loss=2.1962, dev acc=36.53%
saving, test loss=2.2034, test acc=36.39%
epoch: 23/10000, 12s, train loss=2.2637, train acc=29.62%, dev loss=2.1845, dev acc=36.88%
saving, test loss=2.1919, test acc=36.65%
epoch: 24/10000, 12s, train loss=2.2509, train acc=30.06%, dev loss=2.1732, dev acc=37.44%
saving, test loss=2.1804, test acc=37.16%
epoch: 25/10000, 12s, train loss=2.2449, train acc=30.34%, dev loss=2.1622, dev acc=37.63%
saving, test loss=2.1696, test acc=37.26%
epoch: 26/10000, 12s, train loss=2.2391, train acc=30.51%, dev loss=2.1516, dev acc=38.12%
saving, test loss=2.1591, test acc=37.67%
epoch: 27/10000, 12s, train loss=2.2298, train acc=30.49%, dev loss=2.1414, dev acc=38.55%
saving, test loss=2.1489, test acc=38.04%
epoch: 28/10000, 12s, train loss=2.2254, train acc=30.55%, dev loss=2.1318, dev acc=38.62%
saving, test loss=2.1392, test acc=38.04%
epoch: 29/10000, 12s, train loss=2.2185, train acc=30.82%, dev loss=2.1225, dev acc=38.91%
saving, test loss=2.1300, test acc=38.42%
epoch: 30/10000, 12s, train loss=2.2124, train acc=30.82%, dev loss=2.1135, dev acc=39.09%
saving, test loss=2.1208, test acc=38.64%
epoch: 31/10000, 12s, train loss=2.2048, train acc=31.15%, dev loss=2.1046, dev acc=39.32%
saving, test loss=2.1120, test acc=39.23%
epoch: 32/10000, 12s, train loss=2.2024, train acc=31.07%, dev loss=2.0963, dev acc=39.11%
epoch: 33/10000, 12s, train loss=2.1929, train acc=31.45%, dev loss=2.0881, dev acc=39.56%
saving, test loss=2.0955, test acc=39.64%
epoch: 34/10000, 12s, train loss=2.1865, train acc=31.45%, dev loss=2.0803, dev acc=39.69%
saving, test loss=2.0877, test acc=39.60%
epoch: 35/10000, 12s, train loss=2.1836, train acc=31.60%, dev loss=2.0724, dev acc=39.94%
saving, test loss=2.0799, test acc=40.04%
epoch: 36/10000, 12s, train loss=2.1797, train acc=31.77%, dev loss=2.0651, dev acc=40.31%
saving, test loss=2.0726, test acc=40.13%
epoch: 37/10000, 12s, train loss=2.1745, train acc=31.58%, dev loss=2.0580, dev acc=40.31%
epoch: 38/10000, 12s, train loss=2.1694, train acc=31.84%, dev loss=2.0511, dev acc=40.46%
saving, test loss=2.0586, test acc=40.49%
epoch: 39/10000, 12s, train loss=2.1691, train acc=31.62%, dev loss=2.0446, dev acc=41.00%
saving, test loss=2.0519, test acc=40.57%
epoch: 40/10000, 12s, train loss=2.1606, train acc=31.79%, dev loss=2.0382, dev acc=40.89%
epoch: 41/10000, 12s, train loss=2.1549, train acc=31.97%, dev loss=2.0318, dev acc=41.21%
saving, test loss=2.0393, test acc=40.65%
epoch: 42/10000, 12s, train loss=2.1536, train acc=32.12%, dev loss=2.0257, dev acc=41.25%
saving, test loss=2.0332, test acc=40.76%
epoch: 43/10000, 12s, train loss=2.1479, train acc=32.14%, dev loss=2.0199, dev acc=41.40%
saving, test loss=2.0272, test acc=40.90%
epoch: 44/10000, 12s, train loss=2.1413, train acc=32.48%, dev loss=2.0142, dev acc=41.32%
epoch: 45/10000, 12s, train loss=2.1433, train acc=32.22%, dev loss=2.0086, dev acc=41.53%
saving, test loss=2.0160, test acc=41.14%
epoch: 46/10000, 12s, train loss=2.1385, train acc=32.61%, dev loss=2.0031, dev acc=41.47%
epoch: 47/10000, 12s, train loss=2.1401, train acc=32.38%, dev loss=1.9979, dev acc=41.81%
saving, test loss=2.0051, test acc=41.49%
epoch: 48/10000, 12s, train loss=2.1341, train acc=32.47%, dev loss=1.9931, dev acc=41.74%
epoch: 49/10000, 12s, train loss=2.1277, train acc=32.60%, dev loss=1.9882, dev acc=41.98%
saving, test loss=1.9955, test acc=41.58%
epoch: 50/10000, 12s, train loss=2.1263, train acc=32.65%, dev loss=1.9832, dev acc=42.00%
saving, test loss=1.9906, test acc=41.66%
epoch: 51/10000, 12s, train loss=2.1249, train acc=32.63%, dev loss=1.9787, dev acc=41.98%
epoch: 52/10000, 12s, train loss=2.1249, train acc=32.76%, dev loss=1.9743, dev acc=42.00%
epoch: 53/10000, 12s, train loss=2.1186, train acc=32.84%, dev loss=1.9702, dev acc=42.35%
saving, test loss=1.9774, test acc=41.98%
epoch: 54/10000, 12s, train loss=2.1144, train acc=32.66%, dev loss=1.9658, dev acc=42.20%
epoch: 55/10000, 12s, train loss=2.1141, train acc=32.67%, dev loss=1.9617, dev acc=42.34%
epoch: 56/10000, 12s, train loss=2.1115, train acc=32.99%, dev loss=1.9579, dev acc=42.37%
saving, test loss=1.9652, test acc=42.04%
epoch: 57/10000, 12s, train loss=2.1074, train acc=33.04%, dev loss=1.9540, dev acc=42.60%
saving, test loss=1.9614, test acc=42.32%
epoch: 58/10000, 12s, train loss=2.1050, train acc=33.15%, dev loss=1.9503, dev acc=42.37%
epoch: 59/10000, 12s, train loss=2.1028, train acc=33.04%, dev loss=1.9464, dev acc=42.56%
epoch: 60/10000, 12s, train loss=2.0977, train acc=33.12%, dev loss=1.9428, dev acc=42.71%
saving, test loss=1.9502, test acc=42.54%
epoch: 61/10000, 12s, train loss=2.0977, train acc=33.32%, dev loss=1.9396, dev acc=42.52%
epoch: 62/10000, 12s, train loss=2.0948, train acc=33.10%, dev loss=1.9362, dev acc=42.58%
epoch: 63/10000, 12s, train loss=2.0936, train acc=33.27%, dev loss=1.9327, dev acc=42.63%
epoch: 64/10000, 12s, train loss=2.0954, train acc=33.12%, dev loss=1.9296, dev acc=42.63%
epoch: 65/10000, 12s, train loss=2.0923, train acc=33.19%, dev loss=1.9265, dev acc=42.80%
saving, test loss=1.9334, test acc=42.68%
epoch: 66/10000, 12s, train loss=2.0934, train acc=33.54%, dev loss=1.9233, dev acc=42.86%
saving, test loss=1.9304, test acc=42.73%
epoch: 67/10000, 12s, train loss=2.0881, train acc=33.37%, dev loss=1.9206, dev acc=42.84%
epoch: 68/10000, 12s, train loss=2.0911, train acc=33.47%, dev loss=1.9175, dev acc=42.95%
saving, test loss=1.9245, test acc=43.05%
epoch: 69/10000, 12s, train loss=2.0811, train acc=33.49%, dev loss=1.9147, dev acc=43.03%
saving, test loss=1.9216, test acc=43.00%
epoch: 70/10000, 12s, train loss=2.0817, train acc=33.52%, dev loss=1.9119, dev acc=43.14%
saving, test loss=1.9189, test acc=43.00%
epoch: 71/10000, 12s, train loss=2.0811, train acc=33.71%, dev loss=1.9094, dev acc=43.08%
epoch: 72/10000, 12s, train loss=2.0833, train acc=33.35%, dev loss=1.9067, dev acc=43.16%
saving, test loss=1.9135, test acc=43.16%
epoch: 73/10000, 12s, train loss=2.0791, train acc=33.67%, dev loss=1.9041, dev acc=43.25%
saving, test loss=1.9110, test acc=43.16%
epoch: 74/10000, 12s, train loss=2.0771, train acc=33.77%, dev loss=1.9017, dev acc=43.20%
epoch: 75/10000, 12s, train loss=2.0814, train acc=33.20%, dev loss=1.8995, dev acc=43.23%
epoch: 76/10000, 12s, train loss=2.0785, train acc=33.65%, dev loss=1.8970, dev acc=43.44%
saving, test loss=1.9037, test acc=43.36%
epoch: 77/10000, 12s, train loss=2.0781, train acc=33.85%, dev loss=1.8946, dev acc=43.29%
epoch: 78/10000, 12s, train loss=2.0670, train acc=33.96%, dev loss=1.8925, dev acc=43.38%
epoch: 79/10000, 12s, train loss=2.0693, train acc=33.65%, dev loss=1.8903, dev acc=43.33%
epoch: 80/10000, 12s, train loss=2.0702, train acc=33.90%, dev loss=1.8881, dev acc=43.65%
saving, test loss=1.8948, test acc=43.54%
epoch: 81/10000, 12s, train loss=2.0653, train acc=33.89%, dev loss=1.8860, dev acc=43.63%
epoch: 82/10000, 12s, train loss=2.0674, train acc=33.68%, dev loss=1.8839, dev acc=43.52%
epoch: 83/10000, 12s, train loss=2.0694, train acc=33.94%, dev loss=1.8819, dev acc=43.74%
saving, test loss=1.8885, test acc=43.74%
epoch: 84/10000, 12s, train loss=2.0651, train acc=33.77%, dev loss=1.8801, dev acc=43.53%
epoch: 85/10000, 12s, train loss=2.0656, train acc=34.01%, dev loss=1.8783, dev acc=43.55%
epoch: 86/10000, 12s, train loss=2.0668, train acc=33.70%, dev loss=1.8763, dev acc=43.68%
epoch: 87/10000, 12s, train loss=2.0609, train acc=33.99%, dev loss=1.8747, dev acc=43.55%
epoch: 88/10000, 12s, train loss=2.0624, train acc=33.70%, dev loss=1.8728, dev acc=43.50%
time used=1402.6s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='2', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-legal-electra-large-generator', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_finance': 0, 'news_world': 1, 'news_stock': 2, 'news_entertainment': 3, 'news_culture': 4, 'news_car': 5, 'news_tech': 6, 'news_sports': 7, 'news_story': 8, 'news_house': 9, 'news_travel': 10, 'news_game': 11, 'news_edu': 12, 'news_military': 13, 'news_agriculture': 14}
index_labels_dict={0: 'news_finance', 1: 'news_world', 2: 'news_stock', 3: 'news_entertainment', 4: 'news_culture', 5: 'news_car', 6: 'news_tech', 7: 'news_sports', 8: 'news_story', 9: 'news_house', 10: 'news_travel', 11: 'news_game', 12: 'news_edu', 13: 'news_military', 14: 'news_agriculture'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-legal-electra-large-generator were not used when initializing ElectraModel: ['generator_predictions.dense.weight', 'generator_lm_head.weight', 'generator_predictions.LayerNorm.bias', 'generator_lm_head.bias', 'generator_predictions.LayerNorm.weight', 'generator_predictions.dense.bias']
- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (embeddings_project): Linear(in_features=1024, out_features=256, bias=True)
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (12): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (13): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (14): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (15): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (16): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (17): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (18): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (19): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (20): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (21): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (22): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (23): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=256, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.embeddings_project.weight False
pretrained_model.embeddings_project.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.attention.self.query.weight False
pretrained_model.encoder.layer.12.attention.self.query.bias False
pretrained_model.encoder.layer.12.attention.self.key.weight False
pretrained_model.encoder.layer.12.attention.self.key.bias False
pretrained_model.encoder.layer.12.attention.self.value.weight False
pretrained_model.encoder.layer.12.attention.self.value.bias False
pretrained_model.encoder.layer.12.attention.output.dense.weight False
pretrained_model.encoder.layer.12.attention.output.dense.bias False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.intermediate.dense.weight False
pretrained_model.encoder.layer.12.intermediate.dense.bias False
pretrained_model.encoder.layer.12.output.dense.weight False
pretrained_model.encoder.layer.12.output.dense.bias False
pretrained_model.encoder.layer.12.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.attention.self.query.weight False
pretrained_model.encoder.layer.13.attention.self.query.bias False
pretrained_model.encoder.layer.13.attention.self.key.weight False
pretrained_model.encoder.layer.13.attention.self.key.bias False
pretrained_model.encoder.layer.13.attention.self.value.weight False
pretrained_model.encoder.layer.13.attention.self.value.bias False
pretrained_model.encoder.layer.13.attention.output.dense.weight False
pretrained_model.encoder.layer.13.attention.output.dense.bias False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.intermediate.dense.weight False
pretrained_model.encoder.layer.13.intermediate.dense.bias False
pretrained_model.encoder.layer.13.output.dense.weight False
pretrained_model.encoder.layer.13.output.dense.bias False
pretrained_model.encoder.layer.13.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.attention.self.query.weight False
pretrained_model.encoder.layer.14.attention.self.query.bias False
pretrained_model.encoder.layer.14.attention.self.key.weight False
pretrained_model.encoder.layer.14.attention.self.key.bias False
pretrained_model.encoder.layer.14.attention.self.value.weight False
pretrained_model.encoder.layer.14.attention.self.value.bias False
pretrained_model.encoder.layer.14.attention.output.dense.weight False
pretrained_model.encoder.layer.14.attention.output.dense.bias False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.intermediate.dense.weight False
pretrained_model.encoder.layer.14.intermediate.dense.bias False
pretrained_model.encoder.layer.14.output.dense.weight False
pretrained_model.encoder.layer.14.output.dense.bias False
pretrained_model.encoder.layer.14.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.attention.self.query.weight False
pretrained_model.encoder.layer.15.attention.self.query.bias False
pretrained_model.encoder.layer.15.attention.self.key.weight False
pretrained_model.encoder.layer.15.attention.self.key.bias False
pretrained_model.encoder.layer.15.attention.self.value.weight False
pretrained_model.encoder.layer.15.attention.self.value.bias False
pretrained_model.encoder.layer.15.attention.output.dense.weight False
pretrained_model.encoder.layer.15.attention.output.dense.bias False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.intermediate.dense.weight False
pretrained_model.encoder.layer.15.intermediate.dense.bias False
pretrained_model.encoder.layer.15.output.dense.weight False
pretrained_model.encoder.layer.15.output.dense.bias False
pretrained_model.encoder.layer.15.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.attention.self.query.weight False
pretrained_model.encoder.layer.16.attention.self.query.bias False
pretrained_model.encoder.layer.16.attention.self.key.weight False
pretrained_model.encoder.layer.16.attention.self.key.bias False
pretrained_model.encoder.layer.16.attention.self.value.weight False
pretrained_model.encoder.layer.16.attention.self.value.bias False
pretrained_model.encoder.layer.16.attention.output.dense.weight False
pretrained_model.encoder.layer.16.attention.output.dense.bias False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.intermediate.dense.weight False
pretrained_model.encoder.layer.16.intermediate.dense.bias False
pretrained_model.encoder.layer.16.output.dense.weight False
pretrained_model.encoder.layer.16.output.dense.bias False
pretrained_model.encoder.layer.16.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.attention.self.query.weight False
pretrained_model.encoder.layer.17.attention.self.query.bias False
pretrained_model.encoder.layer.17.attention.self.key.weight False
pretrained_model.encoder.layer.17.attention.self.key.bias False
pretrained_model.encoder.layer.17.attention.self.value.weight False
pretrained_model.encoder.layer.17.attention.self.value.bias False
pretrained_model.encoder.layer.17.attention.output.dense.weight False
pretrained_model.encoder.layer.17.attention.output.dense.bias False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.intermediate.dense.weight False
pretrained_model.encoder.layer.17.intermediate.dense.bias False
pretrained_model.encoder.layer.17.output.dense.weight False
pretrained_model.encoder.layer.17.output.dense.bias False
pretrained_model.encoder.layer.17.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.attention.self.query.weight False
pretrained_model.encoder.layer.18.attention.self.query.bias False
pretrained_model.encoder.layer.18.attention.self.key.weight False
pretrained_model.encoder.layer.18.attention.self.key.bias False
pretrained_model.encoder.layer.18.attention.self.value.weight False
pretrained_model.encoder.layer.18.attention.self.value.bias False
pretrained_model.encoder.layer.18.attention.output.dense.weight False
pretrained_model.encoder.layer.18.attention.output.dense.bias False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.intermediate.dense.weight False
pretrained_model.encoder.layer.18.intermediate.dense.bias False
pretrained_model.encoder.layer.18.output.dense.weight False
pretrained_model.encoder.layer.18.output.dense.bias False
pretrained_model.encoder.layer.18.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.attention.self.query.weight False
pretrained_model.encoder.layer.19.attention.self.query.bias False
pretrained_model.encoder.layer.19.attention.self.key.weight False
pretrained_model.encoder.layer.19.attention.self.key.bias False
pretrained_model.encoder.layer.19.attention.self.value.weight False
pretrained_model.encoder.layer.19.attention.self.value.bias False
pretrained_model.encoder.layer.19.attention.output.dense.weight False
pretrained_model.encoder.layer.19.attention.output.dense.bias False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.intermediate.dense.weight False
pretrained_model.encoder.layer.19.intermediate.dense.bias False
pretrained_model.encoder.layer.19.output.dense.weight False
pretrained_model.encoder.layer.19.output.dense.bias False
pretrained_model.encoder.layer.19.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.attention.self.query.weight False
pretrained_model.encoder.layer.20.attention.self.query.bias False
pretrained_model.encoder.layer.20.attention.self.key.weight False
pretrained_model.encoder.layer.20.attention.self.key.bias False
pretrained_model.encoder.layer.20.attention.self.value.weight False
pretrained_model.encoder.layer.20.attention.self.value.bias False
pretrained_model.encoder.layer.20.attention.output.dense.weight False
pretrained_model.encoder.layer.20.attention.output.dense.bias False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.intermediate.dense.weight False
pretrained_model.encoder.layer.20.intermediate.dense.bias False
pretrained_model.encoder.layer.20.output.dense.weight False
pretrained_model.encoder.layer.20.output.dense.bias False
pretrained_model.encoder.layer.20.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.attention.self.query.weight False
pretrained_model.encoder.layer.21.attention.self.query.bias False
pretrained_model.encoder.layer.21.attention.self.key.weight False
pretrained_model.encoder.layer.21.attention.self.key.bias False
pretrained_model.encoder.layer.21.attention.self.value.weight False
pretrained_model.encoder.layer.21.attention.self.value.bias False
pretrained_model.encoder.layer.21.attention.output.dense.weight False
pretrained_model.encoder.layer.21.attention.output.dense.bias False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.intermediate.dense.weight False
pretrained_model.encoder.layer.21.intermediate.dense.bias False
pretrained_model.encoder.layer.21.output.dense.weight False
pretrained_model.encoder.layer.21.output.dense.bias False
pretrained_model.encoder.layer.21.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.attention.self.query.weight False
pretrained_model.encoder.layer.22.attention.self.query.bias False
pretrained_model.encoder.layer.22.attention.self.key.weight False
pretrained_model.encoder.layer.22.attention.self.key.bias False
pretrained_model.encoder.layer.22.attention.self.value.weight False
pretrained_model.encoder.layer.22.attention.self.value.bias False
pretrained_model.encoder.layer.22.attention.output.dense.weight False
pretrained_model.encoder.layer.22.attention.output.dense.bias False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.intermediate.dense.weight False
pretrained_model.encoder.layer.22.intermediate.dense.bias False
pretrained_model.encoder.layer.22.output.dense.weight False
pretrained_model.encoder.layer.22.output.dense.bias False
pretrained_model.encoder.layer.22.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.attention.self.query.weight False
pretrained_model.encoder.layer.23.attention.self.query.bias False
pretrained_model.encoder.layer.23.attention.self.key.weight False
pretrained_model.encoder.layer.23.attention.self.key.bias False
pretrained_model.encoder.layer.23.attention.self.value.weight False
pretrained_model.encoder.layer.23.attention.self.value.bias False
pretrained_model.encoder.layer.23.attention.output.dense.weight False
pretrained_model.encoder.layer.23.attention.output.dense.bias False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.intermediate.dense.weight False
pretrained_model.encoder.layer.23.intermediate.dense.bias False
pretrained_model.encoder.layer.23.output.dense.weight False
pretrained_model.encoder.layer.23.output.dense.bias False
pretrained_model.encoder.layer.23.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.output.LayerNorm.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 30s, train loss=2.6247, train acc=11.02%, dev loss=2.5108, dev acc=15.09%
saving, test loss=2.5158, test acc=13.89%
epoch: 2/10000, 30s, train loss=2.5051, train acc=16.85%, dev loss=2.4158, dev acc=23.37%
saving, test loss=2.4215, test acc=22.42%
epoch: 3/10000, 30s, train loss=2.4227, train acc=22.11%, dev loss=2.3353, dev acc=29.03%
saving, test loss=2.3415, test acc=27.70%
epoch: 4/10000, 30s, train loss=2.3531, train acc=26.16%, dev loss=2.2657, dev acc=32.33%
saving, test loss=2.2724, test acc=31.63%
epoch: 5/10000, 30s, train loss=2.2950, train acc=28.33%, dev loss=2.2052, dev acc=34.73%
saving, test loss=2.2122, test acc=34.48%
epoch: 6/10000, 30s, train loss=2.2491, train acc=30.24%, dev loss=2.1525, dev acc=36.58%
saving, test loss=2.1598, test acc=36.76%
epoch: 7/10000, 30s, train loss=2.2112, train acc=31.40%, dev loss=2.1070, dev acc=38.27%
saving, test loss=2.1143, test acc=38.34%
epoch: 8/10000, 30s, train loss=2.1755, train acc=32.39%, dev loss=2.0669, dev acc=39.69%
saving, test loss=2.0742, test acc=39.60%
epoch: 9/10000, 30s, train loss=2.1467, train acc=33.23%, dev loss=2.0319, dev acc=40.69%
saving, test loss=2.0393, test acc=40.59%
epoch: 10/10000, 30s, train loss=2.1242, train acc=33.61%, dev loss=2.0012, dev acc=41.38%
saving, test loss=2.0085, test acc=41.57%
epoch: 11/10000, 30s, train loss=2.1039, train acc=34.12%, dev loss=1.9737, dev acc=42.05%
saving, test loss=1.9809, test acc=42.20%
epoch: 12/10000, 30s, train loss=2.0817, train acc=34.51%, dev loss=1.9493, dev acc=42.47%
saving, test loss=1.9564, test acc=42.96%
epoch: 13/10000, 30s, train loss=2.0696, train acc=34.78%, dev loss=1.9276, dev acc=42.71%
saving, test loss=1.9347, test acc=43.35%
epoch: 14/10000, 30s, train loss=2.0534, train acc=35.36%, dev loss=1.9082, dev acc=43.18%
saving, test loss=1.9149, test acc=44.04%
epoch: 15/10000, 30s, train loss=2.0440, train acc=35.47%, dev loss=1.8905, dev acc=43.68%
saving, test loss=1.8973, test acc=44.51%
epoch: 16/10000, 30s, train loss=2.0337, train acc=35.51%, dev loss=1.8749, dev acc=44.06%
saving, test loss=1.8817, test acc=44.68%
epoch: 17/10000, 30s, train loss=2.0260, train acc=35.89%, dev loss=1.8610, dev acc=44.30%
saving, test loss=1.8677, test acc=44.93%
epoch: 18/10000, 30s, train loss=2.0121, train acc=36.19%, dev loss=1.8483, dev acc=44.58%
saving, test loss=1.8550, test acc=45.06%
epoch: 19/10000, 30s, train loss=2.0056, train acc=36.24%, dev loss=1.8363, dev acc=45.01%
saving, test loss=1.8429, test acc=45.38%
epoch: 20/10000, 30s, train loss=2.0013, train acc=36.35%, dev loss=1.8257, dev acc=45.24%
saving, test loss=1.8324, test acc=45.64%
epoch: 21/10000, 30s, train loss=1.9985, train acc=36.35%, dev loss=1.8159, dev acc=45.65%
saving, test loss=1.8226, test acc=45.88%
epoch: 22/10000, 30s, train loss=1.9898, train acc=36.88%, dev loss=1.8071, dev acc=45.84%
saving, test loss=1.8138, test acc=45.95%
epoch: 23/10000, 30s, train loss=1.9855, train acc=36.93%, dev loss=1.7988, dev acc=46.01%
saving, test loss=1.8054, test acc=46.24%
epoch: 24/10000, 30s, train loss=1.9816, train acc=36.81%, dev loss=1.7912, dev acc=46.14%
saving, test loss=1.7979, test acc=46.22%
epoch: 25/10000, 30s, train loss=1.9763, train acc=36.96%, dev loss=1.7844, dev acc=46.08%
epoch: 26/10000, 30s, train loss=1.9720, train acc=36.89%, dev loss=1.7776, dev acc=46.29%
saving, test loss=1.7844, test acc=46.50%
epoch: 27/10000, 30s, train loss=1.9703, train acc=37.08%, dev loss=1.7710, dev acc=46.57%
saving, test loss=1.7780, test acc=46.75%
epoch: 28/10000, 30s, train loss=1.9684, train acc=37.10%, dev loss=1.7657, dev acc=46.53%
epoch: 29/10000, 30s, train loss=1.9571, train acc=37.36%, dev loss=1.7610, dev acc=46.68%
saving, test loss=1.7680, test acc=46.56%
epoch: 30/10000, 30s, train loss=1.9625, train acc=37.13%, dev loss=1.7563, dev acc=46.85%
saving, test loss=1.7630, test acc=46.84%
epoch: 31/10000, 30s, train loss=1.9580, train acc=37.23%, dev loss=1.7510, dev acc=46.89%
saving, test loss=1.7580, test acc=46.89%
epoch: 32/10000, 30s, train loss=1.9546, train acc=37.43%, dev loss=1.7467, dev acc=47.00%
saving, test loss=1.7535, test acc=46.99%
epoch: 33/10000, 30s, train loss=1.9598, train acc=37.09%, dev loss=1.7430, dev acc=47.23%
saving, test loss=1.7500, test acc=47.16%
epoch: 34/10000, 30s, train loss=1.9514, train acc=37.50%, dev loss=1.7394, dev acc=47.15%
epoch: 35/10000, 30s, train loss=1.9529, train acc=37.42%, dev loss=1.7361, dev acc=47.23%
epoch: 36/10000, 30s, train loss=1.9449, train acc=37.57%, dev loss=1.7328, dev acc=47.23%
epoch: 37/10000, 30s, train loss=1.9510, train acc=37.19%, dev loss=1.7296, dev acc=47.26%
saving, test loss=1.7371, test acc=47.26%
epoch: 38/10000, 30s, train loss=1.9489, train acc=37.51%, dev loss=1.7275, dev acc=47.23%
epoch: 39/10000, 30s, train loss=1.9487, train acc=37.45%, dev loss=1.7249, dev acc=47.17%
epoch: 40/10000, 30s, train loss=1.9381, train acc=37.88%, dev loss=1.7225, dev acc=47.08%
epoch: 41/10000, 30s, train loss=1.9443, train acc=37.49%, dev loss=1.7196, dev acc=47.41%
saving, test loss=1.7268, test acc=47.39%
epoch: 42/10000, 30s, train loss=1.9455, train acc=37.72%, dev loss=1.7176, dev acc=47.45%
saving, test loss=1.7250, test acc=47.34%
epoch: 43/10000, 30s, train loss=1.9375, train acc=37.58%, dev loss=1.7150, dev acc=47.58%
saving, test loss=1.7225, test acc=47.56%
epoch: 44/10000, 30s, train loss=1.9404, train acc=37.60%, dev loss=1.7129, dev acc=47.56%
epoch: 45/10000, 30s, train loss=1.9388, train acc=37.93%, dev loss=1.7110, dev acc=47.45%
epoch: 46/10000, 30s, train loss=1.9362, train acc=37.59%, dev loss=1.7089, dev acc=47.47%
epoch: 47/10000, 30s, train loss=1.9414, train acc=37.48%, dev loss=1.7075, dev acc=47.62%
saving, test loss=1.7152, test acc=47.39%
epoch: 48/10000, 30s, train loss=1.9384, train acc=37.65%, dev loss=1.7063, dev acc=47.58%
epoch: 49/10000, 30s, train loss=1.9346, train acc=37.87%, dev loss=1.7046, dev acc=47.53%
epoch: 50/10000, 30s, train loss=1.9294, train acc=38.07%, dev loss=1.7029, dev acc=47.49%
epoch: 51/10000, 30s, train loss=1.9282, train acc=37.98%, dev loss=1.7012, dev acc=47.45%
epoch: 52/10000, 30s, train loss=1.9329, train acc=37.79%, dev loss=1.7000, dev acc=47.51%
time used=1993.0s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='2', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-legal-electra-small-discriminator', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_edu': 0, 'news_sports': 1, 'news_military': 2, 'news_world': 3, 'news_stock': 4, 'news_house': 5, 'news_game': 6, 'news_tech': 7, 'news_finance': 8, 'news_culture': 9, 'news_agriculture': 10, 'news_car': 11, 'news_travel': 12, 'news_story': 13, 'news_entertainment': 14}
index_labels_dict={0: 'news_edu', 1: 'news_sports', 2: 'news_military', 3: 'news_world', 4: 'news_stock', 5: 'news_house', 6: 'news_game', 7: 'news_tech', 8: 'news_finance', 9: 'news_culture', 10: 'news_agriculture', 11: 'news_car', 12: 'news_travel', 13: 'news_story', 14: 'news_entertainment'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-legal-electra-small-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']
- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(21128, 128, padding_idx=0)
      (position_embeddings): Embedding(512, 128)
      (token_type_embeddings): Embedding(2, 128)
      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=256, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.embeddings_project.weight False
pretrained_model.embeddings_project.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 15s, train loss=2.6486, train acc=10.56%, dev loss=2.5600, dev acc=13.49%
saving, test loss=2.5668, test acc=13.17%
epoch: 2/10000, 15s, train loss=2.5638, train acc=13.95%, dev loss=2.5197, dev acc=16.92%
saving, test loss=2.5271, test acc=16.14%
epoch: 3/10000, 15s, train loss=2.5258, train acc=15.89%, dev loss=2.4928, dev acc=18.61%
saving, test loss=2.5007, test acc=17.81%
epoch: 4/10000, 15s, train loss=2.4981, train acc=17.85%, dev loss=2.4720, dev acc=19.40%
saving, test loss=2.4799, test acc=18.73%
epoch: 5/10000, 15s, train loss=2.4770, train acc=18.75%, dev loss=2.4536, dev acc=19.81%
saving, test loss=2.4612, test acc=19.19%
epoch: 6/10000, 15s, train loss=2.4594, train acc=19.34%, dev loss=2.4387, dev acc=20.33%
saving, test loss=2.4459, test acc=20.06%
epoch: 7/10000, 15s, train loss=2.4472, train acc=19.78%, dev loss=2.4251, dev acc=20.86%
saving, test loss=2.4319, test acc=20.38%
epoch: 8/10000, 15s, train loss=2.4336, train acc=20.36%, dev loss=2.4134, dev acc=21.38%
saving, test loss=2.4199, test acc=20.79%
epoch: 9/10000, 15s, train loss=2.4199, train acc=20.86%, dev loss=2.4030, dev acc=21.57%
saving, test loss=2.4093, test acc=21.23%
epoch: 10/10000, 15s, train loss=2.4136, train acc=21.17%, dev loss=2.3930, dev acc=22.02%
saving, test loss=2.3987, test acc=21.63%
epoch: 11/10000, 15s, train loss=2.4050, train acc=21.16%, dev loss=2.3837, dev acc=22.40%
saving, test loss=2.3890, test acc=22.07%
epoch: 12/10000, 15s, train loss=2.3990, train acc=21.61%, dev loss=2.3771, dev acc=22.64%
saving, test loss=2.3823, test acc=22.46%
epoch: 13/10000, 15s, train loss=2.3941, train acc=21.88%, dev loss=2.3688, dev acc=23.05%
saving, test loss=2.3738, test acc=22.70%
epoch: 14/10000, 15s, train loss=2.3860, train acc=22.14%, dev loss=2.3622, dev acc=23.22%
saving, test loss=2.3668, test acc=23.01%
epoch: 15/10000, 15s, train loss=2.3858, train acc=21.91%, dev loss=2.3563, dev acc=23.31%
saving, test loss=2.3610, test acc=23.33%
epoch: 16/10000, 15s, train loss=2.3799, train acc=22.38%, dev loss=2.3508, dev acc=23.61%
saving, test loss=2.3553, test acc=23.61%
epoch: 17/10000, 15s, train loss=2.3742, train acc=22.67%, dev loss=2.3467, dev acc=23.82%
saving, test loss=2.3511, test acc=23.75%
epoch: 18/10000, 15s, train loss=2.3744, train acc=22.64%, dev loss=2.3415, dev acc=23.97%
saving, test loss=2.3459, test acc=23.95%
epoch: 19/10000, 15s, train loss=2.3684, train acc=22.62%, dev loss=2.3370, dev acc=24.21%
saving, test loss=2.3412, test acc=24.24%
epoch: 20/10000, 15s, train loss=2.3658, train acc=22.79%, dev loss=2.3322, dev acc=24.40%
saving, test loss=2.3361, test acc=24.34%
epoch: 21/10000, 15s, train loss=2.3639, train acc=22.93%, dev loss=2.3281, dev acc=24.57%
saving, test loss=2.3320, test acc=24.53%
epoch: 22/10000, 15s, train loss=2.3591, train acc=23.03%, dev loss=2.3247, dev acc=24.66%
saving, test loss=2.3286, test acc=24.55%
epoch: 23/10000, 15s, train loss=2.3570, train acc=23.09%, dev loss=2.3214, dev acc=24.72%
saving, test loss=2.3253, test acc=24.61%
epoch: 24/10000, 15s, train loss=2.3521, train acc=23.09%, dev loss=2.3187, dev acc=24.72%
epoch: 25/10000, 15s, train loss=2.3555, train acc=23.06%, dev loss=2.3152, dev acc=24.83%
saving, test loss=2.3187, test acc=24.88%
epoch: 26/10000, 15s, train loss=2.3515, train acc=23.02%, dev loss=2.3109, dev acc=24.94%
saving, test loss=2.3144, test acc=25.13%
epoch: 27/10000, 15s, train loss=2.3551, train acc=23.33%, dev loss=2.3106, dev acc=24.98%
saving, test loss=2.3141, test acc=25.08%
epoch: 28/10000, 15s, train loss=2.3500, train acc=23.24%, dev loss=2.3074, dev acc=24.91%
epoch: 29/10000, 15s, train loss=2.3501, train acc=23.03%, dev loss=2.3042, dev acc=25.09%
saving, test loss=2.3075, test acc=25.26%
epoch: 30/10000, 15s, train loss=2.3490, train acc=23.27%, dev loss=2.3013, dev acc=25.24%
saving, test loss=2.3042, test acc=25.31%
epoch: 31/10000, 15s, train loss=2.3465, train acc=23.45%, dev loss=2.3002, dev acc=25.30%
saving, test loss=2.3032, test acc=25.41%
epoch: 32/10000, 15s, train loss=2.3462, train acc=23.29%, dev loss=2.2995, dev acc=25.19%
epoch: 33/10000, 15s, train loss=2.3433, train acc=23.29%, dev loss=2.2973, dev acc=25.37%
saving, test loss=2.3003, test acc=25.46%
epoch: 34/10000, 15s, train loss=2.3449, train acc=23.50%, dev loss=2.2954, dev acc=25.37%
epoch: 35/10000, 15s, train loss=2.3425, train acc=23.26%, dev loss=2.2922, dev acc=25.47%
saving, test loss=2.2949, test acc=25.57%
epoch: 36/10000, 15s, train loss=2.3395, train acc=23.53%, dev loss=2.2916, dev acc=25.52%
saving, test loss=2.2945, test acc=25.64%
epoch: 37/10000, 15s, train loss=2.3426, train acc=23.48%, dev loss=2.2897, dev acc=25.47%
epoch: 38/10000, 15s, train loss=2.3400, train acc=23.52%, dev loss=2.2893, dev acc=25.45%
epoch: 39/10000, 15s, train loss=2.3394, train acc=23.39%, dev loss=2.2876, dev acc=25.51%
epoch: 40/10000, 15s, train loss=2.3399, train acc=23.62%, dev loss=2.2867, dev acc=25.43%
epoch: 41/10000, 15s, train loss=2.3424, train acc=23.60%, dev loss=2.2845, dev acc=25.60%
saving, test loss=2.2869, test acc=26.03%
epoch: 42/10000, 15s, train loss=2.3381, train acc=23.48%, dev loss=2.2834, dev acc=25.62%
saving, test loss=2.2859, test acc=25.80%
epoch: 43/10000, 15s, train loss=2.3375, train acc=23.54%, dev loss=2.2827, dev acc=25.75%
saving, test loss=2.2853, test acc=25.90%
epoch: 44/10000, 15s, train loss=2.3386, train acc=23.56%, dev loss=2.2810, dev acc=25.82%
saving, test loss=2.2834, test acc=25.99%
epoch: 45/10000, 15s, train loss=2.3339, train acc=23.81%, dev loss=2.2820, dev acc=25.79%
epoch: 46/10000, 15s, train loss=2.3363, train acc=23.72%, dev loss=2.2803, dev acc=25.73%
epoch: 47/10000, 15s, train loss=2.3396, train acc=23.65%, dev loss=2.2801, dev acc=25.67%
epoch: 48/10000, 15s, train loss=2.3337, train acc=23.54%, dev loss=2.2785, dev acc=25.66%
epoch: 49/10000, 15s, train loss=2.3382, train acc=23.52%, dev loss=2.2777, dev acc=25.66%
time used=951.2s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='2', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-legal-electra-small-generator', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_car': 0, 'news_entertainment': 1, 'news_world': 2, 'news_travel': 3, 'news_edu': 4, 'news_house': 5, 'news_game': 6, 'news_stock': 7, 'news_agriculture': 8, 'news_story': 9, 'news_military': 10, 'news_culture': 11, 'news_tech': 12, 'news_finance': 13, 'news_sports': 14}
index_labels_dict={0: 'news_car', 1: 'news_entertainment', 2: 'news_world', 3: 'news_travel', 4: 'news_edu', 5: 'news_house', 6: 'news_game', 7: 'news_stock', 8: 'news_agriculture', 9: 'news_story', 10: 'news_military', 11: 'news_culture', 12: 'news_tech', 13: 'news_finance', 14: 'news_sports'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-legal-electra-small-generator were not used when initializing ElectraModel: ['generator_predictions.LayerNorm.bias', 'generator_predictions.dense.bias', 'generator_lm_head.weight', 'generator_lm_head.bias', 'generator_predictions.dense.weight', 'generator_predictions.LayerNorm.weight']
- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(21128, 128, padding_idx=0)
      (position_embeddings): Embedding(512, 128)
      (token_type_embeddings): Embedding(2, 128)
      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (embeddings_project): Linear(in_features=128, out_features=64, bias=True)
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=64, out_features=64, bias=True)
              (key): Linear(in_features=64, out_features=64, bias=True)
              (value): Linear(in_features=64, out_features=64, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=64, out_features=64, bias=True)
              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=64, out_features=256, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=256, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=64, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.embeddings_project.weight False
pretrained_model.embeddings_project.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 5s, train loss=2.6803, train acc=8.81%, dev loss=2.6204, dev acc=11.06%
saving, test loss=2.6228, test acc=11.26%
epoch: 2/10000, 5s, train loss=2.6282, train acc=10.64%, dev loss=2.5928, dev acc=11.24%
saving, test loss=2.5949, test acc=11.24%
epoch: 3/10000, 5s, train loss=2.6069, train acc=11.47%, dev loss=2.5760, dev acc=11.66%
saving, test loss=2.5778, test acc=11.68%
epoch: 4/10000, 5s, train loss=2.5876, train acc=12.55%, dev loss=2.5618, dev acc=12.33%
saving, test loss=2.5638, test acc=12.40%
epoch: 5/10000, 5s, train loss=2.5719, train acc=13.45%, dev loss=2.5494, dev acc=12.97%
saving, test loss=2.5514, test acc=13.19%
epoch: 6/10000, 5s, train loss=2.5603, train acc=14.12%, dev loss=2.5380, dev acc=14.22%
saving, test loss=2.5399, test acc=14.09%
epoch: 7/10000, 5s, train loss=2.5486, train acc=14.73%, dev loss=2.5276, dev acc=14.94%
saving, test loss=2.5293, test acc=14.75%
epoch: 8/10000, 5s, train loss=2.5407, train acc=15.47%, dev loss=2.5178, dev acc=16.06%
saving, test loss=2.5195, test acc=15.88%
epoch: 9/10000, 5s, train loss=2.5311, train acc=15.49%, dev loss=2.5086, dev acc=17.09%
saving, test loss=2.5103, test acc=16.76%
epoch: 10/10000, 5s, train loss=2.5243, train acc=15.88%, dev loss=2.5002, dev acc=17.84%
saving, test loss=2.5018, test acc=17.75%
epoch: 11/10000, 5s, train loss=2.5165, train acc=16.12%, dev loss=2.4922, dev acc=19.02%
saving, test loss=2.4937, test acc=18.46%
epoch: 12/10000, 5s, train loss=2.5123, train acc=16.45%, dev loss=2.4850, dev acc=19.02%
epoch: 13/10000, 5s, train loss=2.5045, train acc=16.86%, dev loss=2.4779, dev acc=19.40%
saving, test loss=2.4792, test acc=19.19%
epoch: 14/10000, 5s, train loss=2.4997, train acc=17.20%, dev loss=2.4712, dev acc=20.28%
saving, test loss=2.4724, test acc=19.73%
epoch: 15/10000, 5s, train loss=2.4958, train acc=17.15%, dev loss=2.4649, dev acc=20.73%
saving, test loss=2.4661, test acc=19.90%
epoch: 16/10000, 5s, train loss=2.4912, train acc=17.38%, dev loss=2.4590, dev acc=20.99%
saving, test loss=2.4601, test acc=20.22%
epoch: 17/10000, 5s, train loss=2.4885, train acc=17.71%, dev loss=2.4534, dev acc=21.25%
saving, test loss=2.4544, test acc=20.61%
epoch: 18/10000, 5s, train loss=2.4831, train acc=17.78%, dev loss=2.4482, dev acc=21.40%
saving, test loss=2.4491, test acc=20.75%
epoch: 19/10000, 5s, train loss=2.4806, train acc=18.02%, dev loss=2.4431, dev acc=21.87%
saving, test loss=2.4438, test acc=20.99%
epoch: 20/10000, 5s, train loss=2.4772, train acc=17.72%, dev loss=2.4384, dev acc=22.04%
saving, test loss=2.4390, test acc=21.30%
epoch: 21/10000, 5s, train loss=2.4754, train acc=17.91%, dev loss=2.4339, dev acc=22.23%
saving, test loss=2.4343, test acc=21.49%
epoch: 22/10000, 5s, train loss=2.4754, train acc=17.93%, dev loss=2.4296, dev acc=22.51%
saving, test loss=2.4299, test acc=21.79%
epoch: 23/10000, 5s, train loss=2.4704, train acc=18.16%, dev loss=2.4256, dev acc=22.41%
epoch: 24/10000, 5s, train loss=2.4657, train acc=18.42%, dev loss=2.4218, dev acc=22.62%
saving, test loss=2.4220, test acc=21.97%
epoch: 25/10000, 5s, train loss=2.4659, train acc=18.23%, dev loss=2.4181, dev acc=22.73%
saving, test loss=2.4181, test acc=22.36%
epoch: 26/10000, 5s, train loss=2.4625, train acc=18.46%, dev loss=2.4148, dev acc=22.81%
saving, test loss=2.4147, test acc=22.41%
epoch: 27/10000, 5s, train loss=2.4645, train acc=18.08%, dev loss=2.4116, dev acc=22.90%
saving, test loss=2.4114, test acc=22.48%
epoch: 28/10000, 5s, train loss=2.4592, train acc=18.39%, dev loss=2.4084, dev acc=23.05%
saving, test loss=2.4081, test acc=22.58%
epoch: 29/10000, 5s, train loss=2.4585, train acc=18.38%, dev loss=2.4054, dev acc=23.14%
saving, test loss=2.4050, test acc=22.68%
epoch: 30/10000, 5s, train loss=2.4586, train acc=18.54%, dev loss=2.4026, dev acc=23.20%
saving, test loss=2.4020, test acc=22.73%
epoch: 31/10000, 5s, train loss=2.4564, train acc=18.74%, dev loss=2.3998, dev acc=23.24%
saving, test loss=2.3991, test acc=23.01%
epoch: 32/10000, 5s, train loss=2.4522, train acc=18.64%, dev loss=2.3972, dev acc=23.14%
epoch: 33/10000, 5s, train loss=2.4540, train acc=18.35%, dev loss=2.3948, dev acc=23.39%
saving, test loss=2.3939, test acc=22.94%
epoch: 34/10000, 5s, train loss=2.4532, train acc=18.36%, dev loss=2.3924, dev acc=23.31%
epoch: 35/10000, 5s, train loss=2.4514, train acc=18.84%, dev loss=2.3901, dev acc=23.41%
saving, test loss=2.3891, test acc=23.13%
epoch: 36/10000, 5s, train loss=2.4515, train acc=18.61%, dev loss=2.3880, dev acc=23.37%
epoch: 37/10000, 5s, train loss=2.4497, train acc=18.68%, dev loss=2.3860, dev acc=23.56%
saving, test loss=2.3846, test acc=23.19%
epoch: 38/10000, 5s, train loss=2.4513, train acc=18.67%, dev loss=2.3843, dev acc=23.58%
saving, test loss=2.3828, test acc=23.11%
epoch: 39/10000, 5s, train loss=2.4516, train acc=18.54%, dev loss=2.3826, dev acc=23.67%
saving, test loss=2.3810, test acc=23.34%
epoch: 40/10000, 5s, train loss=2.4446, train acc=18.70%, dev loss=2.3811, dev acc=23.67%
epoch: 41/10000, 5s, train loss=2.4460, train acc=18.90%, dev loss=2.3790, dev acc=23.88%
saving, test loss=2.3774, test acc=23.48%
epoch: 42/10000, 5s, train loss=2.4444, train acc=18.97%, dev loss=2.3773, dev acc=24.16%
saving, test loss=2.3756, test acc=23.70%
epoch: 43/10000, 5s, train loss=2.4472, train acc=18.72%, dev loss=2.3757, dev acc=24.16%
epoch: 44/10000, 5s, train loss=2.4465, train acc=18.84%, dev loss=2.3741, dev acc=24.25%
saving, test loss=2.3722, test acc=23.74%
epoch: 45/10000, 5s, train loss=2.4423, train acc=18.94%, dev loss=2.3727, dev acc=24.29%
saving, test loss=2.3705, test acc=23.93%
epoch: 46/10000, 5s, train loss=2.4430, train acc=18.83%, dev loss=2.3716, dev acc=24.14%
epoch: 47/10000, 5s, train loss=2.4454, train acc=18.83%, dev loss=2.3701, dev acc=24.25%
epoch: 48/10000, 5s, train loss=2.4456, train acc=18.96%, dev loss=2.3690, dev acc=24.36%
saving, test loss=2.3668, test acc=23.93%
epoch: 49/10000, 5s, train loss=2.4415, train acc=19.13%, dev loss=2.3677, dev acc=24.34%
epoch: 50/10000, 5s, train loss=2.4419, train acc=18.82%, dev loss=2.3665, dev acc=24.25%
epoch: 51/10000, 5s, train loss=2.4423, train acc=19.01%, dev loss=2.3656, dev acc=24.10%
epoch: 52/10000, 5s, train loss=2.4406, train acc=19.17%, dev loss=2.3643, dev acc=24.36%
epoch: 53/10000, 5s, train loss=2.4413, train acc=18.97%, dev loss=2.3632, dev acc=24.63%
saving, test loss=2.3605, test acc=24.00%
epoch: 54/10000, 5s, train loss=2.4358, train acc=19.16%, dev loss=2.3622, dev acc=24.40%
epoch: 55/10000, 5s, train loss=2.4400, train acc=18.98%, dev loss=2.3611, dev acc=24.53%
epoch: 56/10000, 5s, train loss=2.4388, train acc=19.17%, dev loss=2.3601, dev acc=24.48%
epoch: 57/10000, 5s, train loss=2.4396, train acc=19.12%, dev loss=2.3595, dev acc=24.66%
saving, test loss=2.3566, test acc=24.25%
epoch: 58/10000, 5s, train loss=2.4388, train acc=18.88%, dev loss=2.3584, dev acc=24.64%
epoch: 59/10000, 5s, train loss=2.4424, train acc=18.93%, dev loss=2.3578, dev acc=24.61%
epoch: 60/10000, 5s, train loss=2.4381, train acc=19.05%, dev loss=2.3571, dev acc=24.48%
epoch: 61/10000, 5s, train loss=2.4394, train acc=19.26%, dev loss=2.3564, dev acc=24.40%
epoch: 62/10000, 5s, train loss=2.4383, train acc=18.72%, dev loss=2.3554, dev acc=24.63%
time used=404.0s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='2', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-macbert-base', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_story': 0, 'news_sports': 1, 'news_game': 2, 'news_house': 3, 'news_car': 4, 'news_entertainment': 5, 'news_military': 6, 'news_culture': 7, 'news_travel': 8, 'news_edu': 9, 'news_finance': 10, 'news_world': 11, 'news_agriculture': 12, 'news_stock': 13, 'news_tech': 14}
index_labels_dict={0: 'news_story', 1: 'news_sports', 2: 'news_game', 3: 'news_house', 4: 'news_car', 5: 'news_entertainment', 6: 'news_military', 7: 'news_culture', 8: 'news_travel', 9: 'news_edu', 10: 'news_finance', 11: 'news_world', 12: 'news_agriculture', 13: 'news_stock', 14: 'news_tech'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-macbert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 70s, train loss=2.3563, train acc=28.15%, dev loss=1.9816, dev acc=46.83%
saving, test loss=1.9826, test acc=46.63%
epoch: 2/10000, 70s, train loss=1.8474, train acc=47.43%, dev loss=1.6447, dev acc=52.04%
saving, test loss=1.6457, test acc=52.19%
epoch: 3/10000, 70s, train loss=1.6216, train acc=50.90%, dev loss=1.4926, dev acc=53.92%
saving, test loss=1.4931, test acc=54.10%
epoch: 4/10000, 70s, train loss=1.5209, train acc=52.28%, dev loss=1.4186, dev acc=54.63%
saving, test loss=1.4192, test acc=54.58%
epoch: 5/10000, 71s, train loss=1.4696, train acc=53.00%, dev loss=1.3778, dev acc=54.52%
epoch: 6/10000, 71s, train loss=1.4363, train acc=53.42%, dev loss=1.3511, dev acc=54.91%
saving, test loss=1.3524, test acc=55.09%
epoch: 7/10000, 71s, train loss=1.4187, train acc=53.44%, dev loss=1.3334, dev acc=54.93%
saving, test loss=1.3348, test acc=55.41%
epoch: 8/10000, 71s, train loss=1.4033, train acc=53.82%, dev loss=1.3205, dev acc=55.00%
saving, test loss=1.3224, test acc=55.40%
epoch: 9/10000, 71s, train loss=1.3949, train acc=53.65%, dev loss=1.3110, dev acc=55.06%
saving, test loss=1.3134, test acc=55.54%
epoch: 10/10000, 71s, train loss=1.3832, train acc=54.15%, dev loss=1.3034, dev acc=55.15%
saving, test loss=1.3063, test acc=55.52%
epoch: 11/10000, 71s, train loss=1.3778, train acc=54.00%, dev loss=1.2972, dev acc=55.30%
saving, test loss=1.3005, test acc=55.66%
epoch: 12/10000, 71s, train loss=1.3721, train acc=54.26%, dev loss=1.2920, dev acc=55.51%
saving, test loss=1.2955, test acc=55.73%
epoch: 13/10000, 71s, train loss=1.3679, train acc=54.14%, dev loss=1.2879, dev acc=55.34%
epoch: 14/10000, 71s, train loss=1.3636, train acc=54.22%, dev loss=1.2845, dev acc=55.34%
epoch: 15/10000, 71s, train loss=1.3629, train acc=54.28%, dev loss=1.2812, dev acc=55.34%
epoch: 16/10000, 71s, train loss=1.3599, train acc=54.47%, dev loss=1.2785, dev acc=55.53%
saving, test loss=1.2821, test acc=55.89%
epoch: 17/10000, 71s, train loss=1.3575, train acc=54.38%, dev loss=1.2759, dev acc=55.64%
saving, test loss=1.2802, test acc=55.84%
epoch: 18/10000, 71s, train loss=1.3533, train acc=54.57%, dev loss=1.2742, dev acc=55.55%
epoch: 19/10000, 71s, train loss=1.3531, train acc=54.42%, dev loss=1.2724, dev acc=55.43%
epoch: 20/10000, 71s, train loss=1.3524, train acc=54.34%, dev loss=1.2708, dev acc=55.73%
saving, test loss=1.2756, test acc=55.85%
epoch: 21/10000, 71s, train loss=1.3525, train acc=54.46%, dev loss=1.2697, dev acc=55.57%
epoch: 22/10000, 71s, train loss=1.3533, train acc=54.42%, dev loss=1.2686, dev acc=55.68%
epoch: 23/10000, 71s, train loss=1.3488, train acc=54.42%, dev loss=1.2677, dev acc=55.66%
epoch: 24/10000, 71s, train loss=1.3453, train acc=54.73%, dev loss=1.2665, dev acc=55.53%
epoch: 25/10000, 71s, train loss=1.3468, train acc=54.56%, dev loss=1.2659, dev acc=55.57%
time used=2187.2s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='2', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-macbert-large', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_world': 0, 'news_stock': 1, 'news_culture': 2, 'news_tech': 3, 'news_story': 4, 'news_car': 5, 'news_agriculture': 6, 'news_game': 7, 'news_travel': 8, 'news_entertainment': 9, 'news_finance': 10, 'news_sports': 11, 'news_house': 12, 'news_military': 13, 'news_edu': 14}
index_labels_dict={0: 'news_world', 1: 'news_stock', 2: 'news_culture', 3: 'news_tech', 4: 'news_story', 5: 'news_car', 6: 'news_agriculture', 7: 'news_game', 8: 'news_travel', 9: 'news_entertainment', 10: 'news_finance', 11: 'news_sports', 12: 'news_house', 13: 'news_military', 14: 'news_edu'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-macbert-large were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (12): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (13): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (14): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (15): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (16): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (17): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (18): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (19): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (20): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (21): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (22): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (23): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=1024, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.attention.self.query.weight False
pretrained_model.encoder.layer.12.attention.self.query.bias False
pretrained_model.encoder.layer.12.attention.self.key.weight False
pretrained_model.encoder.layer.12.attention.self.key.bias False
pretrained_model.encoder.layer.12.attention.self.value.weight False
pretrained_model.encoder.layer.12.attention.self.value.bias False
pretrained_model.encoder.layer.12.attention.output.dense.weight False
pretrained_model.encoder.layer.12.attention.output.dense.bias False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.intermediate.dense.weight False
pretrained_model.encoder.layer.12.intermediate.dense.bias False
pretrained_model.encoder.layer.12.output.dense.weight False
pretrained_model.encoder.layer.12.output.dense.bias False
pretrained_model.encoder.layer.12.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.attention.self.query.weight False
pretrained_model.encoder.layer.13.attention.self.query.bias False
pretrained_model.encoder.layer.13.attention.self.key.weight False
pretrained_model.encoder.layer.13.attention.self.key.bias False
pretrained_model.encoder.layer.13.attention.self.value.weight False
pretrained_model.encoder.layer.13.attention.self.value.bias False
pretrained_model.encoder.layer.13.attention.output.dense.weight False
pretrained_model.encoder.layer.13.attention.output.dense.bias False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.intermediate.dense.weight False
pretrained_model.encoder.layer.13.intermediate.dense.bias False
pretrained_model.encoder.layer.13.output.dense.weight False
pretrained_model.encoder.layer.13.output.dense.bias False
pretrained_model.encoder.layer.13.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.attention.self.query.weight False
pretrained_model.encoder.layer.14.attention.self.query.bias False
pretrained_model.encoder.layer.14.attention.self.key.weight False
pretrained_model.encoder.layer.14.attention.self.key.bias False
pretrained_model.encoder.layer.14.attention.self.value.weight False
pretrained_model.encoder.layer.14.attention.self.value.bias False
pretrained_model.encoder.layer.14.attention.output.dense.weight False
pretrained_model.encoder.layer.14.attention.output.dense.bias False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.intermediate.dense.weight False
pretrained_model.encoder.layer.14.intermediate.dense.bias False
pretrained_model.encoder.layer.14.output.dense.weight False
pretrained_model.encoder.layer.14.output.dense.bias False
pretrained_model.encoder.layer.14.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.attention.self.query.weight False
pretrained_model.encoder.layer.15.attention.self.query.bias False
pretrained_model.encoder.layer.15.attention.self.key.weight False
pretrained_model.encoder.layer.15.attention.self.key.bias False
pretrained_model.encoder.layer.15.attention.self.value.weight False
pretrained_model.encoder.layer.15.attention.self.value.bias False
pretrained_model.encoder.layer.15.attention.output.dense.weight False
pretrained_model.encoder.layer.15.attention.output.dense.bias False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.intermediate.dense.weight False
pretrained_model.encoder.layer.15.intermediate.dense.bias False
pretrained_model.encoder.layer.15.output.dense.weight False
pretrained_model.encoder.layer.15.output.dense.bias False
pretrained_model.encoder.layer.15.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.attention.self.query.weight False
pretrained_model.encoder.layer.16.attention.self.query.bias False
pretrained_model.encoder.layer.16.attention.self.key.weight False
pretrained_model.encoder.layer.16.attention.self.key.bias False
pretrained_model.encoder.layer.16.attention.self.value.weight False
pretrained_model.encoder.layer.16.attention.self.value.bias False
pretrained_model.encoder.layer.16.attention.output.dense.weight False
pretrained_model.encoder.layer.16.attention.output.dense.bias False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.intermediate.dense.weight False
pretrained_model.encoder.layer.16.intermediate.dense.bias False
pretrained_model.encoder.layer.16.output.dense.weight False
pretrained_model.encoder.layer.16.output.dense.bias False
pretrained_model.encoder.layer.16.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.attention.self.query.weight False
pretrained_model.encoder.layer.17.attention.self.query.bias False
pretrained_model.encoder.layer.17.attention.self.key.weight False
pretrained_model.encoder.layer.17.attention.self.key.bias False
pretrained_model.encoder.layer.17.attention.self.value.weight False
pretrained_model.encoder.layer.17.attention.self.value.bias False
pretrained_model.encoder.layer.17.attention.output.dense.weight False
pretrained_model.encoder.layer.17.attention.output.dense.bias False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.intermediate.dense.weight False
pretrained_model.encoder.layer.17.intermediate.dense.bias False
pretrained_model.encoder.layer.17.output.dense.weight False
pretrained_model.encoder.layer.17.output.dense.bias False
pretrained_model.encoder.layer.17.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.attention.self.query.weight False
pretrained_model.encoder.layer.18.attention.self.query.bias False
pretrained_model.encoder.layer.18.attention.self.key.weight False
pretrained_model.encoder.layer.18.attention.self.key.bias False
pretrained_model.encoder.layer.18.attention.self.value.weight False
pretrained_model.encoder.layer.18.attention.self.value.bias False
pretrained_model.encoder.layer.18.attention.output.dense.weight False
pretrained_model.encoder.layer.18.attention.output.dense.bias False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.intermediate.dense.weight False
pretrained_model.encoder.layer.18.intermediate.dense.bias False
pretrained_model.encoder.layer.18.output.dense.weight False
pretrained_model.encoder.layer.18.output.dense.bias False
pretrained_model.encoder.layer.18.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.attention.self.query.weight False
pretrained_model.encoder.layer.19.attention.self.query.bias False
pretrained_model.encoder.layer.19.attention.self.key.weight False
pretrained_model.encoder.layer.19.attention.self.key.bias False
pretrained_model.encoder.layer.19.attention.self.value.weight False
pretrained_model.encoder.layer.19.attention.self.value.bias False
pretrained_model.encoder.layer.19.attention.output.dense.weight False
pretrained_model.encoder.layer.19.attention.output.dense.bias False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.intermediate.dense.weight False
pretrained_model.encoder.layer.19.intermediate.dense.bias False
pretrained_model.encoder.layer.19.output.dense.weight False
pretrained_model.encoder.layer.19.output.dense.bias False
pretrained_model.encoder.layer.19.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.attention.self.query.weight False
pretrained_model.encoder.layer.20.attention.self.query.bias False
pretrained_model.encoder.layer.20.attention.self.key.weight False
pretrained_model.encoder.layer.20.attention.self.key.bias False
pretrained_model.encoder.layer.20.attention.self.value.weight False
pretrained_model.encoder.layer.20.attention.self.value.bias False
pretrained_model.encoder.layer.20.attention.output.dense.weight False
pretrained_model.encoder.layer.20.attention.output.dense.bias False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.intermediate.dense.weight False
pretrained_model.encoder.layer.20.intermediate.dense.bias False
pretrained_model.encoder.layer.20.output.dense.weight False
pretrained_model.encoder.layer.20.output.dense.bias False
pretrained_model.encoder.layer.20.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.attention.self.query.weight False
pretrained_model.encoder.layer.21.attention.self.query.bias False
pretrained_model.encoder.layer.21.attention.self.key.weight False
pretrained_model.encoder.layer.21.attention.self.key.bias False
pretrained_model.encoder.layer.21.attention.self.value.weight False
pretrained_model.encoder.layer.21.attention.self.value.bias False
pretrained_model.encoder.layer.21.attention.output.dense.weight False
pretrained_model.encoder.layer.21.attention.output.dense.bias False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.intermediate.dense.weight False
pretrained_model.encoder.layer.21.intermediate.dense.bias False
pretrained_model.encoder.layer.21.output.dense.weight False
pretrained_model.encoder.layer.21.output.dense.bias False
pretrained_model.encoder.layer.21.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.attention.self.query.weight False
pretrained_model.encoder.layer.22.attention.self.query.bias False
pretrained_model.encoder.layer.22.attention.self.key.weight False
pretrained_model.encoder.layer.22.attention.self.key.bias False
pretrained_model.encoder.layer.22.attention.self.value.weight False
pretrained_model.encoder.layer.22.attention.self.value.bias False
pretrained_model.encoder.layer.22.attention.output.dense.weight False
pretrained_model.encoder.layer.22.attention.output.dense.bias False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.intermediate.dense.weight False
pretrained_model.encoder.layer.22.intermediate.dense.bias False
pretrained_model.encoder.layer.22.output.dense.weight False
pretrained_model.encoder.layer.22.output.dense.bias False
pretrained_model.encoder.layer.22.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.attention.self.query.weight False
pretrained_model.encoder.layer.23.attention.self.query.bias False
pretrained_model.encoder.layer.23.attention.self.key.weight False
pretrained_model.encoder.layer.23.attention.self.key.bias False
pretrained_model.encoder.layer.23.attention.self.value.weight False
pretrained_model.encoder.layer.23.attention.self.value.bias False
pretrained_model.encoder.layer.23.attention.output.dense.weight False
pretrained_model.encoder.layer.23.attention.output.dense.bias False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.intermediate.dense.weight False
pretrained_model.encoder.layer.23.intermediate.dense.bias False
pretrained_model.encoder.layer.23.output.dense.weight False
pretrained_model.encoder.layer.23.output.dense.bias False
pretrained_model.encoder.layer.23.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 222s, train loss=2.2752, train acc=30.52%, dev loss=1.8337, dev acc=50.32%
saving, test loss=1.8314, test acc=51.13%
epoch: 2/10000, 224s, train loss=1.7032, train acc=50.40%, dev loss=1.5286, dev acc=53.04%
saving, test loss=1.5219, test acc=53.61%
epoch: 3/10000, 224s, train loss=1.5165, train acc=52.59%, dev loss=1.4259, dev acc=53.67%
saving, test loss=1.4173, test acc=54.52%
epoch: 4/10000, 224s, train loss=1.4449, train acc=53.33%, dev loss=1.3806, dev acc=54.03%
saving, test loss=1.3716, test acc=54.64%
epoch: 5/10000, 224s, train loss=1.4148, train acc=53.70%, dev loss=1.3558, dev acc=54.55%
saving, test loss=1.3465, test acc=54.95%
epoch: 6/10000, 225s, train loss=1.3937, train acc=54.09%, dev loss=1.3397, dev acc=54.72%
saving, test loss=1.3309, test acc=55.01%
epoch: 7/10000, 225s, train loss=1.3830, train acc=53.80%, dev loss=1.3284, dev acc=55.04%
saving, test loss=1.3197, test acc=55.15%
epoch: 8/10000, 225s, train loss=1.3688, train acc=54.29%, dev loss=1.3201, dev acc=55.34%
saving, test loss=1.3122, test acc=55.34%
epoch: 9/10000, 225s, train loss=1.3627, train acc=54.32%, dev loss=1.3131, dev acc=55.57%
saving, test loss=1.3066, test acc=55.37%
epoch: 10/10000, 225s, train loss=1.3571, train acc=54.52%, dev loss=1.3086, dev acc=55.53%
epoch: 11/10000, 225s, train loss=1.3555, train acc=54.47%, dev loss=1.3045, dev acc=55.64%
saving, test loss=1.2980, test acc=55.50%
epoch: 12/10000, 225s, train loss=1.3534, train acc=54.51%, dev loss=1.3005, dev acc=55.79%
saving, test loss=1.2949, test acc=55.50%
epoch: 13/10000, 225s, train loss=1.3473, train acc=54.63%, dev loss=1.2979, dev acc=55.83%
saving, test loss=1.2921, test acc=55.45%
epoch: 14/10000, 267s, train loss=1.3437, train acc=54.83%, dev loss=1.2953, dev acc=55.94%
saving, test loss=1.2890, test acc=55.53%
epoch: 15/10000, 260s, train loss=1.3417, train acc=55.00%, dev loss=1.2934, dev acc=55.92%
epoch: 16/10000, 223s, train loss=1.3375, train acc=54.98%, dev loss=1.2912, dev acc=55.85%
epoch: 17/10000, 247s, train loss=1.3405, train acc=54.89%, dev loss=1.2892, dev acc=55.94%
epoch: 18/10000, 291s, train loss=1.3388, train acc=54.85%, dev loss=1.2881, dev acc=55.85%
epoch: 19/10000, 318s, train loss=1.3351, train acc=54.69%, dev loss=1.2859, dev acc=55.96%
saving, test loss=1.2829, test acc=55.57%
epoch: 20/10000, 306s, train loss=1.3353, train acc=54.95%, dev loss=1.2846, dev acc=55.98%
saving, test loss=1.2814, test acc=55.55%
epoch: 21/10000, 305s, train loss=1.3313, train acc=54.99%, dev loss=1.2838, dev acc=55.96%
epoch: 22/10000, 304s, train loss=1.3339, train acc=55.07%, dev loss=1.2824, dev acc=56.17%
saving, test loss=1.2799, test acc=55.65%
epoch: 23/10000, 304s, train loss=1.3320, train acc=54.87%, dev loss=1.2820, dev acc=56.17%
epoch: 24/10000, 304s, train loss=1.3339, train acc=54.89%, dev loss=1.2814, dev acc=56.07%
epoch: 25/10000, 289s, train loss=1.3306, train acc=55.03%, dev loss=1.2807, dev acc=56.17%
epoch: 26/10000, 291s, train loss=1.3321, train acc=55.09%, dev loss=1.2799, dev acc=56.22%
saving, test loss=1.2775, test acc=55.65%
epoch: 27/10000, 306s, train loss=1.3314, train acc=54.89%, dev loss=1.2797, dev acc=55.96%
epoch: 28/10000, 308s, train loss=1.3289, train acc=55.02%, dev loss=1.2789, dev acc=56.03%
epoch: 29/10000, 303s, train loss=1.3270, train acc=55.29%, dev loss=1.2785, dev acc=56.03%
epoch: 30/10000, 302s, train loss=1.3331, train acc=54.96%, dev loss=1.2781, dev acc=56.11%
epoch: 31/10000, 223s, train loss=1.3278, train acc=55.07%, dev loss=1.2772, dev acc=56.30%
saving, test loss=1.2756, test acc=55.65%
epoch: 32/10000, 223s, train loss=1.3273, train acc=55.07%, dev loss=1.2772, dev acc=56.33%
saving, test loss=1.2749, test acc=55.77%
epoch: 33/10000, 224s, train loss=1.3233, train acc=55.22%, dev loss=1.2765, dev acc=56.33%
epoch: 34/10000, 224s, train loss=1.3286, train acc=55.07%, dev loss=1.2765, dev acc=56.17%
epoch: 35/10000, 223s, train loss=1.3268, train acc=55.11%, dev loss=1.2764, dev acc=55.83%
epoch: 36/10000, 223s, train loss=1.3313, train acc=54.82%, dev loss=1.2764, dev acc=56.03%
epoch: 37/10000, 223s, train loss=1.3275, train acc=54.93%, dev loss=1.2761, dev acc=56.05%
time used=11393.3s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='2', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-pert-base', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_travel': 0, 'news_tech': 1, 'news_agriculture': 2, 'news_finance': 3, 'news_military': 4, 'news_edu': 5, 'news_story': 6, 'news_house': 7, 'news_game': 8, 'news_stock': 9, 'news_car': 10, 'news_entertainment': 11, 'news_world': 12, 'news_sports': 13, 'news_culture': 14}
index_labels_dict={0: 'news_travel', 1: 'news_tech', 2: 'news_agriculture', 3: 'news_finance', 4: 'news_military', 5: 'news_edu', 6: 'news_story', 7: 'news_house', 8: 'news_game', 9: 'news_stock', 10: 'news_car', 11: 'news_entertainment', 12: 'news_world', 13: 'news_sports', 14: 'news_culture'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-pert-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 70s, train loss=2.6621, train acc=10.48%, dev loss=2.6100, dev acc=11.30%
saving, test loss=2.6118, test acc=10.92%
epoch: 2/10000, 70s, train loss=2.5851, train acc=16.38%, dev loss=2.5531, dev acc=16.19%
saving, test loss=2.5558, test acc=16.17%
epoch: 3/10000, 70s, train loss=2.5391, train acc=15.85%, dev loss=2.5119, dev acc=15.50%
epoch: 4/10000, 70s, train loss=2.5018, train acc=17.89%, dev loss=2.4754, dev acc=20.00%
saving, test loss=2.4787, test acc=19.75%
epoch: 5/10000, 70s, train loss=2.4685, train acc=23.18%, dev loss=2.4411, dev acc=25.54%
saving, test loss=2.4446, test acc=25.22%
epoch: 6/10000, 70s, train loss=2.4365, train acc=27.27%, dev loss=2.4084, dev acc=29.16%
saving, test loss=2.4120, test acc=28.88%
epoch: 7/10000, 71s, train loss=2.4060, train acc=30.63%, dev loss=2.3769, dev acc=32.27%
saving, test loss=2.3805, test acc=32.01%
epoch: 8/10000, 71s, train loss=2.3765, train acc=33.17%, dev loss=2.3464, dev acc=34.90%
saving, test loss=2.3501, test acc=34.60%
epoch: 9/10000, 71s, train loss=2.3477, train acc=35.20%, dev loss=2.3168, dev acc=36.81%
saving, test loss=2.3206, test acc=36.46%
epoch: 10/10000, 71s, train loss=2.3198, train acc=36.75%, dev loss=2.2880, dev acc=38.12%
saving, test loss=2.2918, test acc=38.30%
epoch: 11/10000, 71s, train loss=2.2934, train acc=38.33%, dev loss=2.2601, dev acc=39.71%
saving, test loss=2.2640, test acc=40.02%
epoch: 12/10000, 71s, train loss=2.2675, train acc=39.60%, dev loss=2.2330, dev acc=41.25%
saving, test loss=2.2369, test acc=41.59%
epoch: 13/10000, 71s, train loss=2.2425, train acc=40.76%, dev loss=2.2068, dev acc=42.05%
saving, test loss=2.2106, test acc=42.75%
epoch: 14/10000, 71s, train loss=2.2176, train acc=41.52%, dev loss=2.1811, dev acc=42.86%
saving, test loss=2.1851, test acc=43.46%
epoch: 15/10000, 71s, train loss=2.1943, train acc=42.32%, dev loss=2.1564, dev acc=43.76%
saving, test loss=2.1603, test acc=44.34%
epoch: 16/10000, 71s, train loss=2.1709, train acc=43.02%, dev loss=2.1323, dev acc=44.88%
saving, test loss=2.1362, test acc=45.18%
epoch: 17/10000, 71s, train loss=2.1481, train acc=43.75%, dev loss=2.1089, dev acc=45.46%
saving, test loss=2.1129, test acc=45.90%
epoch: 18/10000, 71s, train loss=2.1269, train acc=43.99%, dev loss=2.0863, dev acc=45.91%
saving, test loss=2.0903, test acc=46.35%
epoch: 19/10000, 71s, train loss=2.1069, train acc=44.47%, dev loss=2.0643, dev acc=46.14%
saving, test loss=2.0683, test acc=46.76%
epoch: 20/10000, 71s, train loss=2.0849, train acc=45.25%, dev loss=2.0430, dev acc=46.53%
saving, test loss=2.0470, test acc=47.31%
epoch: 21/10000, 71s, train loss=2.0661, train acc=45.49%, dev loss=2.0223, dev acc=47.15%
saving, test loss=2.0263, test acc=47.73%
epoch: 22/10000, 71s, train loss=2.0488, train acc=45.60%, dev loss=2.0023, dev acc=47.45%
saving, test loss=2.0064, test acc=48.22%
epoch: 23/10000, 71s, train loss=2.0276, train acc=46.12%, dev loss=1.9829, dev acc=47.86%
saving, test loss=1.9870, test acc=48.58%
epoch: 24/10000, 71s, train loss=2.0103, train acc=46.63%, dev loss=1.9642, dev acc=48.01%
saving, test loss=1.9682, test acc=48.61%
epoch: 25/10000, 71s, train loss=1.9935, train acc=46.72%, dev loss=1.9459, dev acc=48.43%
saving, test loss=1.9499, test acc=49.19%
epoch: 26/10000, 71s, train loss=1.9786, train acc=46.79%, dev loss=1.9283, dev acc=48.61%
saving, test loss=1.9323, test acc=49.45%
epoch: 27/10000, 71s, train loss=1.9623, train acc=47.07%, dev loss=1.9114, dev acc=48.88%
saving, test loss=1.9154, test acc=49.67%
epoch: 28/10000, 71s, train loss=1.9454, train acc=47.60%, dev loss=1.8949, dev acc=49.06%
saving, test loss=1.8989, test acc=49.80%
epoch: 29/10000, 71s, train loss=1.9314, train acc=47.47%, dev loss=1.8790, dev acc=49.34%
saving, test loss=1.8830, test acc=50.08%
epoch: 30/10000, 71s, train loss=1.9168, train acc=47.53%, dev loss=1.8635, dev acc=49.63%
saving, test loss=1.8675, test acc=50.36%
epoch: 31/10000, 71s, train loss=1.9016, train acc=48.11%, dev loss=1.8486, dev acc=49.83%
saving, test loss=1.8526, test acc=50.43%
epoch: 32/10000, 71s, train loss=1.8888, train acc=47.97%, dev loss=1.8341, dev acc=50.07%
saving, test loss=1.8381, test acc=50.59%
epoch: 33/10000, 71s, train loss=1.8747, train acc=48.31%, dev loss=1.8201, dev acc=50.04%
epoch: 34/10000, 71s, train loss=1.8618, train acc=48.34%, dev loss=1.8065, dev acc=50.17%
saving, test loss=1.8105, test acc=50.65%
epoch: 35/10000, 71s, train loss=1.8502, train acc=48.45%, dev loss=1.7934, dev acc=50.39%
saving, test loss=1.7974, test acc=50.80%
epoch: 36/10000, 71s, train loss=1.8383, train acc=48.66%, dev loss=1.7808, dev acc=50.43%
saving, test loss=1.7847, test acc=50.99%
epoch: 37/10000, 71s, train loss=1.8272, train acc=48.88%, dev loss=1.7685, dev acc=50.45%
saving, test loss=1.7724, test acc=51.10%
epoch: 38/10000, 71s, train loss=1.8149, train acc=48.89%, dev loss=1.7566, dev acc=50.56%
saving, test loss=1.7605, test acc=51.26%
epoch: 39/10000, 71s, train loss=1.8040, train acc=49.09%, dev loss=1.7451, dev acc=50.62%
saving, test loss=1.7490, test acc=51.39%
epoch: 40/10000, 71s, train loss=1.7910, train acc=49.20%, dev loss=1.7340, dev acc=50.75%
saving, test loss=1.7378, test acc=51.43%
epoch: 41/10000, 71s, train loss=1.7844, train acc=49.13%, dev loss=1.7232, dev acc=50.90%
saving, test loss=1.7270, test acc=51.57%
epoch: 42/10000, 71s, train loss=1.7733, train acc=49.18%, dev loss=1.7128, dev acc=50.90%
epoch: 43/10000, 71s, train loss=1.7637, train acc=49.55%, dev loss=1.7027, dev acc=50.97%
saving, test loss=1.7064, test acc=51.84%
epoch: 44/10000, 71s, train loss=1.7543, train acc=49.59%, dev loss=1.6928, dev acc=51.11%
saving, test loss=1.6966, test acc=52.05%
epoch: 45/10000, 71s, train loss=1.7455, train acc=49.65%, dev loss=1.6834, dev acc=51.14%
saving, test loss=1.6872, test acc=52.09%
epoch: 46/10000, 71s, train loss=1.7386, train acc=49.44%, dev loss=1.6743, dev acc=51.31%
saving, test loss=1.6780, test acc=52.15%
epoch: 47/10000, 71s, train loss=1.7303, train acc=49.80%, dev loss=1.6654, dev acc=51.41%
saving, test loss=1.6691, test acc=52.16%
epoch: 48/10000, 71s, train loss=1.7215, train acc=49.53%, dev loss=1.6568, dev acc=51.44%
saving, test loss=1.6606, test acc=52.16%
epoch: 49/10000, 71s, train loss=1.7155, train acc=49.98%, dev loss=1.6486, dev acc=51.63%
saving, test loss=1.6522, test acc=52.21%
epoch: 50/10000, 71s, train loss=1.7081, train acc=50.12%, dev loss=1.6405, dev acc=51.67%
saving, test loss=1.6441, test acc=52.23%
epoch: 51/10000, 71s, train loss=1.6988, train acc=49.92%, dev loss=1.6327, dev acc=51.61%
epoch: 52/10000, 71s, train loss=1.6939, train acc=50.30%, dev loss=1.6252, dev acc=51.78%
saving, test loss=1.6288, test acc=52.38%
epoch: 53/10000, 71s, train loss=1.6857, train acc=49.92%, dev loss=1.6178, dev acc=51.72%
epoch: 54/10000, 71s, train loss=1.6776, train acc=50.44%, dev loss=1.6108, dev acc=51.84%
saving, test loss=1.6143, test acc=52.46%
epoch: 55/10000, 71s, train loss=1.6745, train acc=49.93%, dev loss=1.6038, dev acc=51.91%
saving, test loss=1.6074, test acc=52.55%
epoch: 56/10000, 71s, train loss=1.6664, train acc=50.23%, dev loss=1.5972, dev acc=52.08%
saving, test loss=1.6007, test acc=52.49%
epoch: 57/10000, 71s, train loss=1.6617, train acc=50.49%, dev loss=1.5908, dev acc=52.08%
epoch: 58/10000, 71s, train loss=1.6526, train acc=50.32%, dev loss=1.5844, dev acc=52.19%
saving, test loss=1.5880, test acc=52.54%
epoch: 59/10000, 71s, train loss=1.6495, train acc=50.52%, dev loss=1.5783, dev acc=52.12%
epoch: 60/10000, 71s, train loss=1.6430, train acc=50.60%, dev loss=1.5726, dev acc=52.14%
epoch: 61/10000, 71s, train loss=1.6384, train acc=50.48%, dev loss=1.5667, dev acc=52.34%
saving, test loss=1.5702, test acc=52.73%
epoch: 62/10000, 71s, train loss=1.6337, train acc=50.42%, dev loss=1.5612, dev acc=52.23%
epoch: 63/10000, 71s, train loss=1.6268, train acc=50.57%, dev loss=1.5557, dev acc=52.36%
saving, test loss=1.5591, test acc=52.89%
epoch: 64/10000, 71s, train loss=1.6237, train acc=50.39%, dev loss=1.5505, dev acc=52.46%
saving, test loss=1.5538, test acc=52.90%
epoch: 65/10000, 71s, train loss=1.6202, train acc=50.60%, dev loss=1.5454, dev acc=52.51%
saving, test loss=1.5487, test acc=52.90%
epoch: 66/10000, 71s, train loss=1.6144, train acc=50.86%, dev loss=1.5405, dev acc=52.55%
saving, test loss=1.5439, test acc=52.94%
epoch: 67/10000, 71s, train loss=1.6111, train acc=50.91%, dev loss=1.5357, dev acc=52.59%
saving, test loss=1.5390, test acc=53.04%
epoch: 68/10000, 71s, train loss=1.6061, train acc=50.94%, dev loss=1.5310, dev acc=52.64%
saving, test loss=1.5343, test acc=53.10%
epoch: 69/10000, 71s, train loss=1.6004, train acc=50.82%, dev loss=1.5264, dev acc=52.59%
epoch: 70/10000, 71s, train loss=1.5972, train acc=50.86%, dev loss=1.5220, dev acc=52.64%
epoch: 71/10000, 71s, train loss=1.5916, train acc=51.11%, dev loss=1.5178, dev acc=52.62%
epoch: 72/10000, 71s, train loss=1.5882, train acc=51.10%, dev loss=1.5135, dev acc=52.70%
saving, test loss=1.5168, test acc=53.20%
epoch: 73/10000, 71s, train loss=1.5840, train acc=50.92%, dev loss=1.5095, dev acc=52.70%
epoch: 74/10000, 71s, train loss=1.5799, train acc=50.99%, dev loss=1.5055, dev acc=52.68%
epoch: 75/10000, 71s, train loss=1.5775, train acc=51.25%, dev loss=1.5016, dev acc=52.72%
saving, test loss=1.5049, test acc=53.40%
epoch: 76/10000, 72s, train loss=1.5731, train acc=51.31%, dev loss=1.4980, dev acc=52.89%
saving, test loss=1.5011, test acc=53.42%
epoch: 77/10000, 71s, train loss=1.5697, train acc=51.34%, dev loss=1.4943, dev acc=52.89%
epoch: 78/10000, 71s, train loss=1.5664, train acc=51.25%, dev loss=1.4906, dev acc=52.89%
epoch: 79/10000, 71s, train loss=1.5631, train acc=51.18%, dev loss=1.4872, dev acc=52.87%
epoch: 80/10000, 71s, train loss=1.5611, train acc=51.11%, dev loss=1.4838, dev acc=52.98%
saving, test loss=1.4870, test acc=53.39%
epoch: 81/10000, 71s, train loss=1.5587, train acc=51.12%, dev loss=1.4806, dev acc=53.07%
saving, test loss=1.4839, test acc=53.49%
epoch: 82/10000, 71s, train loss=1.5540, train acc=51.36%, dev loss=1.4774, dev acc=53.09%
saving, test loss=1.4807, test acc=53.60%
epoch: 83/10000, 71s, train loss=1.5542, train acc=51.35%, dev loss=1.4743, dev acc=53.04%
epoch: 84/10000, 71s, train loss=1.5496, train acc=51.31%, dev loss=1.4713, dev acc=53.09%
epoch: 85/10000, 71s, train loss=1.5453, train acc=51.18%, dev loss=1.4683, dev acc=53.19%
saving, test loss=1.4716, test acc=53.68%
epoch: 86/10000, 71s, train loss=1.5431, train acc=51.47%, dev loss=1.4654, dev acc=53.17%
epoch: 87/10000, 71s, train loss=1.5434, train acc=51.31%, dev loss=1.4626, dev acc=53.15%
epoch: 88/10000, 71s, train loss=1.5381, train acc=51.37%, dev loss=1.4599, dev acc=53.17%
epoch: 89/10000, 71s, train loss=1.5360, train acc=51.58%, dev loss=1.4572, dev acc=53.15%
epoch: 90/10000, 71s, train loss=1.5353, train acc=51.35%, dev loss=1.4546, dev acc=53.28%
saving, test loss=1.4578, test acc=53.67%
epoch: 91/10000, 71s, train loss=1.5337, train acc=51.65%, dev loss=1.4522, dev acc=53.19%
epoch: 92/10000, 71s, train loss=1.5275, train acc=51.71%, dev loss=1.4497, dev acc=53.24%
epoch: 93/10000, 71s, train loss=1.5285, train acc=51.43%, dev loss=1.4473, dev acc=53.32%
saving, test loss=1.4505, test acc=53.72%
epoch: 94/10000, 71s, train loss=1.5232, train acc=51.55%, dev loss=1.4450, dev acc=53.28%
epoch: 95/10000, 71s, train loss=1.5224, train acc=51.65%, dev loss=1.4426, dev acc=53.30%
epoch: 96/10000, 71s, train loss=1.5208, train acc=51.69%, dev loss=1.4404, dev acc=53.32%
epoch: 97/10000, 71s, train loss=1.5188, train acc=51.54%, dev loss=1.4382, dev acc=53.35%
saving, test loss=1.4416, test acc=53.71%
epoch: 98/10000, 71s, train loss=1.5152, train acc=51.69%, dev loss=1.4361, dev acc=53.32%
epoch: 99/10000, 71s, train loss=1.5132, train acc=51.76%, dev loss=1.4340, dev acc=53.35%
epoch: 100/10000, 71s, train loss=1.5139, train acc=51.67%, dev loss=1.4319, dev acc=53.41%
saving, test loss=1.4353, test acc=53.76%
epoch: 101/10000, 71s, train loss=1.5075, train acc=52.04%, dev loss=1.4299, dev acc=53.49%
saving, test loss=1.4333, test acc=53.69%
epoch: 102/10000, 71s, train loss=1.5070, train acc=51.76%, dev loss=1.4279, dev acc=53.43%
epoch: 103/10000, 71s, train loss=1.5071, train acc=51.59%, dev loss=1.4261, dev acc=53.56%
saving, test loss=1.4294, test acc=53.75%
epoch: 104/10000, 71s, train loss=1.5062, train acc=51.57%, dev loss=1.4242, dev acc=53.47%
epoch: 105/10000, 71s, train loss=1.5039, train acc=51.71%, dev loss=1.4225, dev acc=53.64%
saving, test loss=1.4259, test acc=53.83%
epoch: 106/10000, 71s, train loss=1.5020, train acc=51.95%, dev loss=1.4206, dev acc=53.54%
epoch: 107/10000, 71s, train loss=1.4990, train acc=51.72%, dev loss=1.4189, dev acc=53.58%
epoch: 108/10000, 71s, train loss=1.4960, train acc=51.94%, dev loss=1.4173, dev acc=53.62%
epoch: 109/10000, 71s, train loss=1.4993, train acc=51.81%, dev loss=1.4156, dev acc=53.43%
epoch: 110/10000, 71s, train loss=1.4962, train acc=51.73%, dev loss=1.4139, dev acc=53.62%
time used=9805.4s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='2', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-pert-large', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_house': 0, 'news_entertainment': 1, 'news_travel': 2, 'news_story': 3, 'news_game': 4, 'news_military': 5, 'news_finance': 6, 'news_culture': 7, 'news_tech': 8, 'news_agriculture': 9, 'news_car': 10, 'news_stock': 11, 'news_sports': 12, 'news_world': 13, 'news_edu': 14}
index_labels_dict={0: 'news_house', 1: 'news_entertainment', 2: 'news_travel', 3: 'news_story', 4: 'news_game', 5: 'news_military', 6: 'news_finance', 7: 'news_culture', 8: 'news_tech', 9: 'news_agriculture', 10: 'news_car', 11: 'news_stock', 12: 'news_sports', 13: 'news_world', 14: 'news_edu'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-pert-large were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (12): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (13): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (14): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (15): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (16): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (17): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (18): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (19): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (20): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (21): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (22): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (23): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=1024, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.attention.self.query.weight False
pretrained_model.encoder.layer.12.attention.self.query.bias False
pretrained_model.encoder.layer.12.attention.self.key.weight False
pretrained_model.encoder.layer.12.attention.self.key.bias False
pretrained_model.encoder.layer.12.attention.self.value.weight False
pretrained_model.encoder.layer.12.attention.self.value.bias False
pretrained_model.encoder.layer.12.attention.output.dense.weight False
pretrained_model.encoder.layer.12.attention.output.dense.bias False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.intermediate.dense.weight False
pretrained_model.encoder.layer.12.intermediate.dense.bias False
pretrained_model.encoder.layer.12.output.dense.weight False
pretrained_model.encoder.layer.12.output.dense.bias False
pretrained_model.encoder.layer.12.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.attention.self.query.weight False
pretrained_model.encoder.layer.13.attention.self.query.bias False
pretrained_model.encoder.layer.13.attention.self.key.weight False
pretrained_model.encoder.layer.13.attention.self.key.bias False
pretrained_model.encoder.layer.13.attention.self.value.weight False
pretrained_model.encoder.layer.13.attention.self.value.bias False
pretrained_model.encoder.layer.13.attention.output.dense.weight False
pretrained_model.encoder.layer.13.attention.output.dense.bias False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.intermediate.dense.weight False
pretrained_model.encoder.layer.13.intermediate.dense.bias False
pretrained_model.encoder.layer.13.output.dense.weight False
pretrained_model.encoder.layer.13.output.dense.bias False
pretrained_model.encoder.layer.13.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.attention.self.query.weight False
pretrained_model.encoder.layer.14.attention.self.query.bias False
pretrained_model.encoder.layer.14.attention.self.key.weight False
pretrained_model.encoder.layer.14.attention.self.key.bias False
pretrained_model.encoder.layer.14.attention.self.value.weight False
pretrained_model.encoder.layer.14.attention.self.value.bias False
pretrained_model.encoder.layer.14.attention.output.dense.weight False
pretrained_model.encoder.layer.14.attention.output.dense.bias False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.intermediate.dense.weight False
pretrained_model.encoder.layer.14.intermediate.dense.bias False
pretrained_model.encoder.layer.14.output.dense.weight False
pretrained_model.encoder.layer.14.output.dense.bias False
pretrained_model.encoder.layer.14.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.attention.self.query.weight False
pretrained_model.encoder.layer.15.attention.self.query.bias False
pretrained_model.encoder.layer.15.attention.self.key.weight False
pretrained_model.encoder.layer.15.attention.self.key.bias False
pretrained_model.encoder.layer.15.attention.self.value.weight False
pretrained_model.encoder.layer.15.attention.self.value.bias False
pretrained_model.encoder.layer.15.attention.output.dense.weight False
pretrained_model.encoder.layer.15.attention.output.dense.bias False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.intermediate.dense.weight False
pretrained_model.encoder.layer.15.intermediate.dense.bias False
pretrained_model.encoder.layer.15.output.dense.weight False
pretrained_model.encoder.layer.15.output.dense.bias False
pretrained_model.encoder.layer.15.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.attention.self.query.weight False
pretrained_model.encoder.layer.16.attention.self.query.bias False
pretrained_model.encoder.layer.16.attention.self.key.weight False
pretrained_model.encoder.layer.16.attention.self.key.bias False
pretrained_model.encoder.layer.16.attention.self.value.weight False
pretrained_model.encoder.layer.16.attention.self.value.bias False
pretrained_model.encoder.layer.16.attention.output.dense.weight False
pretrained_model.encoder.layer.16.attention.output.dense.bias False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.intermediate.dense.weight False
pretrained_model.encoder.layer.16.intermediate.dense.bias False
pretrained_model.encoder.layer.16.output.dense.weight False
pretrained_model.encoder.layer.16.output.dense.bias False
pretrained_model.encoder.layer.16.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.attention.self.query.weight False
pretrained_model.encoder.layer.17.attention.self.query.bias False
pretrained_model.encoder.layer.17.attention.self.key.weight False
pretrained_model.encoder.layer.17.attention.self.key.bias False
pretrained_model.encoder.layer.17.attention.self.value.weight False
pretrained_model.encoder.layer.17.attention.self.value.bias False
pretrained_model.encoder.layer.17.attention.output.dense.weight False
pretrained_model.encoder.layer.17.attention.output.dense.bias False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.intermediate.dense.weight False
pretrained_model.encoder.layer.17.intermediate.dense.bias False
pretrained_model.encoder.layer.17.output.dense.weight False
pretrained_model.encoder.layer.17.output.dense.bias False
pretrained_model.encoder.layer.17.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.attention.self.query.weight False
pretrained_model.encoder.layer.18.attention.self.query.bias False
pretrained_model.encoder.layer.18.attention.self.key.weight False
pretrained_model.encoder.layer.18.attention.self.key.bias False
pretrained_model.encoder.layer.18.attention.self.value.weight False
pretrained_model.encoder.layer.18.attention.self.value.bias False
pretrained_model.encoder.layer.18.attention.output.dense.weight False
pretrained_model.encoder.layer.18.attention.output.dense.bias False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.intermediate.dense.weight False
pretrained_model.encoder.layer.18.intermediate.dense.bias False
pretrained_model.encoder.layer.18.output.dense.weight False
pretrained_model.encoder.layer.18.output.dense.bias False
pretrained_model.encoder.layer.18.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.attention.self.query.weight False
pretrained_model.encoder.layer.19.attention.self.query.bias False
pretrained_model.encoder.layer.19.attention.self.key.weight False
pretrained_model.encoder.layer.19.attention.self.key.bias False
pretrained_model.encoder.layer.19.attention.self.value.weight False
pretrained_model.encoder.layer.19.attention.self.value.bias False
pretrained_model.encoder.layer.19.attention.output.dense.weight False
pretrained_model.encoder.layer.19.attention.output.dense.bias False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.intermediate.dense.weight False
pretrained_model.encoder.layer.19.intermediate.dense.bias False
pretrained_model.encoder.layer.19.output.dense.weight False
pretrained_model.encoder.layer.19.output.dense.bias False
pretrained_model.encoder.layer.19.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.attention.self.query.weight False
pretrained_model.encoder.layer.20.attention.self.query.bias False
pretrained_model.encoder.layer.20.attention.self.key.weight False
pretrained_model.encoder.layer.20.attention.self.key.bias False
pretrained_model.encoder.layer.20.attention.self.value.weight False
pretrained_model.encoder.layer.20.attention.self.value.bias False
pretrained_model.encoder.layer.20.attention.output.dense.weight False
pretrained_model.encoder.layer.20.attention.output.dense.bias False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.intermediate.dense.weight False
pretrained_model.encoder.layer.20.intermediate.dense.bias False
pretrained_model.encoder.layer.20.output.dense.weight False
pretrained_model.encoder.layer.20.output.dense.bias False
pretrained_model.encoder.layer.20.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.attention.self.query.weight False
pretrained_model.encoder.layer.21.attention.self.query.bias False
pretrained_model.encoder.layer.21.attention.self.key.weight False
pretrained_model.encoder.layer.21.attention.self.key.bias False
pretrained_model.encoder.layer.21.attention.self.value.weight False
pretrained_model.encoder.layer.21.attention.self.value.bias False
pretrained_model.encoder.layer.21.attention.output.dense.weight False
pretrained_model.encoder.layer.21.attention.output.dense.bias False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.intermediate.dense.weight False
pretrained_model.encoder.layer.21.intermediate.dense.bias False
pretrained_model.encoder.layer.21.output.dense.weight False
pretrained_model.encoder.layer.21.output.dense.bias False
pretrained_model.encoder.layer.21.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.attention.self.query.weight False
pretrained_model.encoder.layer.22.attention.self.query.bias False
pretrained_model.encoder.layer.22.attention.self.key.weight False
pretrained_model.encoder.layer.22.attention.self.key.bias False
pretrained_model.encoder.layer.22.attention.self.value.weight False
pretrained_model.encoder.layer.22.attention.self.value.bias False
pretrained_model.encoder.layer.22.attention.output.dense.weight False
pretrained_model.encoder.layer.22.attention.output.dense.bias False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.intermediate.dense.weight False
pretrained_model.encoder.layer.22.intermediate.dense.bias False
pretrained_model.encoder.layer.22.output.dense.weight False
pretrained_model.encoder.layer.22.output.dense.bias False
pretrained_model.encoder.layer.22.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.attention.self.query.weight False
pretrained_model.encoder.layer.23.attention.self.query.bias False
pretrained_model.encoder.layer.23.attention.self.key.weight False
pretrained_model.encoder.layer.23.attention.self.key.bias False
pretrained_model.encoder.layer.23.attention.self.value.weight False
pretrained_model.encoder.layer.23.attention.self.value.bias False
pretrained_model.encoder.layer.23.attention.output.dense.weight False
pretrained_model.encoder.layer.23.attention.output.dense.bias False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.intermediate.dense.weight False
pretrained_model.encoder.layer.23.intermediate.dense.bias False
pretrained_model.encoder.layer.23.output.dense.weight False
pretrained_model.encoder.layer.23.output.dense.bias False
pretrained_model.encoder.layer.23.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 223s, train loss=2.6390, train acc=10.87%, dev loss=2.5882, dev acc=12.86%
saving, test loss=2.5910, test acc=12.53%
epoch: 2/10000, 224s, train loss=2.5795, train acc=12.38%, dev loss=2.5608, dev acc=12.20%
epoch: 3/10000, 224s, train loss=2.5571, train acc=14.14%, dev loss=2.5425, dev acc=13.47%
saving, test loss=2.5445, test acc=12.92%
epoch: 4/10000, 225s, train loss=2.5388, train acc=15.46%, dev loss=2.5266, dev acc=14.69%
saving, test loss=2.5285, test acc=14.33%
epoch: 5/10000, 225s, train loss=2.5234, train acc=17.19%, dev loss=2.5119, dev acc=16.30%
saving, test loss=2.5139, test acc=15.84%
epoch: 6/10000, 224s, train loss=2.5100, train acc=18.18%, dev loss=2.4983, dev acc=16.74%
saving, test loss=2.5003, test acc=16.54%
epoch: 7/10000, 224s, train loss=2.4965, train acc=19.16%, dev loss=2.4848, dev acc=18.05%
saving, test loss=2.4871, test acc=17.83%
epoch: 8/10000, 225s, train loss=2.4849, train acc=19.82%, dev loss=2.4717, dev acc=19.40%
saving, test loss=2.4740, test acc=18.85%
epoch: 9/10000, 225s, train loss=2.4730, train acc=20.75%, dev loss=2.4598, dev acc=19.73%
saving, test loss=2.4621, test acc=19.09%
epoch: 10/10000, 225s, train loss=2.4612, train acc=21.24%, dev loss=2.4479, dev acc=20.56%
saving, test loss=2.4502, test acc=20.10%
epoch: 11/10000, 225s, train loss=2.4502, train acc=21.65%, dev loss=2.4364, dev acc=21.20%
saving, test loss=2.4388, test acc=20.65%
epoch: 12/10000, 225s, train loss=2.4398, train acc=22.17%, dev loss=2.4256, dev acc=21.91%
saving, test loss=2.4279, test acc=21.42%
epoch: 13/10000, 226s, train loss=2.4309, train acc=22.70%, dev loss=2.4147, dev acc=22.64%
saving, test loss=2.4172, test acc=22.15%
epoch: 14/10000, 226s, train loss=2.4202, train acc=22.77%, dev loss=2.4045, dev acc=22.66%
saving, test loss=2.4067, test acc=22.50%
epoch: 15/10000, 226s, train loss=2.4112, train acc=23.46%, dev loss=2.3947, dev acc=23.48%
saving, test loss=2.3966, test acc=23.06%
epoch: 16/10000, 226s, train loss=2.4010, train acc=23.88%, dev loss=2.3852, dev acc=23.89%
saving, test loss=2.3871, test acc=23.27%
epoch: 17/10000, 226s, train loss=2.3924, train acc=24.19%, dev loss=2.3756, dev acc=24.49%
saving, test loss=2.3775, test acc=23.92%
epoch: 18/10000, 226s, train loss=2.3852, train acc=24.16%, dev loss=2.3671, dev acc=24.68%
saving, test loss=2.3689, test acc=23.95%
epoch: 19/10000, 226s, train loss=2.3774, train acc=24.54%, dev loss=2.3581, dev acc=25.26%
saving, test loss=2.3598, test acc=24.36%
epoch: 20/10000, 226s, train loss=2.3676, train acc=25.17%, dev loss=2.3496, dev acc=25.37%
saving, test loss=2.3509, test acc=24.68%
epoch: 21/10000, 226s, train loss=2.3612, train acc=25.04%, dev loss=2.3416, dev acc=25.81%
saving, test loss=2.3427, test acc=24.85%
epoch: 22/10000, 227s, train loss=2.3536, train acc=25.53%, dev loss=2.3332, dev acc=26.39%
saving, test loss=2.3341, test acc=25.71%
epoch: 23/10000, 227s, train loss=2.3463, train acc=25.76%, dev loss=2.3255, dev acc=26.18%
epoch: 24/10000, 227s, train loss=2.3413, train acc=26.07%, dev loss=2.3177, dev acc=26.59%
saving, test loss=2.3186, test acc=26.00%
epoch: 25/10000, 227s, train loss=2.3323, train acc=26.38%, dev loss=2.3102, dev acc=27.23%
saving, test loss=2.3108, test acc=26.61%
epoch: 26/10000, 227s, train loss=2.3280, train acc=26.17%, dev loss=2.3030, dev acc=27.64%
saving, test loss=2.3034, test acc=27.18%
epoch: 27/10000, 227s, train loss=2.3197, train acc=26.68%, dev loss=2.2959, dev acc=27.83%
saving, test loss=2.2964, test acc=27.45%
epoch: 28/10000, 227s, train loss=2.3138, train acc=27.09%, dev loss=2.2893, dev acc=28.15%
saving, test loss=2.2894, test acc=27.84%
epoch: 29/10000, 228s, train loss=2.3091, train acc=26.93%, dev loss=2.2825, dev acc=28.39%
saving, test loss=2.2823, test acc=27.97%
epoch: 30/10000, 228s, train loss=2.3027, train acc=27.28%, dev loss=2.2758, dev acc=28.52%
saving, test loss=2.2756, test acc=28.42%
epoch: 31/10000, 228s, train loss=2.2956, train acc=27.65%, dev loss=2.2696, dev acc=29.05%
saving, test loss=2.2690, test acc=28.58%
epoch: 32/10000, 228s, train loss=2.2929, train acc=27.47%, dev loss=2.2632, dev acc=29.22%
saving, test loss=2.2625, test acc=28.89%
epoch: 33/10000, 228s, train loss=2.2857, train acc=27.89%, dev loss=2.2573, dev acc=29.50%
saving, test loss=2.2563, test acc=29.28%
epoch: 34/10000, 228s, train loss=2.2807, train acc=28.18%, dev loss=2.2515, dev acc=29.54%
saving, test loss=2.2507, test acc=29.13%
epoch: 35/10000, 228s, train loss=2.2769, train acc=27.73%, dev loss=2.2457, dev acc=29.85%
saving, test loss=2.2445, test acc=29.50%
epoch: 36/10000, 228s, train loss=2.2703, train acc=28.40%, dev loss=2.2397, dev acc=30.49%
saving, test loss=2.2384, test acc=29.97%
epoch: 37/10000, 228s, train loss=2.2672, train acc=28.13%, dev loss=2.2345, dev acc=30.51%
saving, test loss=2.2331, test acc=30.00%
epoch: 38/10000, 228s, train loss=2.2616, train acc=28.66%, dev loss=2.2290, dev acc=30.73%
saving, test loss=2.2275, test acc=30.32%
epoch: 39/10000, 228s, train loss=2.2573, train acc=28.60%, dev loss=2.2237, dev acc=31.15%
saving, test loss=2.2220, test acc=30.77%
epoch: 40/10000, 228s, train loss=2.2510, train acc=28.93%, dev loss=2.2184, dev acc=31.35%
saving, test loss=2.2164, test acc=30.91%
epoch: 41/10000, 228s, train loss=2.2478, train acc=28.85%, dev loss=2.2134, dev acc=31.30%
epoch: 42/10000, 228s, train loss=2.2437, train acc=29.17%, dev loss=2.2085, dev acc=31.58%
saving, test loss=2.2062, test acc=31.34%
epoch: 43/10000, 228s, train loss=2.2401, train acc=29.21%, dev loss=2.2036, dev acc=31.73%
saving, test loss=2.2012, test acc=31.47%
epoch: 44/10000, 228s, train loss=2.2368, train acc=29.39%, dev loss=2.1990, dev acc=31.95%
saving, test loss=2.1965, test acc=31.58%
epoch: 45/10000, 228s, train loss=2.2317, train acc=29.34%, dev loss=2.1938, dev acc=32.14%
saving, test loss=2.1913, test acc=31.92%
epoch: 46/10000, 228s, train loss=2.2305, train acc=29.64%, dev loss=2.1897, dev acc=32.40%
saving, test loss=2.1871, test acc=31.99%
epoch: 47/10000, 228s, train loss=2.2254, train acc=29.59%, dev loss=2.1850, dev acc=32.50%
saving, test loss=2.1824, test acc=32.18%
epoch: 48/10000, 228s, train loss=2.2228, train acc=29.51%, dev loss=2.1809, dev acc=32.53%
saving, test loss=2.1782, test acc=32.33%
epoch: 49/10000, 227s, train loss=2.2161, train acc=29.85%, dev loss=2.1766, dev acc=32.63%
saving, test loss=2.1737, test acc=32.44%
epoch: 50/10000, 227s, train loss=2.2149, train acc=29.89%, dev loss=2.1721, dev acc=32.96%
saving, test loss=2.1692, test acc=32.74%
epoch: 51/10000, 227s, train loss=2.2131, train acc=29.99%, dev loss=2.1681, dev acc=33.19%
saving, test loss=2.1648, test acc=32.97%
epoch: 52/10000, 227s, train loss=2.2086, train acc=30.24%, dev loss=2.1642, dev acc=33.26%
saving, test loss=2.1609, test acc=32.86%
epoch: 53/10000, 228s, train loss=2.2065, train acc=30.22%, dev loss=2.1608, dev acc=33.06%
epoch: 54/10000, 228s, train loss=2.2035, train acc=30.06%, dev loss=2.1567, dev acc=33.41%
saving, test loss=2.1528, test acc=33.31%
epoch: 55/10000, 228s, train loss=2.1994, train acc=30.18%, dev loss=2.1530, dev acc=33.55%
saving, test loss=2.1488, test acc=33.41%
epoch: 56/10000, 228s, train loss=2.1994, train acc=30.47%, dev loss=2.1492, dev acc=33.62%
saving, test loss=2.1452, test acc=33.45%
epoch: 57/10000, 228s, train loss=2.1960, train acc=30.35%, dev loss=2.1455, dev acc=33.85%
saving, test loss=2.1412, test acc=33.71%
epoch: 58/10000, 228s, train loss=2.1894, train acc=30.66%, dev loss=2.1417, dev acc=33.81%
epoch: 59/10000, 228s, train loss=2.1897, train acc=30.59%, dev loss=2.1384, dev acc=34.13%
saving, test loss=2.1340, test acc=33.87%
epoch: 60/10000, 228s, train loss=2.1858, train acc=30.77%, dev loss=2.1350, dev acc=34.18%
saving, test loss=2.1303, test acc=33.98%
epoch: 61/10000, 228s, train loss=2.1844, train acc=30.78%, dev loss=2.1318, dev acc=34.24%
saving, test loss=2.1271, test acc=34.22%
epoch: 62/10000, 228s, train loss=2.1795, train acc=31.06%, dev loss=2.1284, dev acc=34.30%
saving, test loss=2.1236, test acc=34.22%
epoch: 63/10000, 228s, train loss=2.1767, train acc=31.03%, dev loss=2.1249, dev acc=34.45%
saving, test loss=2.1200, test acc=34.49%
epoch: 64/10000, 228s, train loss=2.1738, train acc=31.14%, dev loss=2.1217, dev acc=34.69%
saving, test loss=2.1165, test acc=34.75%
epoch: 65/10000, 228s, train loss=2.1732, train acc=30.98%, dev loss=2.1188, dev acc=34.45%
epoch: 66/10000, 228s, train loss=2.1694, train acc=31.39%, dev loss=2.1147, dev acc=34.84%
saving, test loss=2.1097, test acc=34.93%
epoch: 67/10000, 228s, train loss=2.1665, train acc=31.43%, dev loss=2.1121, dev acc=34.69%
epoch: 68/10000, 228s, train loss=2.1642, train acc=31.37%, dev loss=2.1090, dev acc=34.90%
saving, test loss=2.1038, test acc=34.95%
epoch: 69/10000, 228s, train loss=2.1614, train acc=31.71%, dev loss=2.1063, dev acc=34.93%
saving, test loss=2.1008, test acc=35.06%
epoch: 70/10000, 228s, train loss=2.1623, train acc=31.31%, dev loss=2.1037, dev acc=34.99%
saving, test loss=2.0981, test acc=35.21%
epoch: 71/10000, 228s, train loss=2.1597, train acc=31.51%, dev loss=2.1006, dev acc=35.14%
saving, test loss=2.0949, test acc=35.23%
epoch: 72/10000, 228s, train loss=2.1584, train acc=31.42%, dev loss=2.0972, dev acc=35.25%
saving, test loss=2.0916, test acc=35.58%
epoch: 73/10000, 228s, train loss=2.1533, train acc=31.74%, dev loss=2.0951, dev acc=35.29%
saving, test loss=2.0898, test acc=35.37%
epoch: 74/10000, 228s, train loss=2.1548, train acc=31.45%, dev loss=2.0923, dev acc=35.46%
saving, test loss=2.0865, test acc=35.55%
epoch: 75/10000, 228s, train loss=2.1505, train acc=31.89%, dev loss=2.0894, dev acc=35.53%
saving, test loss=2.0838, test acc=35.47%
epoch: 76/10000, 228s, train loss=2.1487, train acc=31.84%, dev loss=2.0865, dev acc=35.68%
saving, test loss=2.0810, test acc=35.80%
epoch: 77/10000, 228s, train loss=2.1443, train acc=32.06%, dev loss=2.0844, dev acc=35.70%
saving, test loss=2.0786, test acc=35.63%
epoch: 78/10000, 228s, train loss=2.1435, train acc=32.29%, dev loss=2.0816, dev acc=35.83%
saving, test loss=2.0758, test acc=35.95%
epoch: 79/10000, 228s, train loss=2.1477, train acc=31.46%, dev loss=2.0793, dev acc=36.00%
saving, test loss=2.0735, test acc=35.90%
epoch: 80/10000, 228s, train loss=2.1409, train acc=31.96%, dev loss=2.0765, dev acc=35.94%
epoch: 81/10000, 228s, train loss=2.1392, train acc=32.08%, dev loss=2.0745, dev acc=36.26%
saving, test loss=2.0684, test acc=36.19%
epoch: 82/10000, 228s, train loss=2.1396, train acc=32.07%, dev loss=2.0718, dev acc=36.24%
epoch: 83/10000, 228s, train loss=2.1330, train acc=32.04%, dev loss=2.0697, dev acc=36.32%
saving, test loss=2.0633, test acc=36.36%
epoch: 84/10000, 228s, train loss=2.1350, train acc=31.83%, dev loss=2.0677, dev acc=36.39%
saving, test loss=2.0614, test acc=36.47%
epoch: 85/10000, 228s, train loss=2.1318, train acc=32.21%, dev loss=2.0652, dev acc=36.39%
epoch: 86/10000, 228s, train loss=2.1342, train acc=32.11%, dev loss=2.0631, dev acc=36.51%
saving, test loss=2.0566, test acc=36.39%
epoch: 87/10000, 228s, train loss=2.1299, train acc=32.28%, dev loss=2.0607, dev acc=36.62%
saving, test loss=2.0542, test acc=36.67%
epoch: 88/10000, 228s, train loss=2.1276, train acc=32.29%, dev loss=2.0585, dev acc=36.64%
saving, test loss=2.0520, test acc=36.65%
epoch: 89/10000, 228s, train loss=2.1298, train acc=31.98%, dev loss=2.0567, dev acc=36.68%
saving, test loss=2.0500, test acc=36.71%
epoch: 90/10000, 228s, train loss=2.1281, train acc=32.08%, dev loss=2.0539, dev acc=36.75%
saving, test loss=2.0474, test acc=36.90%
epoch: 91/10000, 228s, train loss=2.1255, train acc=32.19%, dev loss=2.0523, dev acc=36.77%
saving, test loss=2.0457, test acc=36.92%
epoch: 92/10000, 228s, train loss=2.1236, train acc=32.15%, dev loss=2.0503, dev acc=36.73%
epoch: 93/10000, 228s, train loss=2.1222, train acc=32.06%, dev loss=2.0482, dev acc=36.81%
saving, test loss=2.0415, test acc=37.00%
epoch: 94/10000, 228s, train loss=2.1207, train acc=32.48%, dev loss=2.0463, dev acc=36.83%
saving, test loss=2.0395, test acc=37.06%
epoch: 95/10000, 228s, train loss=2.1199, train acc=32.44%, dev loss=2.0442, dev acc=36.99%
saving, test loss=2.0377, test acc=37.01%
epoch: 96/10000, 228s, train loss=2.1160, train acc=32.58%, dev loss=2.0423, dev acc=37.03%
saving, test loss=2.0355, test acc=37.04%
epoch: 97/10000, 228s, train loss=2.1213, train acc=32.44%, dev loss=2.0406, dev acc=37.13%
saving, test loss=2.0339, test acc=37.13%
epoch: 98/10000, 228s, train loss=2.1162, train acc=32.37%, dev loss=2.0390, dev acc=37.01%
epoch: 99/10000, 228s, train loss=2.1126, train acc=32.59%, dev loss=2.0365, dev acc=37.22%
saving, test loss=2.0295, test acc=37.43%
epoch: 100/10000, 228s, train loss=2.1159, train acc=32.44%, dev loss=2.0352, dev acc=37.22%
epoch: 101/10000, 228s, train loss=2.1123, train acc=32.73%, dev loss=2.0335, dev acc=37.33%
saving, test loss=2.0265, test acc=37.42%
epoch: 102/10000, 228s, train loss=2.1115, train acc=32.53%, dev loss=2.0310, dev acc=37.43%
saving, test loss=2.0240, test acc=37.52%
epoch: 103/10000, 229s, train loss=2.1122, train acc=32.65%, dev loss=2.0299, dev acc=37.41%
epoch: 104/10000, 228s, train loss=2.1052, train acc=33.02%, dev loss=2.0278, dev acc=37.43%
epoch: 105/10000, 228s, train loss=2.1079, train acc=32.84%, dev loss=2.0265, dev acc=37.46%
saving, test loss=2.0192, test acc=37.62%
epoch: 106/10000, 228s, train loss=2.1056, train acc=32.80%, dev loss=2.0246, dev acc=37.63%
saving, test loss=2.0170, test acc=37.85%
epoch: 107/10000, 228s, train loss=2.1092, train acc=32.59%, dev loss=2.0232, dev acc=37.57%
epoch: 108/10000, 228s, train loss=2.1070, train acc=32.69%, dev loss=2.0216, dev acc=37.63%
epoch: 109/10000, 228s, train loss=2.1046, train acc=32.85%, dev loss=2.0198, dev acc=37.71%
saving, test loss=2.0125, test acc=37.79%
epoch: 110/10000, 228s, train loss=2.1060, train acc=32.70%, dev loss=2.0182, dev acc=37.67%
epoch: 111/10000, 228s, train loss=2.1006, train acc=32.88%, dev loss=2.0166, dev acc=37.72%
saving, test loss=2.0093, test acc=37.96%
epoch: 112/10000, 228s, train loss=2.1022, train acc=32.72%, dev loss=2.0155, dev acc=37.78%
saving, test loss=2.0079, test acc=37.98%
epoch: 113/10000, 228s, train loss=2.0999, train acc=32.92%, dev loss=2.0140, dev acc=37.86%
saving, test loss=2.0065, test acc=38.04%
epoch: 114/10000, 228s, train loss=2.0972, train acc=32.98%, dev loss=2.0126, dev acc=37.78%
epoch: 115/10000, 228s, train loss=2.0941, train acc=32.99%, dev loss=2.0108, dev acc=37.93%
saving, test loss=2.0031, test acc=38.01%
epoch: 116/10000, 228s, train loss=2.0977, train acc=32.86%, dev loss=2.0096, dev acc=37.89%
epoch: 117/10000, 228s, train loss=2.0951, train acc=33.07%, dev loss=2.0080, dev acc=37.86%
epoch: 118/10000, 228s, train loss=2.0966, train acc=33.10%, dev loss=2.0065, dev acc=37.95%
saving, test loss=1.9990, test acc=38.24%
epoch: 119/10000, 228s, train loss=2.0954, train acc=32.83%, dev loss=2.0051, dev acc=37.87%
epoch: 120/10000, 228s, train loss=2.0956, train acc=33.21%, dev loss=2.0037, dev acc=38.01%
saving, test loss=1.9961, test acc=38.44%
epoch: 121/10000, 228s, train loss=2.0906, train acc=33.35%, dev loss=2.0025, dev acc=38.08%
saving, test loss=1.9948, test acc=38.43%
epoch: 122/10000, 228s, train loss=2.0937, train acc=32.91%, dev loss=2.0012, dev acc=38.04%
epoch: 123/10000, 228s, train loss=2.0899, train acc=33.18%, dev loss=1.9999, dev acc=38.06%
epoch: 124/10000, 228s, train loss=2.0917, train acc=33.08%, dev loss=1.9986, dev acc=38.08%
epoch: 125/10000, 228s, train loss=2.0878, train acc=33.39%, dev loss=1.9973, dev acc=38.19%
saving, test loss=1.9895, test acc=38.47%
epoch: 126/10000, 228s, train loss=2.0867, train acc=33.04%, dev loss=1.9958, dev acc=38.16%
epoch: 127/10000, 229s, train loss=2.0865, train acc=33.26%, dev loss=1.9949, dev acc=38.12%
epoch: 128/10000, 228s, train loss=2.0908, train acc=32.98%, dev loss=1.9933, dev acc=38.21%
saving, test loss=1.9855, test acc=38.53%
epoch: 129/10000, 228s, train loss=2.0901, train acc=33.05%, dev loss=1.9917, dev acc=38.29%
saving, test loss=1.9840, test acc=38.52%
epoch: 130/10000, 228s, train loss=2.0859, train acc=33.15%, dev loss=1.9907, dev acc=38.31%
saving, test loss=1.9827, test acc=38.62%
epoch: 131/10000, 229s, train loss=2.0837, train acc=33.31%, dev loss=1.9901, dev acc=38.29%
epoch: 132/10000, 229s, train loss=2.0844, train acc=33.23%, dev loss=1.9886, dev acc=38.36%
saving, test loss=1.9807, test acc=38.76%
epoch: 133/10000, 229s, train loss=2.0810, train acc=33.27%, dev loss=1.9877, dev acc=38.36%
epoch: 134/10000, 228s, train loss=2.0832, train acc=33.13%, dev loss=1.9863, dev acc=38.36%
epoch: 135/10000, 228s, train loss=2.0812, train acc=33.38%, dev loss=1.9849, dev acc=38.36%
epoch: 136/10000, 228s, train loss=2.0782, train acc=33.46%, dev loss=1.9839, dev acc=38.32%
epoch: 137/10000, 228s, train loss=2.0823, train acc=33.19%, dev loss=1.9831, dev acc=38.38%
saving, test loss=1.9751, test acc=38.72%
epoch: 138/10000, 228s, train loss=2.0835, train acc=33.23%, dev loss=1.9823, dev acc=38.38%
epoch: 139/10000, 229s, train loss=2.0830, train acc=33.14%, dev loss=1.9806, dev acc=38.47%
saving, test loss=1.9725, test acc=38.81%
epoch: 140/10000, 229s, train loss=2.0741, train acc=33.47%, dev loss=1.9797, dev acc=38.59%
saving, test loss=1.9715, test acc=38.83%
epoch: 141/10000, 228s, train loss=2.0806, train acc=32.98%, dev loss=1.9789, dev acc=38.47%
epoch: 142/10000, 228s, train loss=2.0780, train acc=33.29%, dev loss=1.9774, dev acc=38.59%
epoch: 143/10000, 228s, train loss=2.0767, train acc=33.37%, dev loss=1.9765, dev acc=38.59%
epoch: 144/10000, 228s, train loss=2.0751, train acc=33.19%, dev loss=1.9756, dev acc=38.51%
epoch: 145/10000, 228s, train loss=2.0746, train acc=33.32%, dev loss=1.9745, dev acc=38.36%
time used=41577.8s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='2', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-roberta-wwm-ext', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_game': 0, 'news_sports': 1, 'news_story': 2, 'news_car': 3, 'news_culture': 4, 'news_military': 5, 'news_edu': 6, 'news_tech': 7, 'news_world': 8, 'news_entertainment': 9, 'news_stock': 10, 'news_agriculture': 11, 'news_travel': 12, 'news_house': 13, 'news_finance': 14}
index_labels_dict={0: 'news_game', 1: 'news_sports', 2: 'news_story', 3: 'news_car', 4: 'news_culture', 5: 'news_military', 6: 'news_edu', 7: 'news_tech', 8: 'news_world', 9: 'news_entertainment', 10: 'news_stock', 11: 'news_agriculture', 12: 'news_travel', 13: 'news_house', 14: 'news_finance'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 70s, train loss=2.3658, train acc=28.11%, dev loss=1.9638, dev acc=49.61%
saving, test loss=1.9709, test acc=48.28%
epoch: 2/10000, 70s, train loss=1.8226, train acc=49.08%, dev loss=1.6077, dev acc=53.28%
saving, test loss=1.6152, test acc=52.91%
epoch: 3/10000, 70s, train loss=1.5854, train acc=52.26%, dev loss=1.4538, dev acc=54.63%
saving, test loss=1.4618, test acc=54.92%
epoch: 4/10000, 70s, train loss=1.4810, train acc=53.31%, dev loss=1.3809, dev acc=55.06%
saving, test loss=1.3897, test acc=55.67%
epoch: 5/10000, 70s, train loss=1.4291, train acc=53.80%, dev loss=1.3407, dev acc=55.15%
saving, test loss=1.3503, test acc=55.88%
epoch: 6/10000, 70s, train loss=1.3964, train acc=54.06%, dev loss=1.3150, dev acc=55.17%
saving, test loss=1.3253, test acc=56.06%
epoch: 7/10000, 70s, train loss=1.3774, train acc=54.38%, dev loss=1.2978, dev acc=55.36%
saving, test loss=1.3087, test acc=56.05%
epoch: 8/10000, 70s, train loss=1.3624, train acc=54.65%, dev loss=1.2859, dev acc=55.73%
saving, test loss=1.2972, test acc=56.18%
epoch: 9/10000, 71s, train loss=1.3537, train acc=54.56%, dev loss=1.2773, dev acc=55.83%
saving, test loss=1.2889, test acc=56.31%
epoch: 10/10000, 71s, train loss=1.3416, train acc=54.93%, dev loss=1.2701, dev acc=56.05%
saving, test loss=1.2823, test acc=56.19%
epoch: 11/10000, 71s, train loss=1.3355, train acc=54.87%, dev loss=1.2644, dev acc=56.03%
epoch: 12/10000, 71s, train loss=1.3294, train acc=55.08%, dev loss=1.2597, dev acc=56.09%
saving, test loss=1.2722, test acc=55.97%
epoch: 13/10000, 71s, train loss=1.3292, train acc=54.99%, dev loss=1.2557, dev acc=56.28%
saving, test loss=1.2684, test acc=55.95%
epoch: 14/10000, 71s, train loss=1.3255, train acc=54.93%, dev loss=1.2524, dev acc=56.43%
saving, test loss=1.2649, test acc=55.89%
epoch: 15/10000, 71s, train loss=1.3226, train acc=55.16%, dev loss=1.2494, dev acc=56.52%
saving, test loss=1.2625, test acc=55.95%
epoch: 16/10000, 71s, train loss=1.3193, train acc=55.23%, dev loss=1.2472, dev acc=56.41%
epoch: 17/10000, 71s, train loss=1.3150, train acc=55.13%, dev loss=1.2448, dev acc=56.43%
epoch: 18/10000, 71s, train loss=1.3135, train acc=55.42%, dev loss=1.2434, dev acc=56.50%
epoch: 19/10000, 71s, train loss=1.3133, train acc=55.13%, dev loss=1.2416, dev acc=56.35%
epoch: 20/10000, 71s, train loss=1.3133, train acc=55.11%, dev loss=1.2402, dev acc=56.43%
time used=1786.1s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-roberta-wwm-ext-large', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_travel': 0, 'news_edu': 1, 'news_agriculture': 2, 'news_tech': 3, 'news_story': 4, 'news_game': 5, 'news_stock': 6, 'news_car': 7, 'news_house': 8, 'news_world': 9, 'news_culture': 10, 'news_military': 11, 'news_finance': 12, 'news_sports': 13, 'news_entertainment': 14}
index_labels_dict={0: 'news_travel', 1: 'news_edu', 2: 'news_agriculture', 3: 'news_tech', 4: 'news_story', 5: 'news_game', 6: 'news_stock', 7: 'news_car', 8: 'news_house', 9: 'news_world', 10: 'news_culture', 11: 'news_military', 12: 'news_finance', 13: 'news_sports', 14: 'news_entertainment'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-roberta-wwm-ext-large were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (12): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (13): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (14): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (15): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (16): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (17): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (18): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (19): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (20): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (21): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (22): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (23): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=1024, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.attention.self.query.weight False
pretrained_model.encoder.layer.6.attention.self.query.bias False
pretrained_model.encoder.layer.6.attention.self.key.weight False
pretrained_model.encoder.layer.6.attention.self.key.bias False
pretrained_model.encoder.layer.6.attention.self.value.weight False
pretrained_model.encoder.layer.6.attention.self.value.bias False
pretrained_model.encoder.layer.6.attention.output.dense.weight False
pretrained_model.encoder.layer.6.attention.output.dense.bias False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.6.intermediate.dense.weight False
pretrained_model.encoder.layer.6.intermediate.dense.bias False
pretrained_model.encoder.layer.6.output.dense.weight False
pretrained_model.encoder.layer.6.output.dense.bias False
pretrained_model.encoder.layer.6.output.LayerNorm.weight False
pretrained_model.encoder.layer.6.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.attention.self.query.weight False
pretrained_model.encoder.layer.7.attention.self.query.bias False
pretrained_model.encoder.layer.7.attention.self.key.weight False
pretrained_model.encoder.layer.7.attention.self.key.bias False
pretrained_model.encoder.layer.7.attention.self.value.weight False
pretrained_model.encoder.layer.7.attention.self.value.bias False
pretrained_model.encoder.layer.7.attention.output.dense.weight False
pretrained_model.encoder.layer.7.attention.output.dense.bias False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.7.intermediate.dense.weight False
pretrained_model.encoder.layer.7.intermediate.dense.bias False
pretrained_model.encoder.layer.7.output.dense.weight False
pretrained_model.encoder.layer.7.output.dense.bias False
pretrained_model.encoder.layer.7.output.LayerNorm.weight False
pretrained_model.encoder.layer.7.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.attention.self.query.weight False
pretrained_model.encoder.layer.8.attention.self.query.bias False
pretrained_model.encoder.layer.8.attention.self.key.weight False
pretrained_model.encoder.layer.8.attention.self.key.bias False
pretrained_model.encoder.layer.8.attention.self.value.weight False
pretrained_model.encoder.layer.8.attention.self.value.bias False
pretrained_model.encoder.layer.8.attention.output.dense.weight False
pretrained_model.encoder.layer.8.attention.output.dense.bias False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.8.intermediate.dense.weight False
pretrained_model.encoder.layer.8.intermediate.dense.bias False
pretrained_model.encoder.layer.8.output.dense.weight False
pretrained_model.encoder.layer.8.output.dense.bias False
pretrained_model.encoder.layer.8.output.LayerNorm.weight False
pretrained_model.encoder.layer.8.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.attention.self.query.weight False
pretrained_model.encoder.layer.9.attention.self.query.bias False
pretrained_model.encoder.layer.9.attention.self.key.weight False
pretrained_model.encoder.layer.9.attention.self.key.bias False
pretrained_model.encoder.layer.9.attention.self.value.weight False
pretrained_model.encoder.layer.9.attention.self.value.bias False
pretrained_model.encoder.layer.9.attention.output.dense.weight False
pretrained_model.encoder.layer.9.attention.output.dense.bias False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.9.intermediate.dense.weight False
pretrained_model.encoder.layer.9.intermediate.dense.bias False
pretrained_model.encoder.layer.9.output.dense.weight False
pretrained_model.encoder.layer.9.output.dense.bias False
pretrained_model.encoder.layer.9.output.LayerNorm.weight False
pretrained_model.encoder.layer.9.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.attention.self.query.weight False
pretrained_model.encoder.layer.10.attention.self.query.bias False
pretrained_model.encoder.layer.10.attention.self.key.weight False
pretrained_model.encoder.layer.10.attention.self.key.bias False
pretrained_model.encoder.layer.10.attention.self.value.weight False
pretrained_model.encoder.layer.10.attention.self.value.bias False
pretrained_model.encoder.layer.10.attention.output.dense.weight False
pretrained_model.encoder.layer.10.attention.output.dense.bias False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.10.intermediate.dense.weight False
pretrained_model.encoder.layer.10.intermediate.dense.bias False
pretrained_model.encoder.layer.10.output.dense.weight False
pretrained_model.encoder.layer.10.output.dense.bias False
pretrained_model.encoder.layer.10.output.LayerNorm.weight False
pretrained_model.encoder.layer.10.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.attention.self.query.weight False
pretrained_model.encoder.layer.11.attention.self.query.bias False
pretrained_model.encoder.layer.11.attention.self.key.weight False
pretrained_model.encoder.layer.11.attention.self.key.bias False
pretrained_model.encoder.layer.11.attention.self.value.weight False
pretrained_model.encoder.layer.11.attention.self.value.bias False
pretrained_model.encoder.layer.11.attention.output.dense.weight False
pretrained_model.encoder.layer.11.attention.output.dense.bias False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.11.intermediate.dense.weight False
pretrained_model.encoder.layer.11.intermediate.dense.bias False
pretrained_model.encoder.layer.11.output.dense.weight False
pretrained_model.encoder.layer.11.output.dense.bias False
pretrained_model.encoder.layer.11.output.LayerNorm.weight False
pretrained_model.encoder.layer.11.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.attention.self.query.weight False
pretrained_model.encoder.layer.12.attention.self.query.bias False
pretrained_model.encoder.layer.12.attention.self.key.weight False
pretrained_model.encoder.layer.12.attention.self.key.bias False
pretrained_model.encoder.layer.12.attention.self.value.weight False
pretrained_model.encoder.layer.12.attention.self.value.bias False
pretrained_model.encoder.layer.12.attention.output.dense.weight False
pretrained_model.encoder.layer.12.attention.output.dense.bias False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.12.intermediate.dense.weight False
pretrained_model.encoder.layer.12.intermediate.dense.bias False
pretrained_model.encoder.layer.12.output.dense.weight False
pretrained_model.encoder.layer.12.output.dense.bias False
pretrained_model.encoder.layer.12.output.LayerNorm.weight False
pretrained_model.encoder.layer.12.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.attention.self.query.weight False
pretrained_model.encoder.layer.13.attention.self.query.bias False
pretrained_model.encoder.layer.13.attention.self.key.weight False
pretrained_model.encoder.layer.13.attention.self.key.bias False
pretrained_model.encoder.layer.13.attention.self.value.weight False
pretrained_model.encoder.layer.13.attention.self.value.bias False
pretrained_model.encoder.layer.13.attention.output.dense.weight False
pretrained_model.encoder.layer.13.attention.output.dense.bias False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.13.intermediate.dense.weight False
pretrained_model.encoder.layer.13.intermediate.dense.bias False
pretrained_model.encoder.layer.13.output.dense.weight False
pretrained_model.encoder.layer.13.output.dense.bias False
pretrained_model.encoder.layer.13.output.LayerNorm.weight False
pretrained_model.encoder.layer.13.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.attention.self.query.weight False
pretrained_model.encoder.layer.14.attention.self.query.bias False
pretrained_model.encoder.layer.14.attention.self.key.weight False
pretrained_model.encoder.layer.14.attention.self.key.bias False
pretrained_model.encoder.layer.14.attention.self.value.weight False
pretrained_model.encoder.layer.14.attention.self.value.bias False
pretrained_model.encoder.layer.14.attention.output.dense.weight False
pretrained_model.encoder.layer.14.attention.output.dense.bias False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.14.intermediate.dense.weight False
pretrained_model.encoder.layer.14.intermediate.dense.bias False
pretrained_model.encoder.layer.14.output.dense.weight False
pretrained_model.encoder.layer.14.output.dense.bias False
pretrained_model.encoder.layer.14.output.LayerNorm.weight False
pretrained_model.encoder.layer.14.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.attention.self.query.weight False
pretrained_model.encoder.layer.15.attention.self.query.bias False
pretrained_model.encoder.layer.15.attention.self.key.weight False
pretrained_model.encoder.layer.15.attention.self.key.bias False
pretrained_model.encoder.layer.15.attention.self.value.weight False
pretrained_model.encoder.layer.15.attention.self.value.bias False
pretrained_model.encoder.layer.15.attention.output.dense.weight False
pretrained_model.encoder.layer.15.attention.output.dense.bias False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.15.intermediate.dense.weight False
pretrained_model.encoder.layer.15.intermediate.dense.bias False
pretrained_model.encoder.layer.15.output.dense.weight False
pretrained_model.encoder.layer.15.output.dense.bias False
pretrained_model.encoder.layer.15.output.LayerNorm.weight False
pretrained_model.encoder.layer.15.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.attention.self.query.weight False
pretrained_model.encoder.layer.16.attention.self.query.bias False
pretrained_model.encoder.layer.16.attention.self.key.weight False
pretrained_model.encoder.layer.16.attention.self.key.bias False
pretrained_model.encoder.layer.16.attention.self.value.weight False
pretrained_model.encoder.layer.16.attention.self.value.bias False
pretrained_model.encoder.layer.16.attention.output.dense.weight False
pretrained_model.encoder.layer.16.attention.output.dense.bias False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.16.intermediate.dense.weight False
pretrained_model.encoder.layer.16.intermediate.dense.bias False
pretrained_model.encoder.layer.16.output.dense.weight False
pretrained_model.encoder.layer.16.output.dense.bias False
pretrained_model.encoder.layer.16.output.LayerNorm.weight False
pretrained_model.encoder.layer.16.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.attention.self.query.weight False
pretrained_model.encoder.layer.17.attention.self.query.bias False
pretrained_model.encoder.layer.17.attention.self.key.weight False
pretrained_model.encoder.layer.17.attention.self.key.bias False
pretrained_model.encoder.layer.17.attention.self.value.weight False
pretrained_model.encoder.layer.17.attention.self.value.bias False
pretrained_model.encoder.layer.17.attention.output.dense.weight False
pretrained_model.encoder.layer.17.attention.output.dense.bias False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.17.intermediate.dense.weight False
pretrained_model.encoder.layer.17.intermediate.dense.bias False
pretrained_model.encoder.layer.17.output.dense.weight False
pretrained_model.encoder.layer.17.output.dense.bias False
pretrained_model.encoder.layer.17.output.LayerNorm.weight False
pretrained_model.encoder.layer.17.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.attention.self.query.weight False
pretrained_model.encoder.layer.18.attention.self.query.bias False
pretrained_model.encoder.layer.18.attention.self.key.weight False
pretrained_model.encoder.layer.18.attention.self.key.bias False
pretrained_model.encoder.layer.18.attention.self.value.weight False
pretrained_model.encoder.layer.18.attention.self.value.bias False
pretrained_model.encoder.layer.18.attention.output.dense.weight False
pretrained_model.encoder.layer.18.attention.output.dense.bias False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.18.intermediate.dense.weight False
pretrained_model.encoder.layer.18.intermediate.dense.bias False
pretrained_model.encoder.layer.18.output.dense.weight False
pretrained_model.encoder.layer.18.output.dense.bias False
pretrained_model.encoder.layer.18.output.LayerNorm.weight False
pretrained_model.encoder.layer.18.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.attention.self.query.weight False
pretrained_model.encoder.layer.19.attention.self.query.bias False
pretrained_model.encoder.layer.19.attention.self.key.weight False
pretrained_model.encoder.layer.19.attention.self.key.bias False
pretrained_model.encoder.layer.19.attention.self.value.weight False
pretrained_model.encoder.layer.19.attention.self.value.bias False
pretrained_model.encoder.layer.19.attention.output.dense.weight False
pretrained_model.encoder.layer.19.attention.output.dense.bias False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.19.intermediate.dense.weight False
pretrained_model.encoder.layer.19.intermediate.dense.bias False
pretrained_model.encoder.layer.19.output.dense.weight False
pretrained_model.encoder.layer.19.output.dense.bias False
pretrained_model.encoder.layer.19.output.LayerNorm.weight False
pretrained_model.encoder.layer.19.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.attention.self.query.weight False
pretrained_model.encoder.layer.20.attention.self.query.bias False
pretrained_model.encoder.layer.20.attention.self.key.weight False
pretrained_model.encoder.layer.20.attention.self.key.bias False
pretrained_model.encoder.layer.20.attention.self.value.weight False
pretrained_model.encoder.layer.20.attention.self.value.bias False
pretrained_model.encoder.layer.20.attention.output.dense.weight False
pretrained_model.encoder.layer.20.attention.output.dense.bias False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.20.intermediate.dense.weight False
pretrained_model.encoder.layer.20.intermediate.dense.bias False
pretrained_model.encoder.layer.20.output.dense.weight False
pretrained_model.encoder.layer.20.output.dense.bias False
pretrained_model.encoder.layer.20.output.LayerNorm.weight False
pretrained_model.encoder.layer.20.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.attention.self.query.weight False
pretrained_model.encoder.layer.21.attention.self.query.bias False
pretrained_model.encoder.layer.21.attention.self.key.weight False
pretrained_model.encoder.layer.21.attention.self.key.bias False
pretrained_model.encoder.layer.21.attention.self.value.weight False
pretrained_model.encoder.layer.21.attention.self.value.bias False
pretrained_model.encoder.layer.21.attention.output.dense.weight False
pretrained_model.encoder.layer.21.attention.output.dense.bias False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.21.intermediate.dense.weight False
pretrained_model.encoder.layer.21.intermediate.dense.bias False
pretrained_model.encoder.layer.21.output.dense.weight False
pretrained_model.encoder.layer.21.output.dense.bias False
pretrained_model.encoder.layer.21.output.LayerNorm.weight False
pretrained_model.encoder.layer.21.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.attention.self.query.weight False
pretrained_model.encoder.layer.22.attention.self.query.bias False
pretrained_model.encoder.layer.22.attention.self.key.weight False
pretrained_model.encoder.layer.22.attention.self.key.bias False
pretrained_model.encoder.layer.22.attention.self.value.weight False
pretrained_model.encoder.layer.22.attention.self.value.bias False
pretrained_model.encoder.layer.22.attention.output.dense.weight False
pretrained_model.encoder.layer.22.attention.output.dense.bias False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.22.intermediate.dense.weight False
pretrained_model.encoder.layer.22.intermediate.dense.bias False
pretrained_model.encoder.layer.22.output.dense.weight False
pretrained_model.encoder.layer.22.output.dense.bias False
pretrained_model.encoder.layer.22.output.LayerNorm.weight False
pretrained_model.encoder.layer.22.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.attention.self.query.weight False
pretrained_model.encoder.layer.23.attention.self.query.bias False
pretrained_model.encoder.layer.23.attention.self.key.weight False
pretrained_model.encoder.layer.23.attention.self.key.bias False
pretrained_model.encoder.layer.23.attention.self.value.weight False
pretrained_model.encoder.layer.23.attention.self.value.bias False
pretrained_model.encoder.layer.23.attention.output.dense.weight False
pretrained_model.encoder.layer.23.attention.output.dense.bias False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.23.intermediate.dense.weight False
pretrained_model.encoder.layer.23.intermediate.dense.bias False
pretrained_model.encoder.layer.23.output.dense.weight False
pretrained_model.encoder.layer.23.output.dense.bias False
pretrained_model.encoder.layer.23.output.LayerNorm.weight False
pretrained_model.encoder.layer.23.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 221s, train loss=2.2560, train acc=30.71%, dev loss=1.8157, dev acc=50.66%
saving, test loss=1.8176, test acc=50.65%
epoch: 2/10000, 223s, train loss=1.6864, train acc=50.95%, dev loss=1.5063, dev acc=54.42%
saving, test loss=1.5068, test acc=54.04%
epoch: 3/10000, 223s, train loss=1.4962, train acc=53.17%, dev loss=1.3981, dev acc=55.04%
saving, test loss=1.3981, test acc=54.97%
epoch: 4/10000, 223s, train loss=1.4187, train acc=54.23%, dev loss=1.3490, dev acc=55.42%
saving, test loss=1.3491, test acc=55.31%
epoch: 5/10000, 223s, train loss=1.3855, train acc=54.40%, dev loss=1.3208, dev acc=55.49%
saving, test loss=1.3211, test acc=55.49%
epoch: 6/10000, 223s, train loss=1.3620, train acc=55.02%, dev loss=1.3032, dev acc=55.64%
saving, test loss=1.3037, test acc=55.71%
epoch: 7/10000, 223s, train loss=1.3496, train acc=55.05%, dev loss=1.2914, dev acc=55.83%
saving, test loss=1.2925, test acc=55.89%
epoch: 8/10000, 224s, train loss=1.3325, train acc=55.11%, dev loss=1.2825, dev acc=56.07%
saving, test loss=1.2838, test acc=56.08%
epoch: 9/10000, 224s, train loss=1.3275, train acc=55.45%, dev loss=1.2754, dev acc=56.03%
epoch: 10/10000, 224s, train loss=1.3191, train acc=55.53%, dev loss=1.2705, dev acc=56.11%
saving, test loss=1.2722, test acc=56.02%
epoch: 11/10000, 224s, train loss=1.3142, train acc=55.88%, dev loss=1.2659, dev acc=56.13%
saving, test loss=1.2678, test acc=56.01%
epoch: 12/10000, 224s, train loss=1.3156, train acc=55.61%, dev loss=1.2620, dev acc=56.35%
saving, test loss=1.2645, test acc=55.90%
epoch: 13/10000, 225s, train loss=1.3093, train acc=55.95%, dev loss=1.2586, dev acc=56.41%
saving, test loss=1.2620, test acc=56.03%
epoch: 14/10000, 225s, train loss=1.3060, train acc=55.69%, dev loss=1.2561, dev acc=56.39%
epoch: 15/10000, 225s, train loss=1.3062, train acc=55.90%, dev loss=1.2534, dev acc=56.45%
saving, test loss=1.2569, test acc=56.02%
epoch: 16/10000, 225s, train loss=1.3017, train acc=55.97%, dev loss=1.2514, dev acc=56.39%
epoch: 17/10000, 225s, train loss=1.3004, train acc=56.14%, dev loss=1.2507, dev acc=56.47%
saving, test loss=1.2542, test acc=56.03%
epoch: 18/10000, 226s, train loss=1.2997, train acc=55.96%, dev loss=1.2494, dev acc=56.54%
saving, test loss=1.2529, test acc=56.13%
epoch: 19/10000, 226s, train loss=1.2979, train acc=56.04%, dev loss=1.2474, dev acc=56.82%
saving, test loss=1.2513, test acc=56.21%
epoch: 20/10000, 226s, train loss=1.2937, train acc=55.94%, dev loss=1.2465, dev acc=56.71%
epoch: 21/10000, 226s, train loss=1.2951, train acc=55.95%, dev loss=1.2463, dev acc=56.80%
epoch: 22/10000, 226s, train loss=1.2968, train acc=56.15%, dev loss=1.2456, dev acc=56.78%
epoch: 23/10000, 226s, train loss=1.2949, train acc=55.81%, dev loss=1.2449, dev acc=56.58%
epoch: 24/10000, 226s, train loss=1.2927, train acc=56.22%, dev loss=1.2433, dev acc=56.63%
time used=6725.8s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='2', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-xlnet-base', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_finance': 0, 'news_edu': 1, 'news_house': 2, 'news_world': 3, 'news_story': 4, 'news_game': 5, 'news_sports': 6, 'news_agriculture': 7, 'news_travel': 8, 'news_car': 9, 'news_culture': 10, 'news_stock': 11, 'news_tech': 12, 'news_entertainment': 13, 'news_military': 14}
index_labels_dict={0: 'news_finance', 1: 'news_edu', 2: 'news_house', 3: 'news_world', 4: 'news_story', 5: 'news_game', 6: 'news_sports', 7: 'news_agriculture', 8: 'news_travel', 9: 'news_car', 10: 'news_culture', 11: 'news_stock', 12: 'news_tech', 13: 'news_entertainment', 14: 'news_military'}
max_sent_len=105
105	1
54	1
50	1
47	1
46	2
44	1
43	2
42	3
41	5
40	4
39	14
38	20
37	32
36	96
35	57
34	83
33	145
32	257
31	391
30	625
29	940
28	1252
27	1696
26	1931
25	2243
24	2536
23	2580
22	2780
21	2785
20	2690
19	2937
18	2866
17	2657
16	2645
15	2418
14	2441
13	2155
12	2041
11	1791
10	1280
9	893
8	453
7	195
6	63
5	12
4	2
3	1
max_sent_len=40
max_sent_len=40
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-xlnet-base were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']
- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): XLNetModel(
    (word_embedding): Embedding(32000, 768)
    (layer): ModuleList(
      (0): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (6): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (7): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (8): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (9): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (10): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (11): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=15, bias=True)
)
pretrained_model.mask_emb False
pretrained_model.word_embedding.weight False
pretrained_model.layer.0.rel_attn.q False
pretrained_model.layer.0.rel_attn.k False
pretrained_model.layer.0.rel_attn.v False
pretrained_model.layer.0.rel_attn.o False
pretrained_model.layer.0.rel_attn.r False
pretrained_model.layer.0.rel_attn.r_r_bias False
pretrained_model.layer.0.rel_attn.r_s_bias False
pretrained_model.layer.0.rel_attn.r_w_bias False
pretrained_model.layer.0.rel_attn.seg_embed False
pretrained_model.layer.0.rel_attn.layer_norm.weight False
pretrained_model.layer.0.rel_attn.layer_norm.bias False
pretrained_model.layer.0.ff.layer_norm.weight False
pretrained_model.layer.0.ff.layer_norm.bias False
pretrained_model.layer.0.ff.layer_1.weight False
pretrained_model.layer.0.ff.layer_1.bias False
pretrained_model.layer.0.ff.layer_2.weight False
pretrained_model.layer.0.ff.layer_2.bias False
pretrained_model.layer.1.rel_attn.q False
pretrained_model.layer.1.rel_attn.k False
pretrained_model.layer.1.rel_attn.v False
pretrained_model.layer.1.rel_attn.o False
pretrained_model.layer.1.rel_attn.r False
pretrained_model.layer.1.rel_attn.r_r_bias False
pretrained_model.layer.1.rel_attn.r_s_bias False
pretrained_model.layer.1.rel_attn.r_w_bias False
pretrained_model.layer.1.rel_attn.seg_embed False
pretrained_model.layer.1.rel_attn.layer_norm.weight False
pretrained_model.layer.1.rel_attn.layer_norm.bias False
pretrained_model.layer.1.ff.layer_norm.weight False
pretrained_model.layer.1.ff.layer_norm.bias False
pretrained_model.layer.1.ff.layer_1.weight False
pretrained_model.layer.1.ff.layer_1.bias False
pretrained_model.layer.1.ff.layer_2.weight False
pretrained_model.layer.1.ff.layer_2.bias False
pretrained_model.layer.2.rel_attn.q False
pretrained_model.layer.2.rel_attn.k False
pretrained_model.layer.2.rel_attn.v False
pretrained_model.layer.2.rel_attn.o False
pretrained_model.layer.2.rel_attn.r False
pretrained_model.layer.2.rel_attn.r_r_bias False
pretrained_model.layer.2.rel_attn.r_s_bias False
pretrained_model.layer.2.rel_attn.r_w_bias False
pretrained_model.layer.2.rel_attn.seg_embed False
pretrained_model.layer.2.rel_attn.layer_norm.weight False
pretrained_model.layer.2.rel_attn.layer_norm.bias False
pretrained_model.layer.2.ff.layer_norm.weight False
pretrained_model.layer.2.ff.layer_norm.bias False
pretrained_model.layer.2.ff.layer_1.weight False
pretrained_model.layer.2.ff.layer_1.bias False
pretrained_model.layer.2.ff.layer_2.weight False
pretrained_model.layer.2.ff.layer_2.bias False
pretrained_model.layer.3.rel_attn.q False
pretrained_model.layer.3.rel_attn.k False
pretrained_model.layer.3.rel_attn.v False
pretrained_model.layer.3.rel_attn.o False
pretrained_model.layer.3.rel_attn.r False
pretrained_model.layer.3.rel_attn.r_r_bias False
pretrained_model.layer.3.rel_attn.r_s_bias False
pretrained_model.layer.3.rel_attn.r_w_bias False
pretrained_model.layer.3.rel_attn.seg_embed False
pretrained_model.layer.3.rel_attn.layer_norm.weight False
pretrained_model.layer.3.rel_attn.layer_norm.bias False
pretrained_model.layer.3.ff.layer_norm.weight False
pretrained_model.layer.3.ff.layer_norm.bias False
pretrained_model.layer.3.ff.layer_1.weight False
pretrained_model.layer.3.ff.layer_1.bias False
pretrained_model.layer.3.ff.layer_2.weight False
pretrained_model.layer.3.ff.layer_2.bias False
pretrained_model.layer.4.rel_attn.q False
pretrained_model.layer.4.rel_attn.k False
pretrained_model.layer.4.rel_attn.v False
pretrained_model.layer.4.rel_attn.o False
pretrained_model.layer.4.rel_attn.r False
pretrained_model.layer.4.rel_attn.r_r_bias False
pretrained_model.layer.4.rel_attn.r_s_bias False
pretrained_model.layer.4.rel_attn.r_w_bias False
pretrained_model.layer.4.rel_attn.seg_embed False
pretrained_model.layer.4.rel_attn.layer_norm.weight False
pretrained_model.layer.4.rel_attn.layer_norm.bias False
pretrained_model.layer.4.ff.layer_norm.weight False
pretrained_model.layer.4.ff.layer_norm.bias False
pretrained_model.layer.4.ff.layer_1.weight False
pretrained_model.layer.4.ff.layer_1.bias False
pretrained_model.layer.4.ff.layer_2.weight False
pretrained_model.layer.4.ff.layer_2.bias False
pretrained_model.layer.5.rel_attn.q False
pretrained_model.layer.5.rel_attn.k False
pretrained_model.layer.5.rel_attn.v False
pretrained_model.layer.5.rel_attn.o False
pretrained_model.layer.5.rel_attn.r False
pretrained_model.layer.5.rel_attn.r_r_bias False
pretrained_model.layer.5.rel_attn.r_s_bias False
pretrained_model.layer.5.rel_attn.r_w_bias False
pretrained_model.layer.5.rel_attn.seg_embed False
pretrained_model.layer.5.rel_attn.layer_norm.weight False
pretrained_model.layer.5.rel_attn.layer_norm.bias False
pretrained_model.layer.5.ff.layer_norm.weight False
pretrained_model.layer.5.ff.layer_norm.bias False
pretrained_model.layer.5.ff.layer_1.weight False
pretrained_model.layer.5.ff.layer_1.bias False
pretrained_model.layer.5.ff.layer_2.weight False
pretrained_model.layer.5.ff.layer_2.bias False
pretrained_model.layer.6.rel_attn.q False
pretrained_model.layer.6.rel_attn.k False
pretrained_model.layer.6.rel_attn.v False
pretrained_model.layer.6.rel_attn.o False
pretrained_model.layer.6.rel_attn.r False
pretrained_model.layer.6.rel_attn.r_r_bias False
pretrained_model.layer.6.rel_attn.r_s_bias False
pretrained_model.layer.6.rel_attn.r_w_bias False
pretrained_model.layer.6.rel_attn.seg_embed False
pretrained_model.layer.6.rel_attn.layer_norm.weight False
pretrained_model.layer.6.rel_attn.layer_norm.bias False
pretrained_model.layer.6.ff.layer_norm.weight False
pretrained_model.layer.6.ff.layer_norm.bias False
pretrained_model.layer.6.ff.layer_1.weight False
pretrained_model.layer.6.ff.layer_1.bias False
pretrained_model.layer.6.ff.layer_2.weight False
pretrained_model.layer.6.ff.layer_2.bias False
pretrained_model.layer.7.rel_attn.q False
pretrained_model.layer.7.rel_attn.k False
pretrained_model.layer.7.rel_attn.v False
pretrained_model.layer.7.rel_attn.o False
pretrained_model.layer.7.rel_attn.r False
pretrained_model.layer.7.rel_attn.r_r_bias False
pretrained_model.layer.7.rel_attn.r_s_bias False
pretrained_model.layer.7.rel_attn.r_w_bias False
pretrained_model.layer.7.rel_attn.seg_embed False
pretrained_model.layer.7.rel_attn.layer_norm.weight False
pretrained_model.layer.7.rel_attn.layer_norm.bias False
pretrained_model.layer.7.ff.layer_norm.weight False
pretrained_model.layer.7.ff.layer_norm.bias False
pretrained_model.layer.7.ff.layer_1.weight False
pretrained_model.layer.7.ff.layer_1.bias False
pretrained_model.layer.7.ff.layer_2.weight False
pretrained_model.layer.7.ff.layer_2.bias False
pretrained_model.layer.8.rel_attn.q False
pretrained_model.layer.8.rel_attn.k False
pretrained_model.layer.8.rel_attn.v False
pretrained_model.layer.8.rel_attn.o False
pretrained_model.layer.8.rel_attn.r False
pretrained_model.layer.8.rel_attn.r_r_bias False
pretrained_model.layer.8.rel_attn.r_s_bias False
pretrained_model.layer.8.rel_attn.r_w_bias False
pretrained_model.layer.8.rel_attn.seg_embed False
pretrained_model.layer.8.rel_attn.layer_norm.weight False
pretrained_model.layer.8.rel_attn.layer_norm.bias False
pretrained_model.layer.8.ff.layer_norm.weight False
pretrained_model.layer.8.ff.layer_norm.bias False
pretrained_model.layer.8.ff.layer_1.weight False
pretrained_model.layer.8.ff.layer_1.bias False
pretrained_model.layer.8.ff.layer_2.weight False
pretrained_model.layer.8.ff.layer_2.bias False
pretrained_model.layer.9.rel_attn.q False
pretrained_model.layer.9.rel_attn.k False
pretrained_model.layer.9.rel_attn.v False
pretrained_model.layer.9.rel_attn.o False
pretrained_model.layer.9.rel_attn.r False
pretrained_model.layer.9.rel_attn.r_r_bias False
pretrained_model.layer.9.rel_attn.r_s_bias False
pretrained_model.layer.9.rel_attn.r_w_bias False
pretrained_model.layer.9.rel_attn.seg_embed False
pretrained_model.layer.9.rel_attn.layer_norm.weight False
pretrained_model.layer.9.rel_attn.layer_norm.bias False
pretrained_model.layer.9.ff.layer_norm.weight False
pretrained_model.layer.9.ff.layer_norm.bias False
pretrained_model.layer.9.ff.layer_1.weight False
pretrained_model.layer.9.ff.layer_1.bias False
pretrained_model.layer.9.ff.layer_2.weight False
pretrained_model.layer.9.ff.layer_2.bias False
pretrained_model.layer.10.rel_attn.q False
pretrained_model.layer.10.rel_attn.k False
pretrained_model.layer.10.rel_attn.v False
pretrained_model.layer.10.rel_attn.o False
pretrained_model.layer.10.rel_attn.r False
pretrained_model.layer.10.rel_attn.r_r_bias False
pretrained_model.layer.10.rel_attn.r_s_bias False
pretrained_model.layer.10.rel_attn.r_w_bias False
pretrained_model.layer.10.rel_attn.seg_embed False
pretrained_model.layer.10.rel_attn.layer_norm.weight False
pretrained_model.layer.10.rel_attn.layer_norm.bias False
pretrained_model.layer.10.ff.layer_norm.weight False
pretrained_model.layer.10.ff.layer_norm.bias False
pretrained_model.layer.10.ff.layer_1.weight False
pretrained_model.layer.10.ff.layer_1.bias False
pretrained_model.layer.10.ff.layer_2.weight False
pretrained_model.layer.10.ff.layer_2.bias False
pretrained_model.layer.11.rel_attn.q False
pretrained_model.layer.11.rel_attn.k False
pretrained_model.layer.11.rel_attn.v False
pretrained_model.layer.11.rel_attn.o False
pretrained_model.layer.11.rel_attn.r False
pretrained_model.layer.11.rel_attn.r_r_bias False
pretrained_model.layer.11.rel_attn.r_s_bias False
pretrained_model.layer.11.rel_attn.r_w_bias False
pretrained_model.layer.11.rel_attn.seg_embed False
pretrained_model.layer.11.rel_attn.layer_norm.weight False
pretrained_model.layer.11.rel_attn.layer_norm.bias False
pretrained_model.layer.11.ff.layer_norm.weight False
pretrained_model.layer.11.ff.layer_norm.bias False
pretrained_model.layer.11.ff.layer_1.weight False
pretrained_model.layer.11.ff.layer_1.bias False
pretrained_model.layer.11.ff.layer_2.weight False
pretrained_model.layer.11.ff.layer_2.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 81s, train loss=2.5436, train acc=18.73%, dev loss=2.0718, dev acc=36.09%
saving, test loss=2.0713, test acc=36.91%
epoch: 2/10000, 81s, train loss=2.0744, train acc=34.52%, dev loss=1.8381, dev acc=43.48%
saving, test loss=1.8360, test acc=44.81%
epoch: 3/10000, 82s, train loss=1.9413, train acc=38.84%, dev loss=1.7630, dev acc=45.35%
saving, test loss=1.7585, test acc=46.39%
epoch: 4/10000, 82s, train loss=1.8875, train acc=40.67%, dev loss=1.7209, dev acc=45.82%
saving, test loss=1.7192, test acc=47.13%
epoch: 5/10000, 82s, train loss=1.8680, train acc=41.01%, dev loss=1.6970, dev acc=46.59%
saving, test loss=1.6976, test acc=47.35%
epoch: 6/10000, 82s, train loss=1.8428, train acc=41.76%, dev loss=1.6891, dev acc=46.91%
saving, test loss=1.6906, test acc=47.59%
epoch: 7/10000, 82s, train loss=1.8354, train acc=42.26%, dev loss=1.6753, dev acc=47.49%
saving, test loss=1.6768, test acc=47.77%
epoch: 8/10000, 82s, train loss=1.8315, train acc=42.03%, dev loss=1.6685, dev acc=47.40%
epoch: 9/10000, 82s, train loss=1.8271, train acc=42.38%, dev loss=1.6656, dev acc=47.54%
saving, test loss=1.6682, test acc=47.85%
epoch: 10/10000, 82s, train loss=1.8239, train acc=42.34%, dev loss=1.6624, dev acc=47.94%
saving, test loss=1.6665, test acc=47.89%
epoch: 11/10000, 82s, train loss=1.8231, train acc=42.20%, dev loss=1.6559, dev acc=47.84%
epoch: 12/10000, 82s, train loss=1.8170, train acc=42.26%, dev loss=1.6623, dev acc=47.98%
saving, test loss=1.6615, test acc=48.02%
epoch: 13/10000, 82s, train loss=1.8219, train acc=42.42%, dev loss=1.6578, dev acc=47.90%
epoch: 14/10000, 82s, train loss=1.8185, train acc=42.42%, dev loss=1.6545, dev acc=47.71%
epoch: 15/10000, 82s, train loss=1.8186, train acc=42.27%, dev loss=1.6527, dev acc=47.51%
epoch: 16/10000, 82s, train loss=1.8168, train acc=42.40%, dev loss=1.6527, dev acc=47.66%
epoch: 17/10000, 82s, train loss=1.8233, train acc=42.06%, dev loss=1.6528, dev acc=47.84%
time used=1718.5s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='2', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-xlnet-mid', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_story': 0, 'news_military': 1, 'news_entertainment': 2, 'news_agriculture': 3, 'news_house': 4, 'news_tech': 5, 'news_stock': 6, 'news_game': 7, 'news_car': 8, 'news_sports': 9, 'news_edu': 10, 'news_world': 11, 'news_finance': 12, 'news_travel': 13, 'news_culture': 14}
index_labels_dict={0: 'news_story', 1: 'news_military', 2: 'news_entertainment', 3: 'news_agriculture', 4: 'news_house', 5: 'news_tech', 6: 'news_stock', 7: 'news_game', 8: 'news_car', 9: 'news_sports', 10: 'news_edu', 11: 'news_world', 12: 'news_finance', 13: 'news_travel', 14: 'news_culture'}
max_sent_len=105
105	1
54	1
50	1
47	1
46	2
44	1
43	2
42	3
41	5
40	4
39	14
38	20
37	32
36	96
35	57
34	83
33	145
32	257
31	391
30	625
29	940
28	1252
27	1696
26	1931
25	2243
24	2536
23	2580
22	2780
21	2785
20	2690
19	2937
18	2866
17	2657
16	2645
15	2418
14	2441
13	2155
12	2041
11	1791
10	1280
9	893
8	453
7	195
6	63
5	12
4	2
3	1
max_sent_len=40
max_sent_len=40
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_chinese-xlnet-mid were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']
- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): XLNetModel(
    (word_embedding): Embedding(32000, 768)
    (layer): ModuleList(
      (0): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (6): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (7): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (8): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (9): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (10): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (11): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (12): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (13): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (14): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (15): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (16): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (17): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (18): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (19): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (20): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (21): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (22): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (23): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=768, out_features=3072, bias=True)
          (layer_2): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation_function): ReLU()
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=15, bias=True)
)
pretrained_model.mask_emb False
pretrained_model.word_embedding.weight False
pretrained_model.layer.0.rel_attn.q False
pretrained_model.layer.0.rel_attn.k False
pretrained_model.layer.0.rel_attn.v False
pretrained_model.layer.0.rel_attn.o False
pretrained_model.layer.0.rel_attn.r False
pretrained_model.layer.0.rel_attn.r_r_bias False
pretrained_model.layer.0.rel_attn.r_s_bias False
pretrained_model.layer.0.rel_attn.r_w_bias False
pretrained_model.layer.0.rel_attn.seg_embed False
pretrained_model.layer.0.rel_attn.layer_norm.weight False
pretrained_model.layer.0.rel_attn.layer_norm.bias False
pretrained_model.layer.0.ff.layer_norm.weight False
pretrained_model.layer.0.ff.layer_norm.bias False
pretrained_model.layer.0.ff.layer_1.weight False
pretrained_model.layer.0.ff.layer_1.bias False
pretrained_model.layer.0.ff.layer_2.weight False
pretrained_model.layer.0.ff.layer_2.bias False
pretrained_model.layer.1.rel_attn.q False
pretrained_model.layer.1.rel_attn.k False
pretrained_model.layer.1.rel_attn.v False
pretrained_model.layer.1.rel_attn.o False
pretrained_model.layer.1.rel_attn.r False
pretrained_model.layer.1.rel_attn.r_r_bias False
pretrained_model.layer.1.rel_attn.r_s_bias False
pretrained_model.layer.1.rel_attn.r_w_bias False
pretrained_model.layer.1.rel_attn.seg_embed False
pretrained_model.layer.1.rel_attn.layer_norm.weight False
pretrained_model.layer.1.rel_attn.layer_norm.bias False
pretrained_model.layer.1.ff.layer_norm.weight False
pretrained_model.layer.1.ff.layer_norm.bias False
pretrained_model.layer.1.ff.layer_1.weight False
pretrained_model.layer.1.ff.layer_1.bias False
pretrained_model.layer.1.ff.layer_2.weight False
pretrained_model.layer.1.ff.layer_2.bias False
pretrained_model.layer.2.rel_attn.q False
pretrained_model.layer.2.rel_attn.k False
pretrained_model.layer.2.rel_attn.v False
pretrained_model.layer.2.rel_attn.o False
pretrained_model.layer.2.rel_attn.r False
pretrained_model.layer.2.rel_attn.r_r_bias False
pretrained_model.layer.2.rel_attn.r_s_bias False
pretrained_model.layer.2.rel_attn.r_w_bias False
pretrained_model.layer.2.rel_attn.seg_embed False
pretrained_model.layer.2.rel_attn.layer_norm.weight False
pretrained_model.layer.2.rel_attn.layer_norm.bias False
pretrained_model.layer.2.ff.layer_norm.weight False
pretrained_model.layer.2.ff.layer_norm.bias False
pretrained_model.layer.2.ff.layer_1.weight False
pretrained_model.layer.2.ff.layer_1.bias False
pretrained_model.layer.2.ff.layer_2.weight False
pretrained_model.layer.2.ff.layer_2.bias False
pretrained_model.layer.3.rel_attn.q False
pretrained_model.layer.3.rel_attn.k False
pretrained_model.layer.3.rel_attn.v False
pretrained_model.layer.3.rel_attn.o False
pretrained_model.layer.3.rel_attn.r False
pretrained_model.layer.3.rel_attn.r_r_bias False
pretrained_model.layer.3.rel_attn.r_s_bias False
pretrained_model.layer.3.rel_attn.r_w_bias False
pretrained_model.layer.3.rel_attn.seg_embed False
pretrained_model.layer.3.rel_attn.layer_norm.weight False
pretrained_model.layer.3.rel_attn.layer_norm.bias False
pretrained_model.layer.3.ff.layer_norm.weight False
pretrained_model.layer.3.ff.layer_norm.bias False
pretrained_model.layer.3.ff.layer_1.weight False
pretrained_model.layer.3.ff.layer_1.bias False
pretrained_model.layer.3.ff.layer_2.weight False
pretrained_model.layer.3.ff.layer_2.bias False
pretrained_model.layer.4.rel_attn.q False
pretrained_model.layer.4.rel_attn.k False
pretrained_model.layer.4.rel_attn.v False
pretrained_model.layer.4.rel_attn.o False
pretrained_model.layer.4.rel_attn.r False
pretrained_model.layer.4.rel_attn.r_r_bias False
pretrained_model.layer.4.rel_attn.r_s_bias False
pretrained_model.layer.4.rel_attn.r_w_bias False
pretrained_model.layer.4.rel_attn.seg_embed False
pretrained_model.layer.4.rel_attn.layer_norm.weight False
pretrained_model.layer.4.rel_attn.layer_norm.bias False
pretrained_model.layer.4.ff.layer_norm.weight False
pretrained_model.layer.4.ff.layer_norm.bias False
pretrained_model.layer.4.ff.layer_1.weight False
pretrained_model.layer.4.ff.layer_1.bias False
pretrained_model.layer.4.ff.layer_2.weight False
pretrained_model.layer.4.ff.layer_2.bias False
pretrained_model.layer.5.rel_attn.q False
pretrained_model.layer.5.rel_attn.k False
pretrained_model.layer.5.rel_attn.v False
pretrained_model.layer.5.rel_attn.o False
pretrained_model.layer.5.rel_attn.r False
pretrained_model.layer.5.rel_attn.r_r_bias False
pretrained_model.layer.5.rel_attn.r_s_bias False
pretrained_model.layer.5.rel_attn.r_w_bias False
pretrained_model.layer.5.rel_attn.seg_embed False
pretrained_model.layer.5.rel_attn.layer_norm.weight False
pretrained_model.layer.5.rel_attn.layer_norm.bias False
pretrained_model.layer.5.ff.layer_norm.weight False
pretrained_model.layer.5.ff.layer_norm.bias False
pretrained_model.layer.5.ff.layer_1.weight False
pretrained_model.layer.5.ff.layer_1.bias False
pretrained_model.layer.5.ff.layer_2.weight False
pretrained_model.layer.5.ff.layer_2.bias False
pretrained_model.layer.6.rel_attn.q False
pretrained_model.layer.6.rel_attn.k False
pretrained_model.layer.6.rel_attn.v False
pretrained_model.layer.6.rel_attn.o False
pretrained_model.layer.6.rel_attn.r False
pretrained_model.layer.6.rel_attn.r_r_bias False
pretrained_model.layer.6.rel_attn.r_s_bias False
pretrained_model.layer.6.rel_attn.r_w_bias False
pretrained_model.layer.6.rel_attn.seg_embed False
pretrained_model.layer.6.rel_attn.layer_norm.weight False
pretrained_model.layer.6.rel_attn.layer_norm.bias False
pretrained_model.layer.6.ff.layer_norm.weight False
pretrained_model.layer.6.ff.layer_norm.bias False
pretrained_model.layer.6.ff.layer_1.weight False
pretrained_model.layer.6.ff.layer_1.bias False
pretrained_model.layer.6.ff.layer_2.weight False
pretrained_model.layer.6.ff.layer_2.bias False
pretrained_model.layer.7.rel_attn.q False
pretrained_model.layer.7.rel_attn.k False
pretrained_model.layer.7.rel_attn.v False
pretrained_model.layer.7.rel_attn.o False
pretrained_model.layer.7.rel_attn.r False
pretrained_model.layer.7.rel_attn.r_r_bias False
pretrained_model.layer.7.rel_attn.r_s_bias False
pretrained_model.layer.7.rel_attn.r_w_bias False
pretrained_model.layer.7.rel_attn.seg_embed False
pretrained_model.layer.7.rel_attn.layer_norm.weight False
pretrained_model.layer.7.rel_attn.layer_norm.bias False
pretrained_model.layer.7.ff.layer_norm.weight False
pretrained_model.layer.7.ff.layer_norm.bias False
pretrained_model.layer.7.ff.layer_1.weight False
pretrained_model.layer.7.ff.layer_1.bias False
pretrained_model.layer.7.ff.layer_2.weight False
pretrained_model.layer.7.ff.layer_2.bias False
pretrained_model.layer.8.rel_attn.q False
pretrained_model.layer.8.rel_attn.k False
pretrained_model.layer.8.rel_attn.v False
pretrained_model.layer.8.rel_attn.o False
pretrained_model.layer.8.rel_attn.r False
pretrained_model.layer.8.rel_attn.r_r_bias False
pretrained_model.layer.8.rel_attn.r_s_bias False
pretrained_model.layer.8.rel_attn.r_w_bias False
pretrained_model.layer.8.rel_attn.seg_embed False
pretrained_model.layer.8.rel_attn.layer_norm.weight False
pretrained_model.layer.8.rel_attn.layer_norm.bias False
pretrained_model.layer.8.ff.layer_norm.weight False
pretrained_model.layer.8.ff.layer_norm.bias False
pretrained_model.layer.8.ff.layer_1.weight False
pretrained_model.layer.8.ff.layer_1.bias False
pretrained_model.layer.8.ff.layer_2.weight False
pretrained_model.layer.8.ff.layer_2.bias False
pretrained_model.layer.9.rel_attn.q False
pretrained_model.layer.9.rel_attn.k False
pretrained_model.layer.9.rel_attn.v False
pretrained_model.layer.9.rel_attn.o False
pretrained_model.layer.9.rel_attn.r False
pretrained_model.layer.9.rel_attn.r_r_bias False
pretrained_model.layer.9.rel_attn.r_s_bias False
pretrained_model.layer.9.rel_attn.r_w_bias False
pretrained_model.layer.9.rel_attn.seg_embed False
pretrained_model.layer.9.rel_attn.layer_norm.weight False
pretrained_model.layer.9.rel_attn.layer_norm.bias False
pretrained_model.layer.9.ff.layer_norm.weight False
pretrained_model.layer.9.ff.layer_norm.bias False
pretrained_model.layer.9.ff.layer_1.weight False
pretrained_model.layer.9.ff.layer_1.bias False
pretrained_model.layer.9.ff.layer_2.weight False
pretrained_model.layer.9.ff.layer_2.bias False
pretrained_model.layer.10.rel_attn.q False
pretrained_model.layer.10.rel_attn.k False
pretrained_model.layer.10.rel_attn.v False
pretrained_model.layer.10.rel_attn.o False
pretrained_model.layer.10.rel_attn.r False
pretrained_model.layer.10.rel_attn.r_r_bias False
pretrained_model.layer.10.rel_attn.r_s_bias False
pretrained_model.layer.10.rel_attn.r_w_bias False
pretrained_model.layer.10.rel_attn.seg_embed False
pretrained_model.layer.10.rel_attn.layer_norm.weight False
pretrained_model.layer.10.rel_attn.layer_norm.bias False
pretrained_model.layer.10.ff.layer_norm.weight False
pretrained_model.layer.10.ff.layer_norm.bias False
pretrained_model.layer.10.ff.layer_1.weight False
pretrained_model.layer.10.ff.layer_1.bias False
pretrained_model.layer.10.ff.layer_2.weight False
pretrained_model.layer.10.ff.layer_2.bias False
pretrained_model.layer.11.rel_attn.q False
pretrained_model.layer.11.rel_attn.k False
pretrained_model.layer.11.rel_attn.v False
pretrained_model.layer.11.rel_attn.o False
pretrained_model.layer.11.rel_attn.r False
pretrained_model.layer.11.rel_attn.r_r_bias False
pretrained_model.layer.11.rel_attn.r_s_bias False
pretrained_model.layer.11.rel_attn.r_w_bias False
pretrained_model.layer.11.rel_attn.seg_embed False
pretrained_model.layer.11.rel_attn.layer_norm.weight False
pretrained_model.layer.11.rel_attn.layer_norm.bias False
pretrained_model.layer.11.ff.layer_norm.weight False
pretrained_model.layer.11.ff.layer_norm.bias False
pretrained_model.layer.11.ff.layer_1.weight False
pretrained_model.layer.11.ff.layer_1.bias False
pretrained_model.layer.11.ff.layer_2.weight False
pretrained_model.layer.11.ff.layer_2.bias False
pretrained_model.layer.12.rel_attn.q False
pretrained_model.layer.12.rel_attn.k False
pretrained_model.layer.12.rel_attn.v False
pretrained_model.layer.12.rel_attn.o False
pretrained_model.layer.12.rel_attn.r False
pretrained_model.layer.12.rel_attn.r_r_bias False
pretrained_model.layer.12.rel_attn.r_s_bias False
pretrained_model.layer.12.rel_attn.r_w_bias False
pretrained_model.layer.12.rel_attn.seg_embed False
pretrained_model.layer.12.rel_attn.layer_norm.weight False
pretrained_model.layer.12.rel_attn.layer_norm.bias False
pretrained_model.layer.12.ff.layer_norm.weight False
pretrained_model.layer.12.ff.layer_norm.bias False
pretrained_model.layer.12.ff.layer_1.weight False
pretrained_model.layer.12.ff.layer_1.bias False
pretrained_model.layer.12.ff.layer_2.weight False
pretrained_model.layer.12.ff.layer_2.bias False
pretrained_model.layer.13.rel_attn.q False
pretrained_model.layer.13.rel_attn.k False
pretrained_model.layer.13.rel_attn.v False
pretrained_model.layer.13.rel_attn.o False
pretrained_model.layer.13.rel_attn.r False
pretrained_model.layer.13.rel_attn.r_r_bias False
pretrained_model.layer.13.rel_attn.r_s_bias False
pretrained_model.layer.13.rel_attn.r_w_bias False
pretrained_model.layer.13.rel_attn.seg_embed False
pretrained_model.layer.13.rel_attn.layer_norm.weight False
pretrained_model.layer.13.rel_attn.layer_norm.bias False
pretrained_model.layer.13.ff.layer_norm.weight False
pretrained_model.layer.13.ff.layer_norm.bias False
pretrained_model.layer.13.ff.layer_1.weight False
pretrained_model.layer.13.ff.layer_1.bias False
pretrained_model.layer.13.ff.layer_2.weight False
pretrained_model.layer.13.ff.layer_2.bias False
pretrained_model.layer.14.rel_attn.q False
pretrained_model.layer.14.rel_attn.k False
pretrained_model.layer.14.rel_attn.v False
pretrained_model.layer.14.rel_attn.o False
pretrained_model.layer.14.rel_attn.r False
pretrained_model.layer.14.rel_attn.r_r_bias False
pretrained_model.layer.14.rel_attn.r_s_bias False
pretrained_model.layer.14.rel_attn.r_w_bias False
pretrained_model.layer.14.rel_attn.seg_embed False
pretrained_model.layer.14.rel_attn.layer_norm.weight False
pretrained_model.layer.14.rel_attn.layer_norm.bias False
pretrained_model.layer.14.ff.layer_norm.weight False
pretrained_model.layer.14.ff.layer_norm.bias False
pretrained_model.layer.14.ff.layer_1.weight False
pretrained_model.layer.14.ff.layer_1.bias False
pretrained_model.layer.14.ff.layer_2.weight False
pretrained_model.layer.14.ff.layer_2.bias False
pretrained_model.layer.15.rel_attn.q False
pretrained_model.layer.15.rel_attn.k False
pretrained_model.layer.15.rel_attn.v False
pretrained_model.layer.15.rel_attn.o False
pretrained_model.layer.15.rel_attn.r False
pretrained_model.layer.15.rel_attn.r_r_bias False
pretrained_model.layer.15.rel_attn.r_s_bias False
pretrained_model.layer.15.rel_attn.r_w_bias False
pretrained_model.layer.15.rel_attn.seg_embed False
pretrained_model.layer.15.rel_attn.layer_norm.weight False
pretrained_model.layer.15.rel_attn.layer_norm.bias False
pretrained_model.layer.15.ff.layer_norm.weight False
pretrained_model.layer.15.ff.layer_norm.bias False
pretrained_model.layer.15.ff.layer_1.weight False
pretrained_model.layer.15.ff.layer_1.bias False
pretrained_model.layer.15.ff.layer_2.weight False
pretrained_model.layer.15.ff.layer_2.bias False
pretrained_model.layer.16.rel_attn.q False
pretrained_model.layer.16.rel_attn.k False
pretrained_model.layer.16.rel_attn.v False
pretrained_model.layer.16.rel_attn.o False
pretrained_model.layer.16.rel_attn.r False
pretrained_model.layer.16.rel_attn.r_r_bias False
pretrained_model.layer.16.rel_attn.r_s_bias False
pretrained_model.layer.16.rel_attn.r_w_bias False
pretrained_model.layer.16.rel_attn.seg_embed False
pretrained_model.layer.16.rel_attn.layer_norm.weight False
pretrained_model.layer.16.rel_attn.layer_norm.bias False
pretrained_model.layer.16.ff.layer_norm.weight False
pretrained_model.layer.16.ff.layer_norm.bias False
pretrained_model.layer.16.ff.layer_1.weight False
pretrained_model.layer.16.ff.layer_1.bias False
pretrained_model.layer.16.ff.layer_2.weight False
pretrained_model.layer.16.ff.layer_2.bias False
pretrained_model.layer.17.rel_attn.q False
pretrained_model.layer.17.rel_attn.k False
pretrained_model.layer.17.rel_attn.v False
pretrained_model.layer.17.rel_attn.o False
pretrained_model.layer.17.rel_attn.r False
pretrained_model.layer.17.rel_attn.r_r_bias False
pretrained_model.layer.17.rel_attn.r_s_bias False
pretrained_model.layer.17.rel_attn.r_w_bias False
pretrained_model.layer.17.rel_attn.seg_embed False
pretrained_model.layer.17.rel_attn.layer_norm.weight False
pretrained_model.layer.17.rel_attn.layer_norm.bias False
pretrained_model.layer.17.ff.layer_norm.weight False
pretrained_model.layer.17.ff.layer_norm.bias False
pretrained_model.layer.17.ff.layer_1.weight False
pretrained_model.layer.17.ff.layer_1.bias False
pretrained_model.layer.17.ff.layer_2.weight False
pretrained_model.layer.17.ff.layer_2.bias False
pretrained_model.layer.18.rel_attn.q False
pretrained_model.layer.18.rel_attn.k False
pretrained_model.layer.18.rel_attn.v False
pretrained_model.layer.18.rel_attn.o False
pretrained_model.layer.18.rel_attn.r False
pretrained_model.layer.18.rel_attn.r_r_bias False
pretrained_model.layer.18.rel_attn.r_s_bias False
pretrained_model.layer.18.rel_attn.r_w_bias False
pretrained_model.layer.18.rel_attn.seg_embed False
pretrained_model.layer.18.rel_attn.layer_norm.weight False
pretrained_model.layer.18.rel_attn.layer_norm.bias False
pretrained_model.layer.18.ff.layer_norm.weight False
pretrained_model.layer.18.ff.layer_norm.bias False
pretrained_model.layer.18.ff.layer_1.weight False
pretrained_model.layer.18.ff.layer_1.bias False
pretrained_model.layer.18.ff.layer_2.weight False
pretrained_model.layer.18.ff.layer_2.bias False
pretrained_model.layer.19.rel_attn.q False
pretrained_model.layer.19.rel_attn.k False
pretrained_model.layer.19.rel_attn.v False
pretrained_model.layer.19.rel_attn.o False
pretrained_model.layer.19.rel_attn.r False
pretrained_model.layer.19.rel_attn.r_r_bias False
pretrained_model.layer.19.rel_attn.r_s_bias False
pretrained_model.layer.19.rel_attn.r_w_bias False
pretrained_model.layer.19.rel_attn.seg_embed False
pretrained_model.layer.19.rel_attn.layer_norm.weight False
pretrained_model.layer.19.rel_attn.layer_norm.bias False
pretrained_model.layer.19.ff.layer_norm.weight False
pretrained_model.layer.19.ff.layer_norm.bias False
pretrained_model.layer.19.ff.layer_1.weight False
pretrained_model.layer.19.ff.layer_1.bias False
pretrained_model.layer.19.ff.layer_2.weight False
pretrained_model.layer.19.ff.layer_2.bias False
pretrained_model.layer.20.rel_attn.q False
pretrained_model.layer.20.rel_attn.k False
pretrained_model.layer.20.rel_attn.v False
pretrained_model.layer.20.rel_attn.o False
pretrained_model.layer.20.rel_attn.r False
pretrained_model.layer.20.rel_attn.r_r_bias False
pretrained_model.layer.20.rel_attn.r_s_bias False
pretrained_model.layer.20.rel_attn.r_w_bias False
pretrained_model.layer.20.rel_attn.seg_embed False
pretrained_model.layer.20.rel_attn.layer_norm.weight False
pretrained_model.layer.20.rel_attn.layer_norm.bias False
pretrained_model.layer.20.ff.layer_norm.weight False
pretrained_model.layer.20.ff.layer_norm.bias False
pretrained_model.layer.20.ff.layer_1.weight False
pretrained_model.layer.20.ff.layer_1.bias False
pretrained_model.layer.20.ff.layer_2.weight False
pretrained_model.layer.20.ff.layer_2.bias False
pretrained_model.layer.21.rel_attn.q False
pretrained_model.layer.21.rel_attn.k False
pretrained_model.layer.21.rel_attn.v False
pretrained_model.layer.21.rel_attn.o False
pretrained_model.layer.21.rel_attn.r False
pretrained_model.layer.21.rel_attn.r_r_bias False
pretrained_model.layer.21.rel_attn.r_s_bias False
pretrained_model.layer.21.rel_attn.r_w_bias False
pretrained_model.layer.21.rel_attn.seg_embed False
pretrained_model.layer.21.rel_attn.layer_norm.weight False
pretrained_model.layer.21.rel_attn.layer_norm.bias False
pretrained_model.layer.21.ff.layer_norm.weight False
pretrained_model.layer.21.ff.layer_norm.bias False
pretrained_model.layer.21.ff.layer_1.weight False
pretrained_model.layer.21.ff.layer_1.bias False
pretrained_model.layer.21.ff.layer_2.weight False
pretrained_model.layer.21.ff.layer_2.bias False
pretrained_model.layer.22.rel_attn.q False
pretrained_model.layer.22.rel_attn.k False
pretrained_model.layer.22.rel_attn.v False
pretrained_model.layer.22.rel_attn.o False
pretrained_model.layer.22.rel_attn.r False
pretrained_model.layer.22.rel_attn.r_r_bias False
pretrained_model.layer.22.rel_attn.r_s_bias False
pretrained_model.layer.22.rel_attn.r_w_bias False
pretrained_model.layer.22.rel_attn.seg_embed False
pretrained_model.layer.22.rel_attn.layer_norm.weight False
pretrained_model.layer.22.rel_attn.layer_norm.bias False
pretrained_model.layer.22.ff.layer_norm.weight False
pretrained_model.layer.22.ff.layer_norm.bias False
pretrained_model.layer.22.ff.layer_1.weight False
pretrained_model.layer.22.ff.layer_1.bias False
pretrained_model.layer.22.ff.layer_2.weight False
pretrained_model.layer.22.ff.layer_2.bias False
pretrained_model.layer.23.rel_attn.q False
pretrained_model.layer.23.rel_attn.k False
pretrained_model.layer.23.rel_attn.v False
pretrained_model.layer.23.rel_attn.o False
pretrained_model.layer.23.rel_attn.r False
pretrained_model.layer.23.rel_attn.r_r_bias False
pretrained_model.layer.23.rel_attn.r_s_bias False
pretrained_model.layer.23.rel_attn.r_w_bias False
pretrained_model.layer.23.rel_attn.seg_embed False
pretrained_model.layer.23.rel_attn.layer_norm.weight False
pretrained_model.layer.23.rel_attn.layer_norm.bias False
pretrained_model.layer.23.ff.layer_norm.weight False
pretrained_model.layer.23.ff.layer_norm.bias False
pretrained_model.layer.23.ff.layer_1.weight False
pretrained_model.layer.23.ff.layer_1.bias False
pretrained_model.layer.23.ff.layer_2.weight False
pretrained_model.layer.23.ff.layer_2.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 157s, train loss=2.6297, train acc=15.84%, dev loss=2.2267, dev acc=28.15%
saving, test loss=2.2405, test acc=27.17%
epoch: 2/10000, 158s, train loss=2.2795, train acc=27.88%, dev loss=2.0167, dev acc=37.48%
saving, test loss=2.0284, test acc=35.91%
epoch: 3/10000, 158s, train loss=2.1600, train acc=32.47%, dev loss=1.9272, dev acc=40.61%
saving, test loss=1.9316, test acc=40.12%
epoch: 4/10000, 158s, train loss=2.1172, train acc=34.01%, dev loss=1.8877, dev acc=42.07%
saving, test loss=1.8922, test acc=41.19%
epoch: 5/10000, 158s, train loss=2.0915, train acc=34.86%, dev loss=1.8655, dev acc=43.37%
saving, test loss=1.8694, test acc=42.01%
epoch: 6/10000, 159s, train loss=2.0722, train acc=35.43%, dev loss=1.8483, dev acc=43.38%
saving, test loss=1.8533, test acc=42.43%
epoch: 7/10000, 159s, train loss=2.0630, train acc=35.40%, dev loss=1.8384, dev acc=43.65%
saving, test loss=1.8448, test acc=42.23%
epoch: 8/10000, 159s, train loss=2.0623, train acc=35.78%, dev loss=1.8305, dev acc=43.18%
epoch: 9/10000, 159s, train loss=2.0531, train acc=35.75%, dev loss=1.8222, dev acc=43.50%
epoch: 10/10000, 159s, train loss=2.0526, train acc=36.09%, dev loss=1.8179, dev acc=44.43%
saving, test loss=1.8269, test acc=43.17%
epoch: 11/10000, 159s, train loss=2.0518, train acc=36.07%, dev loss=1.8172, dev acc=44.12%
epoch: 12/10000, 159s, train loss=2.0509, train acc=36.10%, dev loss=1.8139, dev acc=44.32%
epoch: 13/10000, 159s, train loss=2.0438, train acc=36.27%, dev loss=1.8078, dev acc=44.08%
epoch: 14/10000, 159s, train loss=2.0396, train acc=36.36%, dev loss=1.8100, dev acc=44.40%
epoch: 15/10000, 159s, train loss=2.0443, train acc=36.32%, dev loss=1.8107, dev acc=44.08%
time used=2898.2s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='2', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_rbt3', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_story': 0, 'news_tech': 1, 'news_travel': 2, 'news_stock': 3, 'news_finance': 4, 'news_game': 5, 'news_sports': 6, 'news_world': 7, 'news_car': 8, 'news_military': 9, 'news_house': 10, 'news_culture': 11, 'news_edu': 12, 'news_agriculture': 13, 'news_entertainment': 14}
index_labels_dict={0: 'news_story', 1: 'news_tech', 2: 'news_travel', 3: 'news_stock', 4: 'news_finance', 5: 'news_game', 6: 'news_sports', 7: 'news_world', 8: 'news_car', 9: 'news_military', 10: 'news_house', 11: 'news_culture', 12: 'news_edu', 13: 'news_agriculture', 14: 'news_entertainment'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_rbt3 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 18s, train loss=2.4772, train acc=20.31%, dev loss=2.2088, dev acc=38.12%
saving, test loss=2.2148, test acc=37.11%
epoch: 2/10000, 18s, train loss=2.0995, train acc=41.17%, dev loss=1.9174, dev acc=46.03%
saving, test loss=1.9244, test acc=46.23%
epoch: 3/10000, 18s, train loss=1.8810, train acc=46.08%, dev loss=1.7442, dev acc=48.43%
saving, test loss=1.7516, test acc=48.74%
epoch: 4/10000, 18s, train loss=1.7534, train acc=47.95%, dev loss=1.6419, dev acc=49.40%
saving, test loss=1.6499, test acc=50.07%
epoch: 5/10000, 18s, train loss=1.6764, train acc=48.75%, dev loss=1.5780, dev acc=50.79%
saving, test loss=1.5869, test acc=50.68%
epoch: 6/10000, 18s, train loss=1.6232, train acc=49.46%, dev loss=1.5365, dev acc=51.07%
saving, test loss=1.5462, test acc=50.85%
epoch: 7/10000, 18s, train loss=1.5933, train acc=49.77%, dev loss=1.5076, dev acc=51.24%
saving, test loss=1.5181, test acc=51.25%
epoch: 8/10000, 18s, train loss=1.5678, train acc=50.05%, dev loss=1.4867, dev acc=51.26%
saving, test loss=1.4977, test acc=51.41%
epoch: 9/10000, 18s, train loss=1.5517, train acc=50.01%, dev loss=1.4706, dev acc=51.61%
saving, test loss=1.4823, test acc=51.73%
epoch: 10/10000, 18s, train loss=1.5359, train acc=50.47%, dev loss=1.4583, dev acc=51.71%
saving, test loss=1.4705, test acc=51.88%
epoch: 11/10000, 18s, train loss=1.5268, train acc=50.32%, dev loss=1.4485, dev acc=51.87%
saving, test loss=1.4612, test acc=52.01%
epoch: 12/10000, 18s, train loss=1.5192, train acc=50.52%, dev loss=1.4407, dev acc=51.72%
epoch: 13/10000, 18s, train loss=1.5126, train acc=50.75%, dev loss=1.4340, dev acc=51.87%
epoch: 14/10000, 18s, train loss=1.5050, train acc=50.83%, dev loss=1.4286, dev acc=51.80%
epoch: 15/10000, 18s, train loss=1.5001, train acc=50.79%, dev loss=1.4235, dev acc=51.71%
epoch: 16/10000, 18s, train loss=1.4981, train acc=50.82%, dev loss=1.4198, dev acc=51.67%
time used=381.6s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='2', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_rbt4', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_car': 0, 'news_travel': 1, 'news_game': 2, 'news_entertainment': 3, 'news_edu': 4, 'news_military': 5, 'news_stock': 6, 'news_sports': 7, 'news_house': 8, 'news_agriculture': 9, 'news_culture': 10, 'news_world': 11, 'news_finance': 12, 'news_story': 13, 'news_tech': 14}
index_labels_dict={0: 'news_car', 1: 'news_travel', 2: 'news_game', 3: 'news_entertainment', 4: 'news_edu', 5: 'news_military', 6: 'news_stock', 7: 'news_sports', 8: 'news_house', 9: 'news_agriculture', 10: 'news_culture', 11: 'news_world', 12: 'news_finance', 13: 'news_story', 14: 'news_tech'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_rbt4 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 23s, train loss=2.4715, train acc=22.37%, dev loss=2.2389, dev acc=41.42%
saving, test loss=2.2446, test acc=41.29%
epoch: 2/10000, 23s, train loss=2.1440, train acc=41.29%, dev loss=1.9590, dev acc=46.96%
saving, test loss=1.9661, test acc=47.22%
epoch: 3/10000, 24s, train loss=1.9283, train acc=46.25%, dev loss=1.7773, dev acc=49.51%
saving, test loss=1.7851, test acc=49.55%
epoch: 4/10000, 24s, train loss=1.7911, train acc=48.32%, dev loss=1.6627, dev acc=50.75%
saving, test loss=1.6700, test acc=50.68%
epoch: 5/10000, 24s, train loss=1.7055, train acc=48.94%, dev loss=1.5892, dev acc=51.69%
saving, test loss=1.5960, test acc=51.44%
epoch: 6/10000, 24s, train loss=1.6432, train acc=49.62%, dev loss=1.5402, dev acc=52.14%
saving, test loss=1.5465, test acc=52.11%
epoch: 7/10000, 24s, train loss=1.6054, train acc=49.90%, dev loss=1.5061, dev acc=52.08%
epoch: 8/10000, 24s, train loss=1.5764, train acc=50.45%, dev loss=1.4815, dev acc=52.23%
saving, test loss=1.4866, test acc=52.52%
epoch: 9/10000, 24s, train loss=1.5548, train acc=50.62%, dev loss=1.4627, dev acc=52.44%
saving, test loss=1.4678, test acc=52.72%
epoch: 10/10000, 24s, train loss=1.5393, train acc=50.67%, dev loss=1.4484, dev acc=52.75%
saving, test loss=1.4533, test acc=52.84%
epoch: 11/10000, 24s, train loss=1.5253, train acc=50.90%, dev loss=1.4372, dev acc=52.83%
saving, test loss=1.4420, test acc=53.05%
epoch: 12/10000, 24s, train loss=1.5146, train acc=50.93%, dev loss=1.4280, dev acc=52.90%
saving, test loss=1.4327, test acc=53.04%
epoch: 13/10000, 24s, train loss=1.5020, train acc=51.20%, dev loss=1.4202, dev acc=53.19%
saving, test loss=1.4253, test acc=52.98%
epoch: 14/10000, 24s, train loss=1.5003, train acc=51.25%, dev loss=1.4140, dev acc=53.05%
epoch: 15/10000, 24s, train loss=1.4954, train acc=51.20%, dev loss=1.4086, dev acc=53.13%
epoch: 16/10000, 24s, train loss=1.4890, train acc=51.20%, dev loss=1.4039, dev acc=52.96%
epoch: 17/10000, 24s, train loss=1.4858, train acc=51.36%, dev loss=1.3999, dev acc=52.92%
epoch: 18/10000, 24s, train loss=1.4812, train acc=51.31%, dev loss=1.3967, dev acc=52.96%
time used=555.8s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='2', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_rbt6', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_military': 0, 'news_stock': 1, 'news_agriculture': 2, 'news_tech': 3, 'news_car': 4, 'news_story': 5, 'news_culture': 6, 'news_world': 7, 'news_house': 8, 'news_travel': 9, 'news_entertainment': 10, 'news_sports': 11, 'news_edu': 12, 'news_game': 13, 'news_finance': 14}
index_labels_dict={0: 'news_military', 1: 'news_stock', 2: 'news_agriculture', 3: 'news_tech', 4: 'news_car', 5: 'news_story', 6: 'news_culture', 7: 'news_world', 8: 'news_house', 9: 'news_travel', 10: 'news_entertainment', 11: 'news_sports', 12: 'news_edu', 13: 'news_game', 14: 'news_finance'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_rbt6 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.attention.self.query.weight False
pretrained_model.encoder.layer.3.attention.self.query.bias False
pretrained_model.encoder.layer.3.attention.self.key.weight False
pretrained_model.encoder.layer.3.attention.self.key.bias False
pretrained_model.encoder.layer.3.attention.self.value.weight False
pretrained_model.encoder.layer.3.attention.self.value.bias False
pretrained_model.encoder.layer.3.attention.output.dense.weight False
pretrained_model.encoder.layer.3.attention.output.dense.bias False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.3.intermediate.dense.weight False
pretrained_model.encoder.layer.3.intermediate.dense.bias False
pretrained_model.encoder.layer.3.output.dense.weight False
pretrained_model.encoder.layer.3.output.dense.bias False
pretrained_model.encoder.layer.3.output.LayerNorm.weight False
pretrained_model.encoder.layer.3.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.attention.self.query.weight False
pretrained_model.encoder.layer.4.attention.self.query.bias False
pretrained_model.encoder.layer.4.attention.self.key.weight False
pretrained_model.encoder.layer.4.attention.self.key.bias False
pretrained_model.encoder.layer.4.attention.self.value.weight False
pretrained_model.encoder.layer.4.attention.self.value.bias False
pretrained_model.encoder.layer.4.attention.output.dense.weight False
pretrained_model.encoder.layer.4.attention.output.dense.bias False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.4.intermediate.dense.weight False
pretrained_model.encoder.layer.4.intermediate.dense.bias False
pretrained_model.encoder.layer.4.output.dense.weight False
pretrained_model.encoder.layer.4.output.dense.bias False
pretrained_model.encoder.layer.4.output.LayerNorm.weight False
pretrained_model.encoder.layer.4.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.attention.self.query.weight False
pretrained_model.encoder.layer.5.attention.self.query.bias False
pretrained_model.encoder.layer.5.attention.self.key.weight False
pretrained_model.encoder.layer.5.attention.self.key.bias False
pretrained_model.encoder.layer.5.attention.self.value.weight False
pretrained_model.encoder.layer.5.attention.self.value.bias False
pretrained_model.encoder.layer.5.attention.output.dense.weight False
pretrained_model.encoder.layer.5.attention.output.dense.bias False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.5.intermediate.dense.weight False
pretrained_model.encoder.layer.5.intermediate.dense.bias False
pretrained_model.encoder.layer.5.output.dense.weight False
pretrained_model.encoder.layer.5.output.dense.bias False
pretrained_model.encoder.layer.5.output.LayerNorm.weight False
pretrained_model.encoder.layer.5.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 35s, train loss=2.4752, train acc=22.07%, dev loss=2.2422, dev acc=37.74%
saving, test loss=2.2497, test acc=37.48%
epoch: 2/10000, 35s, train loss=2.1464, train acc=40.62%, dev loss=1.9628, dev acc=46.18%
saving, test loss=1.9708, test acc=46.63%
epoch: 3/10000, 35s, train loss=1.9296, train acc=45.84%, dev loss=1.7799, dev acc=49.16%
saving, test loss=1.7884, test acc=49.30%
epoch: 4/10000, 35s, train loss=1.7868, train acc=48.09%, dev loss=1.6618, dev acc=50.66%
saving, test loss=1.6698, test acc=50.52%
epoch: 5/10000, 35s, train loss=1.6994, train acc=49.06%, dev loss=1.5845, dev acc=51.54%
saving, test loss=1.5918, test acc=51.23%
epoch: 6/10000, 35s, train loss=1.6409, train acc=49.79%, dev loss=1.5321, dev acc=51.63%
saving, test loss=1.5384, test acc=51.98%
epoch: 7/10000, 35s, train loss=1.5954, train acc=50.50%, dev loss=1.4955, dev acc=52.01%
saving, test loss=1.5010, test acc=52.50%
epoch: 8/10000, 35s, train loss=1.5632, train acc=50.83%, dev loss=1.4690, dev acc=52.01%
epoch: 9/10000, 35s, train loss=1.5383, train acc=51.14%, dev loss=1.4491, dev acc=52.21%
saving, test loss=1.4535, test acc=52.84%
epoch: 10/10000, 35s, train loss=1.5241, train acc=51.28%, dev loss=1.4333, dev acc=52.31%
saving, test loss=1.4374, test acc=53.02%
epoch: 11/10000, 35s, train loss=1.5081, train acc=51.46%, dev loss=1.4213, dev acc=52.55%
saving, test loss=1.4250, test acc=53.01%
epoch: 12/10000, 35s, train loss=1.4995, train acc=51.38%, dev loss=1.4112, dev acc=52.60%
saving, test loss=1.4148, test acc=53.14%
epoch: 13/10000, 35s, train loss=1.4889, train acc=51.72%, dev loss=1.4029, dev acc=52.81%
saving, test loss=1.4067, test acc=53.32%
epoch: 14/10000, 35s, train loss=1.4866, train acc=51.76%, dev loss=1.3967, dev acc=52.83%
saving, test loss=1.4000, test acc=53.44%
epoch: 15/10000, 36s, train loss=1.4779, train acc=51.90%, dev loss=1.3909, dev acc=52.90%
saving, test loss=1.3943, test acc=53.58%
epoch: 16/10000, 36s, train loss=1.4733, train acc=51.64%, dev loss=1.3861, dev acc=52.90%
epoch: 17/10000, 36s, train loss=1.4676, train acc=51.86%, dev loss=1.3819, dev acc=52.87%
epoch: 18/10000, 36s, train loss=1.4637, train acc=51.84%, dev loss=1.3781, dev acc=52.94%
saving, test loss=1.3814, test acc=53.75%
epoch: 19/10000, 36s, train loss=1.4569, train acc=52.04%, dev loss=1.3752, dev acc=52.92%
epoch: 20/10000, 36s, train loss=1.4581, train acc=52.00%, dev loss=1.3720, dev acc=53.17%
saving, test loss=1.3750, test acc=53.81%
epoch: 21/10000, 36s, train loss=1.4579, train acc=52.12%, dev loss=1.3696, dev acc=52.98%
epoch: 22/10000, 36s, train loss=1.4470, train acc=52.07%, dev loss=1.3667, dev acc=53.05%
epoch: 23/10000, 36s, train loss=1.4488, train acc=52.18%, dev loss=1.3647, dev acc=53.04%
epoch: 24/10000, 36s, train loss=1.4508, train acc=52.01%, dev loss=1.3634, dev acc=53.02%
epoch: 25/10000, 36s, train loss=1.4458, train acc=52.20%, dev loss=1.3612, dev acc=53.02%
time used=1126.9s
Namespace(batch_size=2048, dropout=0.5, epochs=10000, freeze=True, gpu='2', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_name='/data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_rbtl3', patience=5, seed=2022)
device=cuda:0
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_car': 0, 'news_finance': 1, 'news_sports': 2, 'news_travel': 3, 'news_world': 4, 'news_stock': 5, 'news_military': 6, 'news_edu': 7, 'news_entertainment': 8, 'news_house': 9, 'news_tech': 10, 'news_game': 11, 'news_culture': 12, 'news_story': 13, 'news_agriculture': 14}
index_labels_dict={0: 'news_car', 1: 'news_finance', 2: 'news_sports', 3: 'news_travel', 4: 'news_world', 5: 'news_stock', 6: 'news_military', 7: 'news_edu', 8: 'news_entertainment', 9: 'news_house', 10: 'news_tech', 11: 'news_game', 12: 'news_culture', 13: 'news_story', 14: 'news_agriculture'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
max_sent_len=50
Some weights of the model checkpoint at /data0/nfs_data/zhaoxi9/pretrained_language_model/huggingface_pretrained_models/hfl_rbtl3 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
CustomModel(
  (pretrained_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=1024, out_features=1024, bias=True)
      (activation): Tanh()
    )
  )
  (fc_dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=1024, out_features=15, bias=True)
)
pretrained_model.embeddings.word_embeddings.weight False
pretrained_model.embeddings.position_embeddings.weight False
pretrained_model.embeddings.token_type_embeddings.weight False
pretrained_model.embeddings.LayerNorm.weight False
pretrained_model.embeddings.LayerNorm.bias False
pretrained_model.encoder.layer.0.attention.self.query.weight False
pretrained_model.encoder.layer.0.attention.self.query.bias False
pretrained_model.encoder.layer.0.attention.self.key.weight False
pretrained_model.encoder.layer.0.attention.self.key.bias False
pretrained_model.encoder.layer.0.attention.self.value.weight False
pretrained_model.encoder.layer.0.attention.self.value.bias False
pretrained_model.encoder.layer.0.attention.output.dense.weight False
pretrained_model.encoder.layer.0.attention.output.dense.bias False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.0.intermediate.dense.weight False
pretrained_model.encoder.layer.0.intermediate.dense.bias False
pretrained_model.encoder.layer.0.output.dense.weight False
pretrained_model.encoder.layer.0.output.dense.bias False
pretrained_model.encoder.layer.0.output.LayerNorm.weight False
pretrained_model.encoder.layer.0.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.attention.self.query.weight False
pretrained_model.encoder.layer.1.attention.self.query.bias False
pretrained_model.encoder.layer.1.attention.self.key.weight False
pretrained_model.encoder.layer.1.attention.self.key.bias False
pretrained_model.encoder.layer.1.attention.self.value.weight False
pretrained_model.encoder.layer.1.attention.self.value.bias False
pretrained_model.encoder.layer.1.attention.output.dense.weight False
pretrained_model.encoder.layer.1.attention.output.dense.bias False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.1.intermediate.dense.weight False
pretrained_model.encoder.layer.1.intermediate.dense.bias False
pretrained_model.encoder.layer.1.output.dense.weight False
pretrained_model.encoder.layer.1.output.dense.bias False
pretrained_model.encoder.layer.1.output.LayerNorm.weight False
pretrained_model.encoder.layer.1.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.attention.self.query.weight False
pretrained_model.encoder.layer.2.attention.self.query.bias False
pretrained_model.encoder.layer.2.attention.self.key.weight False
pretrained_model.encoder.layer.2.attention.self.key.bias False
pretrained_model.encoder.layer.2.attention.self.value.weight False
pretrained_model.encoder.layer.2.attention.self.value.bias False
pretrained_model.encoder.layer.2.attention.output.dense.weight False
pretrained_model.encoder.layer.2.attention.output.dense.bias False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias False
pretrained_model.encoder.layer.2.intermediate.dense.weight False
pretrained_model.encoder.layer.2.intermediate.dense.bias False
pretrained_model.encoder.layer.2.output.dense.weight False
pretrained_model.encoder.layer.2.output.dense.bias False
pretrained_model.encoder.layer.2.output.LayerNorm.weight False
pretrained_model.encoder.layer.2.output.LayerNorm.bias False
pretrained_model.pooler.dense.weight False
pretrained_model.pooler.dense.bias False
fc.weight True
fc.bias True
epoch: 1/10000, 28s, train loss=2.3901, train acc=25.96%, dev loss=2.0552, dev acc=42.75%
saving, test loss=2.0651, test acc=43.19%
epoch: 2/10000, 28s, train loss=1.9353, train acc=44.58%, dev loss=1.7524, dev acc=47.84%
saving, test loss=1.7631, test acc=48.68%
epoch: 3/10000, 28s, train loss=1.7267, train acc=48.16%, dev loss=1.6106, dev acc=49.93%
saving, test loss=1.6212, test acc=50.68%
epoch: 4/10000, 28s, train loss=1.6249, train acc=49.43%, dev loss=1.5379, dev acc=50.49%
saving, test loss=1.5481, test acc=51.66%
epoch: 5/10000, 28s, train loss=1.5724, train acc=49.95%, dev loss=1.4963, dev acc=50.99%
saving, test loss=1.5071, test acc=52.04%
epoch: 6/10000, 28s, train loss=1.5384, train acc=50.41%, dev loss=1.4700, dev acc=51.20%
saving, test loss=1.4811, test acc=52.24%
epoch: 7/10000, 28s, train loss=1.5163, train acc=50.91%, dev loss=1.4518, dev acc=51.57%
saving, test loss=1.4632, test acc=52.51%
epoch: 8/10000, 28s, train loss=1.4997, train acc=51.14%, dev loss=1.4384, dev acc=51.99%
saving, test loss=1.4503, test acc=52.63%
epoch: 9/10000, 28s, train loss=1.4925, train acc=51.10%, dev loss=1.4285, dev acc=52.17%
saving, test loss=1.4410, test acc=52.63%
epoch: 10/10000, 28s, train loss=1.4820, train acc=51.44%, dev loss=1.4193, dev acc=52.31%
saving, test loss=1.4328, test acc=52.85%
epoch: 11/10000, 28s, train loss=1.4744, train acc=51.56%, dev loss=1.4126, dev acc=52.40%
saving, test loss=1.4267, test acc=53.10%
epoch: 12/10000, 28s, train loss=1.4685, train acc=52.00%, dev loss=1.4074, dev acc=52.42%
saving, test loss=1.4216, test acc=53.15%
epoch: 13/10000, 28s, train loss=1.4635, train acc=52.07%, dev loss=1.4026, dev acc=52.51%
saving, test loss=1.4176, test acc=53.11%
epoch: 14/10000, 28s, train loss=1.4600, train acc=51.91%, dev loss=1.3989, dev acc=52.38%
epoch: 15/10000, 28s, train loss=1.4559, train acc=51.97%, dev loss=1.3952, dev acc=52.47%
epoch: 16/10000, 28s, train loss=1.4533, train acc=52.11%, dev loss=1.3924, dev acc=52.55%
saving, test loss=1.4088, test acc=53.23%
epoch: 17/10000, 28s, train loss=1.4524, train acc=51.96%, dev loss=1.3894, dev acc=52.72%
saving, test loss=1.4062, test acc=53.31%
epoch: 18/10000, 28s, train loss=1.4529, train acc=51.98%, dev loss=1.3868, dev acc=52.87%
saving, test loss=1.4042, test acc=53.45%
epoch: 19/10000, 28s, train loss=1.4484, train acc=51.93%, dev loss=1.3850, dev acc=52.98%
saving, test loss=1.4022, test acc=53.50%
epoch: 20/10000, 28s, train loss=1.4464, train acc=51.89%, dev loss=1.3830, dev acc=53.15%
saving, test loss=1.4001, test acc=53.39%
epoch: 21/10000, 28s, train loss=1.4454, train acc=52.12%, dev loss=1.3811, dev acc=53.00%
epoch: 22/10000, 28s, train loss=1.4443, train acc=52.37%, dev loss=1.3802, dev acc=53.04%
epoch: 23/10000, 28s, train loss=1.4435, train acc=52.20%, dev loss=1.3782, dev acc=53.04%
epoch: 24/10000, 28s, train loss=1.4393, train acc=52.21%, dev loss=1.3768, dev acc=53.11%
epoch: 25/10000, 28s, train loss=1.4404, train acc=52.22%, dev loss=1.3762, dev acc=53.07%
time used=916.3s
