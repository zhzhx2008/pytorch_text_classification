nohup: ignoring input
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert/albert_base/model.ckpt-best', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert/albert_base/albert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='albert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert/albert_base/vocab_chinese.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_sports': 0, 'news_military': 1, 'news_culture': 2, 'news_travel': 3, 'news_stock': 4, 'news_world': 5, 'news_entertainment': 6, 'news_story': 7, 'news_finance': 8, 'news_edu': 9, 'news_house': 10, 'news_car': 11, 'news_tech': 12, 'news_agriculture': 13, 'news_game': 14}
index_labels_dict={0: 'news_sports', 1: 'news_military', 2: 'news_culture', 3: 'news_travel', 4: 'news_stock', 5: 'news_world', 6: 'news_entertainment', 7: 'news_story', 8: 'news_finance', 9: 'news_edu', 10: 'news_house', 11: 'news_car', 12: 'news_tech', 13: 'news_agriculture', 14: 'news_game'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-11 00:18:23.457034: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-11 00:18:24.780735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 128)    2704384     Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Mapping (Dense)       (None, None, 768)    99072       Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 768)    2362368     Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-FeedForward-Norm[10][
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 768)    0           Embedding-Mapping[0][0]          
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 768)    1536        Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-FeedForward (FeedFo (None, None, 768)    4722432     Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-FeedForward-Add (Ad (None, None, 768)    0           Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[0][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[1][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[2][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[3][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[4][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[5][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[6][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[7][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[8][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[9][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[10][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[11][0]   
__________________________________________________________________________________________________
Transformer-FeedForward-Norm (L (None, None, 768)    1536        Transformer-FeedForward-Add[0][0]
                                                                 Transformer-FeedForward-Add[1][0]
                                                                 Transformer-FeedForward-Add[2][0]
                                                                 Transformer-FeedForward-Add[3][0]
                                                                 Transformer-FeedForward-Add[4][0]
                                                                 Transformer-FeedForward-Add[5][0]
                                                                 Transformer-FeedForward-Add[6][0]
                                                                 Transformer-FeedForward-Add[7][0]
                                                                 Transformer-FeedForward-Add[8][0]
                                                                 Transformer-FeedForward-Add[9][0]
                                                                 Transformer-FeedForward-Add[10][0
                                                                 Transformer-FeedForward-Add[11][0
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 768)          0           Transformer-FeedForward-Norm[11][
__________________________________________________________________________________________________
dropout (Dropout)               (None, 768)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           11535       dropout[0][0]                    
==================================================================================================
Total params: 9,968,911
Trainable params: 11,535
Non-trainable params: 9,957,376
__________________________________________________________________________________________________
2022-05-11 00:18:27.391574: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 79s - loss: 2.1627 - accuracy: 0.3419
dev acc=43.68%
test acc=44.90%
Epoch 2/10000
188/188 - 69s - loss: 1.8262 - accuracy: 0.4349
dev acc=45.97%
test acc=46.82%
Epoch 3/10000
188/188 - 70s - loss: 1.7467 - accuracy: 0.4488
dev acc=46.98%
test acc=47.57%
Epoch 4/10000
188/188 - 69s - loss: 1.7174 - accuracy: 0.4518
dev acc=47.47%
test acc=47.88%
Epoch 5/10000
188/188 - 70s - loss: 1.7049 - accuracy: 0.4568
dev acc=47.49%
test acc=48.19%
Epoch 6/10000
188/188 - 70s - loss: 1.6930 - accuracy: 0.4604
dev acc=47.54%
test acc=48.32%
Epoch 7/10000
188/188 - 70s - loss: 1.6864 - accuracy: 0.4617
dev acc=47.43%
Epoch 8/10000
188/188 - 69s - loss: 1.6878 - accuracy: 0.4585
dev acc=47.73%
test acc=48.74%
Epoch 9/10000
188/188 - 70s - loss: 1.6807 - accuracy: 0.4605
dev acc=48.11%
test acc=48.70%
Epoch 10/10000
188/188 - 70s - loss: 1.6757 - accuracy: 0.4622
dev acc=47.84%
Epoch 11/10000
188/188 - 70s - loss: 1.6783 - accuracy: 0.4598
dev acc=48.05%
Epoch 12/10000
188/188 - 70s - loss: 1.6776 - accuracy: 0.4615
dev acc=48.05%
Epoch 13/10000
188/188 - 70s - loss: 1.6756 - accuracy: 0.4618
dev acc=48.11%
Epoch 14/10000
188/188 - 70s - loss: 1.6728 - accuracy: 0.4613
dev acc=48.13%
test acc=49.04%
Epoch 15/10000
188/188 - 71s - loss: 1.6740 - accuracy: 0.4631
dev acc=48.28%
test acc=49.04%
Epoch 16/10000
188/188 - 71s - loss: 1.6738 - accuracy: 0.4614
dev acc=48.09%
Epoch 17/10000
188/188 - 70s - loss: 1.6718 - accuracy: 0.4625
dev acc=48.20%
Epoch 18/10000
188/188 - 70s - loss: 1.6718 - accuracy: 0.4626
dev acc=48.28%
Epoch 19/10000
188/188 - 70s - loss: 1.6721 - accuracy: 0.4615
dev acc=48.39%
test acc=49.01%
Epoch 20/10000
188/188 - 70s - loss: 1.6716 - accuracy: 0.4624
dev acc=48.33%
Epoch 21/10000
188/188 - 70s - loss: 1.6710 - accuracy: 0.4627
dev acc=48.29%
Epoch 22/10000
188/188 - 70s - loss: 1.6666 - accuracy: 0.4630
dev acc=48.26%
Epoch 23/10000
188/188 - 70s - loss: 1.6690 - accuracy: 0.4641
dev acc=48.56%
test acc=49.02%
Epoch 24/10000
188/188 - 70s - loss: 1.6768 - accuracy: 0.4630
dev acc=48.46%
Epoch 25/10000
188/188 - 70s - loss: 1.6743 - accuracy: 0.4623
dev acc=48.24%
Epoch 26/10000
188/188 - 71s - loss: 1.6694 - accuracy: 0.4634
dev acc=48.24%
Epoch 27/10000
188/188 - 70s - loss: 1.6726 - accuracy: 0.4597
dev acc=48.26%
Epoch 28/10000
188/188 - 70s - loss: 1.6728 - accuracy: 0.4626
dev acc=48.33%
time used=2697.0s
2022-05-11 01:03:15.890713: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert/albert_large/model.ckpt-best', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert/albert_large/albert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='albert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert/albert_large/vocab_chinese.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_agriculture': 0, 'news_tech': 1, 'news_world': 2, 'news_stock': 3, 'news_edu': 4, 'news_car': 5, 'news_house': 6, 'news_story': 7, 'news_entertainment': 8, 'news_military': 9, 'news_culture': 10, 'news_sports': 11, 'news_game': 12, 'news_travel': 13, 'news_finance': 14}
index_labels_dict={0: 'news_agriculture', 1: 'news_tech', 2: 'news_world', 3: 'news_stock', 4: 'news_edu', 5: 'news_car', 6: 'news_house', 7: 'news_story', 8: 'news_entertainment', 9: 'news_military', 10: 'news_culture', 11: 'news_sports', 12: 'news_game', 13: 'news_travel', 14: 'news_finance'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-11 01:03:23.917279: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-11 01:03:25.148154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 128)    2704384     Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Mapping (Dense)       (None, None, 1024)   132096      Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 1024)   4198400     Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-FeedForward-Norm[11][
                                                                 Transformer-FeedForward-Norm[11][
                                                                 Transformer-FeedForward-Norm[11][
                                                                 Transformer-FeedForward-Norm[12][
                                                                 Transformer-FeedForward-Norm[12][
                                                                 Transformer-FeedForward-Norm[12][
                                                                 Transformer-FeedForward-Norm[13][
                                                                 Transformer-FeedForward-Norm[13][
                                                                 Transformer-FeedForward-Norm[13][
                                                                 Transformer-FeedForward-Norm[14][
                                                                 Transformer-FeedForward-Norm[14][
                                                                 Transformer-FeedForward-Norm[14][
                                                                 Transformer-FeedForward-Norm[15][
                                                                 Transformer-FeedForward-Norm[15][
                                                                 Transformer-FeedForward-Norm[15][
                                                                 Transformer-FeedForward-Norm[16][
                                                                 Transformer-FeedForward-Norm[16][
                                                                 Transformer-FeedForward-Norm[16][
                                                                 Transformer-FeedForward-Norm[17][
                                                                 Transformer-FeedForward-Norm[17][
                                                                 Transformer-FeedForward-Norm[17][
                                                                 Transformer-FeedForward-Norm[18][
                                                                 Transformer-FeedForward-Norm[18][
                                                                 Transformer-FeedForward-Norm[18][
                                                                 Transformer-FeedForward-Norm[19][
                                                                 Transformer-FeedForward-Norm[19][
                                                                 Transformer-FeedForward-Norm[19][
                                                                 Transformer-FeedForward-Norm[20][
                                                                 Transformer-FeedForward-Norm[20][
                                                                 Transformer-FeedForward-Norm[20][
                                                                 Transformer-FeedForward-Norm[21][
                                                                 Transformer-FeedForward-Norm[21][
                                                                 Transformer-FeedForward-Norm[21][
                                                                 Transformer-FeedForward-Norm[22][
                                                                 Transformer-FeedForward-Norm[22][
                                                                 Transformer-FeedForward-Norm[22][
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 1024)   0           Embedding-Mapping[0][0]          
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[11][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[12][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[13][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[14][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[15][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[16][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[17][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[18][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[19][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[20][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[21][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[22][
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 1024)   2048        Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-FeedForward (FeedFo (None, None, 1024)   8393728     Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-FeedForward-Add (Ad (None, None, 1024)   0           Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[0][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[1][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[2][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[3][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[4][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[5][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[6][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[7][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[8][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[9][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[10][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[11][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[12][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[13][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[14][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[15][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[16][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[17][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[18][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[19][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[20][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[21][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[22][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[23][0]   
__________________________________________________________________________________________________
Transformer-FeedForward-Norm (L (None, None, 1024)   2048        Transformer-FeedForward-Add[0][0]
                                                                 Transformer-FeedForward-Add[1][0]
                                                                 Transformer-FeedForward-Add[2][0]
                                                                 Transformer-FeedForward-Add[3][0]
                                                                 Transformer-FeedForward-Add[4][0]
                                                                 Transformer-FeedForward-Add[5][0]
                                                                 Transformer-FeedForward-Add[6][0]
                                                                 Transformer-FeedForward-Add[7][0]
                                                                 Transformer-FeedForward-Add[8][0]
                                                                 Transformer-FeedForward-Add[9][0]
                                                                 Transformer-FeedForward-Add[10][0
                                                                 Transformer-FeedForward-Add[11][0
                                                                 Transformer-FeedForward-Add[12][0
                                                                 Transformer-FeedForward-Add[13][0
                                                                 Transformer-FeedForward-Add[14][0
                                                                 Transformer-FeedForward-Add[15][0
                                                                 Transformer-FeedForward-Add[16][0
                                                                 Transformer-FeedForward-Add[17][0
                                                                 Transformer-FeedForward-Add[18][0
                                                                 Transformer-FeedForward-Add[19][0
                                                                 Transformer-FeedForward-Add[20][0
                                                                 Transformer-FeedForward-Add[21][0
                                                                 Transformer-FeedForward-Add[22][0
                                                                 Transformer-FeedForward-Add[23][0
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 1024)         0           Transformer-FeedForward-Norm[23][
__________________________________________________________________________________________________
dropout (Dropout)               (None, 1024)         0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           15375       dropout[0][0]                    
==================================================================================================
Total params: 15,514,127
Trainable params: 15,375
Non-trainable params: 15,498,752
__________________________________________________________________________________________________
2022-05-11 01:03:29.243111: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 233s - loss: 2.0024 - accuracy: 0.4002
dev acc=47.04%
test acc=48.17%
Epoch 2/10000
188/188 - 217s - loss: 1.6757 - accuracy: 0.4732
dev acc=48.56%
test acc=49.40%
Epoch 3/10000
188/188 - 217s - loss: 1.6218 - accuracy: 0.4814
dev acc=48.65%
test acc=49.71%
Epoch 4/10000
188/188 - 218s - loss: 1.6008 - accuracy: 0.4865
dev acc=49.12%
test acc=49.73%
Epoch 5/10000
188/188 - 219s - loss: 1.5909 - accuracy: 0.4875
dev acc=49.36%
test acc=49.93%
Epoch 6/10000
188/188 - 218s - loss: 1.5803 - accuracy: 0.4913
dev acc=49.33%
Epoch 7/10000
188/188 - 218s - loss: 1.5769 - accuracy: 0.4892
dev acc=49.66%
test acc=50.31%
Epoch 8/10000
188/188 - 218s - loss: 1.5697 - accuracy: 0.4944
dev acc=49.85%
test acc=50.42%
Epoch 9/10000
188/188 - 218s - loss: 1.5659 - accuracy: 0.4922
dev acc=49.66%
Epoch 10/10000
188/188 - 218s - loss: 1.5691 - accuracy: 0.4915
dev acc=49.64%
Epoch 11/10000
188/188 - 218s - loss: 1.5609 - accuracy: 0.4952
dev acc=49.83%
Epoch 12/10000
188/188 - 218s - loss: 1.5636 - accuracy: 0.4959
dev acc=49.70%
Epoch 13/10000
188/188 - 218s - loss: 1.5617 - accuracy: 0.4946
dev acc=49.87%
test acc=50.78%
Epoch 14/10000
188/188 - 218s - loss: 1.5584 - accuracy: 0.4949
dev acc=50.13%
test acc=50.85%
Epoch 15/10000
188/188 - 218s - loss: 1.5544 - accuracy: 0.4969
dev acc=50.06%
Epoch 16/10000
188/188 - 218s - loss: 1.5591 - accuracy: 0.4963
dev acc=50.22%
test acc=50.79%
Epoch 17/10000
188/188 - 218s - loss: 1.5538 - accuracy: 0.4957
dev acc=50.09%
Epoch 18/10000
188/188 - 218s - loss: 1.5546 - accuracy: 0.4962
dev acc=49.94%
Epoch 19/10000
188/188 - 218s - loss: 1.5566 - accuracy: 0.4952
dev acc=50.06%
Epoch 20/10000
188/188 - 218s - loss: 1.5510 - accuracy: 0.4986
dev acc=50.17%
Epoch 21/10000
188/188 - 218s - loss: 1.5548 - accuracy: 0.4960
dev acc=50.11%
time used=5934.6s
2022-05-11 02:42:14.070854: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert/albert_xlarge/model.ckpt-best', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert/albert_xlarge/albert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='albert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert/albert_xlarge/vocab_chinese.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_story': 0, 'news_game': 1, 'news_world': 2, 'news_agriculture': 3, 'news_stock': 4, 'news_sports': 5, 'news_military': 6, 'news_travel': 7, 'news_tech': 8, 'news_entertainment': 9, 'news_house': 10, 'news_finance': 11, 'news_edu': 12, 'news_culture': 13, 'news_car': 14}
index_labels_dict={0: 'news_story', 1: 'news_game', 2: 'news_world', 3: 'news_agriculture', 4: 'news_stock', 5: 'news_sports', 6: 'news_military', 7: 'news_travel', 8: 'news_tech', 9: 'news_entertainment', 10: 'news_house', 11: 'news_finance', 12: 'news_edu', 13: 'news_culture', 14: 'news_car'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-11 02:42:22.301325: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-11 02:42:23.188870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 128)    2704384     Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Mapping (Dense)       (None, None, 2048)   264192      Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 2048)   16785408    Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-FeedForward-Norm[11][
                                                                 Transformer-FeedForward-Norm[11][
                                                                 Transformer-FeedForward-Norm[11][
                                                                 Transformer-FeedForward-Norm[12][
                                                                 Transformer-FeedForward-Norm[12][
                                                                 Transformer-FeedForward-Norm[12][
                                                                 Transformer-FeedForward-Norm[13][
                                                                 Transformer-FeedForward-Norm[13][
                                                                 Transformer-FeedForward-Norm[13][
                                                                 Transformer-FeedForward-Norm[14][
                                                                 Transformer-FeedForward-Norm[14][
                                                                 Transformer-FeedForward-Norm[14][
                                                                 Transformer-FeedForward-Norm[15][
                                                                 Transformer-FeedForward-Norm[15][
                                                                 Transformer-FeedForward-Norm[15][
                                                                 Transformer-FeedForward-Norm[16][
                                                                 Transformer-FeedForward-Norm[16][
                                                                 Transformer-FeedForward-Norm[16][
                                                                 Transformer-FeedForward-Norm[17][
                                                                 Transformer-FeedForward-Norm[17][
                                                                 Transformer-FeedForward-Norm[17][
                                                                 Transformer-FeedForward-Norm[18][
                                                                 Transformer-FeedForward-Norm[18][
                                                                 Transformer-FeedForward-Norm[18][
                                                                 Transformer-FeedForward-Norm[19][
                                                                 Transformer-FeedForward-Norm[19][
                                                                 Transformer-FeedForward-Norm[19][
                                                                 Transformer-FeedForward-Norm[20][
                                                                 Transformer-FeedForward-Norm[20][
                                                                 Transformer-FeedForward-Norm[20][
                                                                 Transformer-FeedForward-Norm[21][
                                                                 Transformer-FeedForward-Norm[21][
                                                                 Transformer-FeedForward-Norm[21][
                                                                 Transformer-FeedForward-Norm[22][
                                                                 Transformer-FeedForward-Norm[22][
                                                                 Transformer-FeedForward-Norm[22][
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 2048)   0           Embedding-Mapping[0][0]          
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[11][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[12][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[13][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[14][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[15][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[16][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[17][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[18][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[19][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[20][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[21][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[22][
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 2048)   4096        Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-FeedForward (FeedFo (None, None, 2048)   33564672    Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-FeedForward-Add (Ad (None, None, 2048)   0           Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[0][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[1][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[2][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[3][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[4][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[5][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[6][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[7][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[8][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[9][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[10][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[11][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[12][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[13][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[14][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[15][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[16][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[17][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[18][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[19][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[20][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[21][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[22][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[23][0]   
__________________________________________________________________________________________________
Transformer-FeedForward-Norm (L (None, None, 2048)   4096        Transformer-FeedForward-Add[0][0]
                                                                 Transformer-FeedForward-Add[1][0]
                                                                 Transformer-FeedForward-Add[2][0]
                                                                 Transformer-FeedForward-Add[3][0]
                                                                 Transformer-FeedForward-Add[4][0]
                                                                 Transformer-FeedForward-Add[5][0]
                                                                 Transformer-FeedForward-Add[6][0]
                                                                 Transformer-FeedForward-Add[7][0]
                                                                 Transformer-FeedForward-Add[8][0]
                                                                 Transformer-FeedForward-Add[9][0]
                                                                 Transformer-FeedForward-Add[10][0
                                                                 Transformer-FeedForward-Add[11][0
                                                                 Transformer-FeedForward-Add[12][0
                                                                 Transformer-FeedForward-Add[13][0
                                                                 Transformer-FeedForward-Add[14][0
                                                                 Transformer-FeedForward-Add[15][0
                                                                 Transformer-FeedForward-Add[16][0
                                                                 Transformer-FeedForward-Add[17][0
                                                                 Transformer-FeedForward-Add[18][0
                                                                 Transformer-FeedForward-Add[19][0
                                                                 Transformer-FeedForward-Add[20][0
                                                                 Transformer-FeedForward-Add[21][0
                                                                 Transformer-FeedForward-Add[22][0
                                                                 Transformer-FeedForward-Add[23][0
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 2048)         0           Transformer-FeedForward-Norm[23][
__________________________________________________________________________________________________
dropout (Dropout)               (None, 2048)         0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           30735       dropout[0][0]                    
==================================================================================================
Total params: 53,423,631
Trainable params: 30,735
Non-trainable params: 53,392,896
__________________________________________________________________________________________________
2022-05-11 02:42:27.525142: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 721s - loss: 1.8582 - accuracy: 0.4272
dev acc=48.24%
test acc=48.49%
Epoch 2/10000
188/188 - 714s - loss: 1.6179 - accuracy: 0.4803
dev acc=49.03%
test acc=49.26%
Epoch 3/10000
188/188 - 716s - loss: 1.5904 - accuracy: 0.4864
dev acc=49.98%
test acc=49.92%
Epoch 4/10000
188/188 - 715s - loss: 1.5697 - accuracy: 0.4912
dev acc=50.04%
test acc=50.19%
Epoch 5/10000
188/188 - 716s - loss: 1.5613 - accuracy: 0.4917
dev acc=50.30%
test acc=50.20%
Epoch 6/10000
188/188 - 717s - loss: 1.5534 - accuracy: 0.4925
dev acc=50.22%
Epoch 7/10000
188/188 - 716s - loss: 1.5460 - accuracy: 0.4954
dev acc=50.22%
Epoch 8/10000
188/188 - 716s - loss: 1.5414 - accuracy: 0.4989
dev acc=50.58%
test acc=50.57%
Epoch 9/10000
188/188 - 715s - loss: 1.5389 - accuracy: 0.4979
dev acc=50.82%
test acc=50.77%
Epoch 10/10000
188/188 - 716s - loss: 1.5394 - accuracy: 0.4972
dev acc=50.77%
Epoch 11/10000
188/188 - 717s - loss: 1.5338 - accuracy: 0.5017
dev acc=50.75%
Epoch 12/10000
188/188 - 717s - loss: 1.5364 - accuracy: 0.4975
dev acc=50.71%
Epoch 13/10000
188/188 - 715s - loss: 1.5313 - accuracy: 0.5014
dev acc=50.56%
Epoch 14/10000
188/188 - 716s - loss: 1.5347 - accuracy: 0.5006
dev acc=50.24%
time used=12466.2s
2022-05-11 06:10:04.002749: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert/albert_xxlarge/model.ckpt-best', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert/albert_xxlarge/albert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='albert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert/albert_xxlarge/vocab_chinese.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_edu': 0, 'news_game': 1, 'news_car': 2, 'news_agriculture': 3, 'news_story': 4, 'news_world': 5, 'news_military': 6, 'news_entertainment': 7, 'news_house': 8, 'news_finance': 9, 'news_culture': 10, 'news_sports': 11, 'news_tech': 12, 'news_stock': 13, 'news_travel': 14}
index_labels_dict={0: 'news_edu', 1: 'news_game', 2: 'news_car', 3: 'news_agriculture', 4: 'news_story', 5: 'news_world', 6: 'news_military', 7: 'news_entertainment', 8: 'news_house', 9: 'news_finance', 10: 'news_culture', 11: 'news_sports', 12: 'news_tech', 13: 'news_stock', 14: 'news_travel'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-11 06:10:12.128240: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-11 06:10:13.003919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 128)    2704384     Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Mapping (Dense)       (None, None, 4096)   528384      Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 4096)   67125248    Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-FeedForward-Norm[10][
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 4096)   0           Embedding-Mapping[0][0]          
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 4096)   8192        Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-FeedForward (FeedFo (None, None, 4096)   134238208   Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-FeedForward-Add (Ad (None, None, 4096)   0           Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[0][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[1][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[2][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[3][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[4][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[5][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[6][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[7][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[8][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[9][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[10][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[11][0]   
__________________________________________________________________________________________________
Transformer-FeedForward-Norm (L (None, None, 4096)   8192        Transformer-FeedForward-Add[0][0]
                                                                 Transformer-FeedForward-Add[1][0]
                                                                 Transformer-FeedForward-Add[2][0]
                                                                 Transformer-FeedForward-Add[3][0]
                                                                 Transformer-FeedForward-Add[4][0]
                                                                 Transformer-FeedForward-Add[5][0]
                                                                 Transformer-FeedForward-Add[6][0]
                                                                 Transformer-FeedForward-Add[7][0]
                                                                 Transformer-FeedForward-Add[8][0]
                                                                 Transformer-FeedForward-Add[9][0]
                                                                 Transformer-FeedForward-Add[10][0
                                                                 Transformer-FeedForward-Add[11][0
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 4096)         0           Transformer-FeedForward-Norm[11][
__________________________________________________________________________________________________
dropout (Dropout)               (None, 4096)         0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           61455       dropout[0][0]                    
==================================================================================================
Total params: 204,740,111
Trainable params: 61,455
Non-trainable params: 204,678,656
__________________________________________________________________________________________________
2022-05-11 06:10:17.828942: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 1329s - loss: 1.8800 - accuracy: 0.4150
dev acc=47.25%
test acc=48.31%
Epoch 2/10000
188/188 - 1349s - loss: 1.6455 - accuracy: 0.4758
dev acc=48.74%
test acc=49.31%
Epoch 3/10000
188/188 - 1354s - loss: 1.5964 - accuracy: 0.4858
dev acc=49.34%
test acc=49.69%
Epoch 4/10000
188/188 - 1355s - loss: 1.5664 - accuracy: 0.4962
dev acc=49.38%
test acc=50.21%
Epoch 5/10000
188/188 - 1354s - loss: 1.5558 - accuracy: 0.4996
dev acc=49.57%
test acc=50.44%
Epoch 6/10000
188/188 - 1354s - loss: 1.5456 - accuracy: 0.5020
dev acc=49.98%
test acc=50.07%
Epoch 7/10000
188/188 - 1351s - loss: 1.5400 - accuracy: 0.5014
dev acc=50.07%
test acc=50.22%
Epoch 8/10000
188/188 - 1348s - loss: 1.5353 - accuracy: 0.5046
dev acc=50.22%
test acc=50.08%
Epoch 9/10000
188/188 - 1347s - loss: 1.5334 - accuracy: 0.5051
dev acc=49.91%
Epoch 10/10000
188/188 - 1352s - loss: 1.5277 - accuracy: 0.5096
dev acc=50.06%
Epoch 11/10000
188/188 - 1351s - loss: 1.5275 - accuracy: 0.5060
dev acc=49.98%
Epoch 12/10000
188/188 - 1353s - loss: 1.5221 - accuracy: 0.5090
dev acc=50.15%
Epoch 13/10000
188/188 - 1348s - loss: 1.5263 - accuracy: 0.5090
dev acc=50.09%
time used=21859.7s
2022-05-11 12:14:27.377509: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert_zh/albert_base_google_zh_additional_36k_steps/albert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert_zh/albert_base_google_zh_additional_36k_steps/albert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='albert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert_zh/albert_base_google_zh_additional_36k_steps/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_car': 0, 'news_sports': 1, 'news_story': 2, 'news_game': 3, 'news_military': 4, 'news_finance': 5, 'news_house': 6, 'news_entertainment': 7, 'news_stock': 8, 'news_edu': 9, 'news_travel': 10, 'news_world': 11, 'news_tech': 12, 'news_agriculture': 13, 'news_culture': 14}
index_labels_dict={0: 'news_car', 1: 'news_sports', 2: 'news_story', 3: 'news_game', 4: 'news_military', 5: 'news_finance', 6: 'news_house', 7: 'news_entertainment', 8: 'news_stock', 9: 'news_edu', 10: 'news_travel', 11: 'news_world', 12: 'news_tech', 13: 'news_agriculture', 14: 'news_culture'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-11 12:14:35.720251: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-11 12:14:36.910330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 768)    16226304    Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 768)    2362368     Embedding-Norm[0][0]             
                                                                 Embedding-Norm[0][0]             
                                                                 Embedding-Norm[0][0]             
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-FeedForward-Norm[10][
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 768)    0           Embedding-Norm[0][0]             
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 768)    1536        Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-FeedForward (FeedFo (None, None, 768)    4722432     Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-FeedForward-Add (Ad (None, None, 768)    0           Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[0][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[1][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[2][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[3][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[4][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[5][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[6][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[7][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[8][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[9][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[10][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[11][0]   
__________________________________________________________________________________________________
Transformer-FeedForward-Norm (L (None, None, 768)    1536        Transformer-FeedForward-Add[0][0]
                                                                 Transformer-FeedForward-Add[1][0]
                                                                 Transformer-FeedForward-Add[2][0]
                                                                 Transformer-FeedForward-Add[3][0]
                                                                 Transformer-FeedForward-Add[4][0]
                                                                 Transformer-FeedForward-Add[5][0]
                                                                 Transformer-FeedForward-Add[6][0]
                                                                 Transformer-FeedForward-Add[7][0]
                                                                 Transformer-FeedForward-Add[8][0]
                                                                 Transformer-FeedForward-Add[9][0]
                                                                 Transformer-FeedForward-Add[10][0
                                                                 Transformer-FeedForward-Add[11][0
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 768)          0           Transformer-FeedForward-Norm[11][
__________________________________________________________________________________________________
dropout (Dropout)               (None, 768)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           11535       dropout[0][0]                    
==================================================================================================
Total params: 23,721,999
Trainable params: 11,535
Non-trainable params: 23,710,464
__________________________________________________________________________________________________
2022-05-11 12:14:40.187209: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 101s - loss: 1.9731 - accuracy: 0.3929
dev acc=47.11%
test acc=48.16%
Epoch 2/10000
188/188 - 90s - loss: 1.6655 - accuracy: 0.4678
dev acc=48.71%
test acc=49.18%
Epoch 3/10000
188/188 - 90s - loss: 1.6181 - accuracy: 0.4756
dev acc=49.16%
test acc=49.54%
Epoch 4/10000
188/188 - 90s - loss: 1.5991 - accuracy: 0.4779
dev acc=49.61%
test acc=49.76%
Epoch 5/10000
188/188 - 90s - loss: 1.5883 - accuracy: 0.4830
dev acc=49.78%
test acc=49.72%
Epoch 6/10000
188/188 - 90s - loss: 1.5853 - accuracy: 0.4807
dev acc=50.04%
test acc=49.93%
Epoch 7/10000
188/188 - 90s - loss: 1.5783 - accuracy: 0.4811
dev acc=50.15%
test acc=50.17%
Epoch 8/10000
188/188 - 91s - loss: 1.5754 - accuracy: 0.4847
dev acc=50.17%
test acc=50.15%
Epoch 9/10000
188/188 - 90s - loss: 1.5745 - accuracy: 0.4869
dev acc=50.21%
test acc=50.52%
Epoch 10/10000
188/188 - 90s - loss: 1.5715 - accuracy: 0.4842
dev acc=50.19%
Epoch 11/10000
188/188 - 90s - loss: 1.5762 - accuracy: 0.4843
dev acc=50.26%
test acc=50.42%
Epoch 12/10000
188/188 - 91s - loss: 1.5760 - accuracy: 0.4822
dev acc=50.22%
Epoch 13/10000
188/188 - 91s - loss: 1.5681 - accuracy: 0.4844
dev acc=50.45%
test acc=50.35%
Epoch 14/10000
188/188 - 91s - loss: 1.5670 - accuracy: 0.4849
dev acc=50.36%
Epoch 15/10000
188/188 - 91s - loss: 1.5686 - accuracy: 0.4860
dev acc=50.43%
Epoch 16/10000
188/188 - 91s - loss: 1.5648 - accuracy: 0.4856
dev acc=50.34%
Epoch 17/10000
188/188 - 91s - loss: 1.5714 - accuracy: 0.4856
dev acc=50.19%
Epoch 18/10000
188/188 - 91s - loss: 1.5721 - accuracy: 0.4831
dev acc=50.43%
time used=2275.9s
2022-05-11 12:52:27.027619: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert_zh/albert_large_google_zh/albert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert_zh/albert_large_google_zh/albert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='albert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert_zh/albert_large_google_zh/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_tech': 0, 'news_game': 1, 'news_stock': 2, 'news_finance': 3, 'news_car': 4, 'news_culture': 5, 'news_edu': 6, 'news_sports': 7, 'news_travel': 8, 'news_house': 9, 'news_military': 10, 'news_agriculture': 11, 'news_entertainment': 12, 'news_story': 13, 'news_world': 14}
index_labels_dict={0: 'news_tech', 1: 'news_game', 2: 'news_stock', 3: 'news_finance', 4: 'news_car', 5: 'news_culture', 6: 'news_edu', 7: 'news_sports', 8: 'news_travel', 9: 'news_house', 10: 'news_military', 11: 'news_agriculture', 12: 'news_entertainment', 13: 'news_story', 14: 'news_world'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-11 12:52:35.399512: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-11 12:52:36.755406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 1024)   21635072    Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 1024)   2048        Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 1024)   0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 1024)   524288      Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 1024)   2048        Embedding-Position[0][0]         
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 1024)   4198400     Embedding-Norm[0][0]             
                                                                 Embedding-Norm[0][0]             
                                                                 Embedding-Norm[0][0]             
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-FeedForward-Norm[11][
                                                                 Transformer-FeedForward-Norm[11][
                                                                 Transformer-FeedForward-Norm[11][
                                                                 Transformer-FeedForward-Norm[12][
                                                                 Transformer-FeedForward-Norm[12][
                                                                 Transformer-FeedForward-Norm[12][
                                                                 Transformer-FeedForward-Norm[13][
                                                                 Transformer-FeedForward-Norm[13][
                                                                 Transformer-FeedForward-Norm[13][
                                                                 Transformer-FeedForward-Norm[14][
                                                                 Transformer-FeedForward-Norm[14][
                                                                 Transformer-FeedForward-Norm[14][
                                                                 Transformer-FeedForward-Norm[15][
                                                                 Transformer-FeedForward-Norm[15][
                                                                 Transformer-FeedForward-Norm[15][
                                                                 Transformer-FeedForward-Norm[16][
                                                                 Transformer-FeedForward-Norm[16][
                                                                 Transformer-FeedForward-Norm[16][
                                                                 Transformer-FeedForward-Norm[17][
                                                                 Transformer-FeedForward-Norm[17][
                                                                 Transformer-FeedForward-Norm[17][
                                                                 Transformer-FeedForward-Norm[18][
                                                                 Transformer-FeedForward-Norm[18][
                                                                 Transformer-FeedForward-Norm[18][
                                                                 Transformer-FeedForward-Norm[19][
                                                                 Transformer-FeedForward-Norm[19][
                                                                 Transformer-FeedForward-Norm[19][
                                                                 Transformer-FeedForward-Norm[20][
                                                                 Transformer-FeedForward-Norm[20][
                                                                 Transformer-FeedForward-Norm[20][
                                                                 Transformer-FeedForward-Norm[21][
                                                                 Transformer-FeedForward-Norm[21][
                                                                 Transformer-FeedForward-Norm[21][
                                                                 Transformer-FeedForward-Norm[22][
                                                                 Transformer-FeedForward-Norm[22][
                                                                 Transformer-FeedForward-Norm[22][
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 1024)   0           Embedding-Norm[0][0]             
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[11][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[12][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[13][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[14][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[15][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[16][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[17][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[18][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[19][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[20][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[21][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[22][
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 1024)   2048        Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-FeedForward (FeedFo (None, None, 1024)   8393728     Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-FeedForward-Add (Ad (None, None, 1024)   0           Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[0][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[1][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[2][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[3][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[4][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[5][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[6][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[7][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[8][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[9][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[10][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[11][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[12][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[13][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[14][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[15][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[16][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[17][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[18][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[19][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[20][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[21][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[22][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[23][0]   
__________________________________________________________________________________________________
Transformer-FeedForward-Norm (L (None, None, 1024)   2048        Transformer-FeedForward-Add[0][0]
                                                                 Transformer-FeedForward-Add[1][0]
                                                                 Transformer-FeedForward-Add[2][0]
                                                                 Transformer-FeedForward-Add[3][0]
                                                                 Transformer-FeedForward-Add[4][0]
                                                                 Transformer-FeedForward-Add[5][0]
                                                                 Transformer-FeedForward-Add[6][0]
                                                                 Transformer-FeedForward-Add[7][0]
                                                                 Transformer-FeedForward-Add[8][0]
                                                                 Transformer-FeedForward-Add[9][0]
                                                                 Transformer-FeedForward-Add[10][0
                                                                 Transformer-FeedForward-Add[11][0
                                                                 Transformer-FeedForward-Add[12][0
                                                                 Transformer-FeedForward-Add[13][0
                                                                 Transformer-FeedForward-Add[14][0
                                                                 Transformer-FeedForward-Add[15][0
                                                                 Transformer-FeedForward-Add[16][0
                                                                 Transformer-FeedForward-Add[17][0
                                                                 Transformer-FeedForward-Add[18][0
                                                                 Transformer-FeedForward-Add[19][0
                                                                 Transformer-FeedForward-Add[20][0
                                                                 Transformer-FeedForward-Add[21][0
                                                                 Transformer-FeedForward-Add[22][0
                                                                 Transformer-FeedForward-Add[23][0
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 1024)         0           Transformer-FeedForward-Norm[23][
__________________________________________________________________________________________________
dropout (Dropout)               (None, 1024)         0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           15375       dropout[0][0]                    
==================================================================================================
Total params: 34,775,055
Trainable params: 15,375
Non-trainable params: 34,759,680
__________________________________________________________________________________________________
2022-05-11 12:52:41.357647: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 293s - loss: 1.9666 - accuracy: 0.4022
dev acc=47.81%
test acc=48.59%
Epoch 2/10000
188/188 - 273s - loss: 1.6459 - accuracy: 0.4790
dev acc=49.44%
test acc=50.00%
Epoch 3/10000
188/188 - 274s - loss: 1.5911 - accuracy: 0.4878
dev acc=50.45%
test acc=50.62%
Epoch 4/10000
188/188 - 273s - loss: 1.5689 - accuracy: 0.4922
dev acc=50.32%
Epoch 5/10000
188/188 - 274s - loss: 1.5544 - accuracy: 0.4970
dev acc=50.73%
test acc=51.10%
Epoch 6/10000
188/188 - 274s - loss: 1.5488 - accuracy: 0.4961
dev acc=50.81%
test acc=51.29%
Epoch 7/10000
188/188 - 274s - loss: 1.5427 - accuracy: 0.4984
dev acc=51.35%
test acc=51.43%
Epoch 8/10000
188/188 - 274s - loss: 1.5400 - accuracy: 0.4982
dev acc=51.20%
Epoch 9/10000
188/188 - 274s - loss: 1.5358 - accuracy: 0.4996
dev acc=51.37%
test acc=51.79%
Epoch 10/10000
188/188 - 274s - loss: 1.5334 - accuracy: 0.4992
dev acc=51.48%
test acc=51.73%
Epoch 11/10000
188/188 - 273s - loss: 1.5319 - accuracy: 0.5006
dev acc=51.69%
test acc=51.86%
Epoch 12/10000
188/188 - 273s - loss: 1.5321 - accuracy: 0.4999
dev acc=51.41%
Epoch 13/10000
188/188 - 273s - loss: 1.5287 - accuracy: 0.5033
dev acc=51.48%
Epoch 14/10000
188/188 - 274s - loss: 1.5279 - accuracy: 0.5023
dev acc=51.71%
test acc=51.81%
Epoch 15/10000
188/188 - 274s - loss: 1.5270 - accuracy: 0.5002
dev acc=51.50%
Epoch 16/10000
188/188 - 273s - loss: 1.5288 - accuracy: 0.5001
dev acc=51.67%
Epoch 17/10000
188/188 - 273s - loss: 1.5237 - accuracy: 0.5054
dev acc=51.91%
test acc=51.88%
Epoch 18/10000
188/188 - 273s - loss: 1.5242 - accuracy: 0.5034
dev acc=51.86%
Epoch 19/10000
188/188 - 273s - loss: 1.5242 - accuracy: 0.5011
dev acc=51.76%
Epoch 20/10000
188/188 - 273s - loss: 1.5233 - accuracy: 0.5013
dev acc=51.61%
Epoch 21/10000
188/188 - 273s - loss: 1.5236 - accuracy: 0.5018
dev acc=51.69%
Epoch 22/10000
188/188 - 273s - loss: 1.5252 - accuracy: 0.4997
dev acc=51.41%
time used=7741.6s
2022-05-11 15:01:32.537016: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert_zh/albert_small_zh_google/albert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert_zh/albert_small_zh_google/albert_config_small_google.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='albert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert_zh/albert_small_zh_google/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_world': 0, 'news_military': 1, 'news_sports': 2, 'news_stock': 3, 'news_culture': 4, 'news_finance': 5, 'news_house': 6, 'news_story': 7, 'news_travel': 8, 'news_agriculture': 9, 'news_game': 10, 'news_edu': 11, 'news_entertainment': 12, 'news_tech': 13, 'news_car': 14}
index_labels_dict={0: 'news_world', 1: 'news_military', 2: 'news_sports', 3: 'news_stock', 4: 'news_culture', 5: 'news_finance', 6: 'news_house', 7: 'news_story', 8: 'news_travel', 9: 'news_agriculture', 10: 'news_game', 11: 'news_edu', 12: 'news_entertainment', 13: 'news_tech', 14: 'news_car'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-11 15:01:41.142019: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-11 15:01:42.547613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 128)    2704384     Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Mapping (Dense)       (None, None, 384)    49536       Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 384)    591360      Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[4][0
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 384)    0           Embedding-Mapping[0][0]          
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 384)    768         Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-FeedForward (FeedFo (None, None, 384)    1181568     Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-FeedForward-Add (Ad (None, None, 384)    0           Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[0][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[1][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[2][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[3][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[4][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[5][0]    
__________________________________________________________________________________________________
Transformer-FeedForward-Norm (L (None, None, 384)    768         Transformer-FeedForward-Add[0][0]
                                                                 Transformer-FeedForward-Add[1][0]
                                                                 Transformer-FeedForward-Add[2][0]
                                                                 Transformer-FeedForward-Add[3][0]
                                                                 Transformer-FeedForward-Add[4][0]
                                                                 Transformer-FeedForward-Add[5][0]
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 384)          0           Transformer-FeedForward-Norm[5][0
__________________________________________________________________________________________________
dropout (Dropout)               (None, 384)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           5775        dropout[0][0]                    
==================================================================================================
Total params: 4,600,207
Trainable params: 5,775
Non-trainable params: 4,594,432
__________________________________________________________________________________________________
2022-05-11 15:01:44.209889: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 27s - loss: 2.1543 - accuracy: 0.3411
dev acc=43.65%
test acc=44.34%
Epoch 2/10000
188/188 - 21s - loss: 1.8211 - accuracy: 0.4288
dev acc=45.31%
test acc=46.10%
Epoch 3/10000
188/188 - 21s - loss: 1.7464 - accuracy: 0.4412
dev acc=45.93%
test acc=46.73%
Epoch 4/10000
188/188 - 21s - loss: 1.7237 - accuracy: 0.4428
dev acc=46.12%
test acc=47.17%
Epoch 5/10000
188/188 - 21s - loss: 1.7150 - accuracy: 0.4459
dev acc=46.53%
test acc=47.45%
Epoch 6/10000
188/188 - 20s - loss: 1.7003 - accuracy: 0.4489
dev acc=46.74%
test acc=47.52%
Epoch 7/10000
188/188 - 21s - loss: 1.6984 - accuracy: 0.4509
dev acc=46.65%
Epoch 8/10000
188/188 - 20s - loss: 1.6987 - accuracy: 0.4503
dev acc=46.80%
test acc=48.00%
Epoch 9/10000
188/188 - 21s - loss: 1.6916 - accuracy: 0.4501
dev acc=47.15%
test acc=47.90%
Epoch 10/10000
188/188 - 20s - loss: 1.6936 - accuracy: 0.4492
dev acc=46.98%
Epoch 11/10000
188/188 - 21s - loss: 1.6930 - accuracy: 0.4503
dev acc=47.11%
Epoch 12/10000
188/188 - 21s - loss: 1.6902 - accuracy: 0.4505
dev acc=47.08%
Epoch 13/10000
188/188 - 20s - loss: 1.6870 - accuracy: 0.4516
dev acc=47.19%
test acc=48.15%
Epoch 14/10000
188/188 - 20s - loss: 1.6929 - accuracy: 0.4495
dev acc=47.17%
Epoch 15/10000
188/188 - 20s - loss: 1.6907 - accuracy: 0.4489
dev acc=47.17%
Epoch 16/10000
188/188 - 21s - loss: 1.6912 - accuracy: 0.4526
dev acc=47.17%
Epoch 17/10000
188/188 - 20s - loss: 1.6929 - accuracy: 0.4526
dev acc=47.21%
test acc=48.26%
Epoch 18/10000
188/188 - 20s - loss: 1.6904 - accuracy: 0.4490
dev acc=47.26%
test acc=48.26%
Epoch 19/10000
188/188 - 20s - loss: 1.6908 - accuracy: 0.4519
dev acc=47.19%
Epoch 20/10000
188/188 - 21s - loss: 1.6912 - accuracy: 0.4524
dev acc=47.17%
Epoch 21/10000
188/188 - 21s - loss: 1.6935 - accuracy: 0.4484
dev acc=47.23%
Epoch 22/10000
188/188 - 21s - loss: 1.6913 - accuracy: 0.4501
dev acc=47.19%
Epoch 23/10000
188/188 - 21s - loss: 1.6848 - accuracy: 0.4529
dev acc=47.43%
test acc=48.30%
Epoch 24/10000
188/188 - 20s - loss: 1.6888 - accuracy: 0.4533
dev acc=47.32%
Epoch 25/10000
188/188 - 21s - loss: 1.6882 - accuracy: 0.4531
dev acc=47.40%
Epoch 26/10000
188/188 - 20s - loss: 1.6925 - accuracy: 0.4492
dev acc=47.64%
test acc=48.42%
Epoch 27/10000
188/188 - 21s - loss: 1.6897 - accuracy: 0.4507
dev acc=47.53%
Epoch 28/10000
188/188 - 21s - loss: 1.6867 - accuracy: 0.4529
dev acc=47.51%
Epoch 29/10000
188/188 - 21s - loss: 1.6882 - accuracy: 0.4519
dev acc=47.43%
Epoch 30/10000
188/188 - 21s - loss: 1.6897 - accuracy: 0.4500
dev acc=47.45%
Epoch 31/10000
188/188 - 20s - loss: 1.6939 - accuracy: 0.4493
dev acc=47.58%
time used=985.7s
2022-05-11 15:18:02.183709: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert_zh/albert_tiny_google_zh_489k/albert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert_zh/albert_tiny_google_zh_489k/albert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='albert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert_zh/albert_tiny_google_zh_489k/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_finance': 0, 'news_agriculture': 1, 'news_entertainment': 2, 'news_world': 3, 'news_travel': 4, 'news_edu': 5, 'news_game': 6, 'news_stock': 7, 'news_culture': 8, 'news_car': 9, 'news_sports': 10, 'news_tech': 11, 'news_house': 12, 'news_story': 13, 'news_military': 14}
index_labels_dict={0: 'news_finance', 1: 'news_agriculture', 2: 'news_entertainment', 3: 'news_world', 4: 'news_travel', 5: 'news_edu', 6: 'news_game', 7: 'news_stock', 8: 'news_culture', 9: 'news_car', 10: 'news_sports', 11: 'news_tech', 12: 'news_house', 13: 'news_story', 14: 'news_military'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-11 15:18:10.643893: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-11 15:18:11.898901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 312)    6591936     Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 312)    624         Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 312)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 312)    159744      Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 312)    624         Embedding-Position[0][0]         
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 312)    390624      Embedding-Norm[0][0]             
                                                                 Embedding-Norm[0][0]             
                                                                 Embedding-Norm[0][0]             
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[2][0
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 312)    0           Embedding-Norm[0][0]             
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 312)    624         Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-FeedForward (FeedFo (None, None, 312)    780312      Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-FeedForward-Add (Ad (None, None, 312)    0           Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[0][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[1][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[2][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[3][0]    
__________________________________________________________________________________________________
Transformer-FeedForward-Norm (L (None, None, 312)    624         Transformer-FeedForward-Add[0][0]
                                                                 Transformer-FeedForward-Add[1][0]
                                                                 Transformer-FeedForward-Add[2][0]
                                                                 Transformer-FeedForward-Add[3][0]
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 312)          0           Transformer-FeedForward-Norm[3][0
__________________________________________________________________________________________________
dropout (Dropout)               (None, 312)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           4695        dropout[0][0]                    
==================================================================================================
Total params: 7,929,807
Trainable params: 4,695
Non-trainable params: 7,925,112
__________________________________________________________________________________________________
2022-05-11 15:18:13.320419: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 17s - loss: 2.4163 - accuracy: 0.2516
dev acc=34.75%
test acc=35.02%
Epoch 2/10000
188/188 - 15s - loss: 2.1307 - accuracy: 0.3670
dev acc=39.77%
test acc=40.19%
Epoch 3/10000
188/188 - 17s - loss: 1.9932 - accuracy: 0.3928
dev acc=41.74%
test acc=42.25%
Epoch 4/10000
188/188 - 17s - loss: 1.9171 - accuracy: 0.4053
dev acc=43.05%
test acc=43.40%
Epoch 5/10000
188/188 - 17s - loss: 1.8698 - accuracy: 0.4121
dev acc=43.91%
test acc=44.60%
Epoch 6/10000
188/188 - 17s - loss: 1.8402 - accuracy: 0.4190
dev acc=44.36%
test acc=45.00%
Epoch 7/10000
188/188 - 16s - loss: 1.8180 - accuracy: 0.4241
dev acc=44.68%
test acc=45.35%
Epoch 8/10000
188/188 - 16s - loss: 1.8040 - accuracy: 0.4245
dev acc=44.96%
test acc=45.40%
Epoch 9/10000
188/188 - 17s - loss: 1.7970 - accuracy: 0.4259
dev acc=45.41%
test acc=45.65%
Epoch 10/10000
188/188 - 16s - loss: 1.7903 - accuracy: 0.4275
dev acc=45.61%
test acc=45.91%
Epoch 11/10000
188/188 - 15s - loss: 1.7734 - accuracy: 0.4339
dev acc=45.56%
Epoch 12/10000
188/188 - 17s - loss: 1.7746 - accuracy: 0.4310
dev acc=45.76%
test acc=46.18%
Epoch 13/10000
188/188 - 16s - loss: 1.7712 - accuracy: 0.4314
dev acc=45.78%
test acc=46.43%
Epoch 14/10000
188/188 - 17s - loss: 1.7674 - accuracy: 0.4333
dev acc=45.93%
test acc=46.69%
Epoch 15/10000
188/188 - 15s - loss: 1.7607 - accuracy: 0.4339
dev acc=45.93%
Epoch 16/10000
188/188 - 16s - loss: 1.7594 - accuracy: 0.4339
dev acc=46.12%
test acc=46.70%
Epoch 17/10000
188/188 - 17s - loss: 1.7599 - accuracy: 0.4334
dev acc=45.99%
Epoch 18/10000
188/188 - 17s - loss: 1.7589 - accuracy: 0.4317
dev acc=46.10%
Epoch 19/10000
188/188 - 17s - loss: 1.7583 - accuracy: 0.4327
dev acc=46.08%
Epoch 20/10000
188/188 - 17s - loss: 1.7550 - accuracy: 0.4349
dev acc=46.10%
Epoch 21/10000
188/188 - 17s - loss: 1.7535 - accuracy: 0.4339
dev acc=45.93%
time used=606.1s
2022-05-11 15:28:11.939960: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert_zh/albert_tiny_zh_google/albert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert_zh/albert_tiny_zh_google/albert_config_tiny_g.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='albert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert_zh/albert_tiny_zh_google/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_story': 0, 'news_world': 1, 'news_culture': 2, 'news_travel': 3, 'news_agriculture': 4, 'news_house': 5, 'news_car': 6, 'news_entertainment': 7, 'news_military': 8, 'news_tech': 9, 'news_finance': 10, 'news_edu': 11, 'news_game': 12, 'news_stock': 13, 'news_sports': 14}
index_labels_dict={0: 'news_story', 1: 'news_world', 2: 'news_culture', 3: 'news_travel', 4: 'news_agriculture', 5: 'news_house', 6: 'news_car', 7: 'news_entertainment', 8: 'news_military', 9: 'news_tech', 10: 'news_finance', 11: 'news_edu', 12: 'news_game', 13: 'news_stock', 14: 'news_sports'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-11 15:28:20.211237: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-11 15:28:21.587097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 128)    2704384     Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Mapping (Dense)       (None, None, 312)    40248       Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 312)    390624      Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[2][0
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 312)    0           Embedding-Mapping[0][0]          
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 312)    624         Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-FeedForward (FeedFo (None, None, 312)    780312      Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-FeedForward-Add (Ad (None, None, 312)    0           Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[0][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[1][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[2][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[3][0]    
__________________________________________________________________________________________________
Transformer-FeedForward-Norm (L (None, None, 312)    624         Transformer-FeedForward-Add[0][0]
                                                                 Transformer-FeedForward-Add[1][0]
                                                                 Transformer-FeedForward-Add[2][0]
                                                                 Transformer-FeedForward-Add[3][0]
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 312)          0           Transformer-FeedForward-Norm[3][0
__________________________________________________________________________________________________
dropout (Dropout)               (None, 312)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           4695        dropout[0][0]                    
==================================================================================================
Total params: 3,987,559
Trainable params: 4,695
Non-trainable params: 3,982,864
__________________________________________________________________________________________________
2022-05-11 15:28:22.988256: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 18s - loss: 2.1745 - accuracy: 0.3295
dev acc=43.27%
test acc=43.14%
Epoch 2/10000
188/188 - 15s - loss: 1.8704 - accuracy: 0.4111
dev acc=44.51%
test acc=45.04%
Epoch 3/10000
188/188 - 15s - loss: 1.8074 - accuracy: 0.4229
dev acc=44.81%
test acc=45.72%
Epoch 4/10000
188/188 - 15s - loss: 1.7867 - accuracy: 0.4249
dev acc=45.28%
test acc=46.13%
Epoch 5/10000
188/188 - 15s - loss: 1.7808 - accuracy: 0.4265
dev acc=45.48%
test acc=46.21%
Epoch 6/10000
188/188 - 15s - loss: 1.7709 - accuracy: 0.4272
dev acc=45.71%
test acc=46.48%
Epoch 7/10000
188/188 - 15s - loss: 1.7663 - accuracy: 0.4312
dev acc=45.90%
test acc=46.62%
Epoch 8/10000
188/188 - 15s - loss: 1.7650 - accuracy: 0.4323
dev acc=46.03%
test acc=46.64%
Epoch 9/10000
188/188 - 15s - loss: 1.7633 - accuracy: 0.4309
dev acc=46.14%
test acc=46.77%
Epoch 10/10000
188/188 - 15s - loss: 1.7645 - accuracy: 0.4307
dev acc=46.14%
Epoch 11/10000
188/188 - 16s - loss: 1.7644 - accuracy: 0.4306
dev acc=46.27%
test acc=46.73%
Epoch 12/10000
188/188 - 15s - loss: 1.7627 - accuracy: 0.4319
dev acc=46.16%
Epoch 13/10000
188/188 - 15s - loss: 1.7635 - accuracy: 0.4288
dev acc=46.20%
Epoch 14/10000
188/188 - 15s - loss: 1.7633 - accuracy: 0.4297
dev acc=46.42%
test acc=46.76%
Epoch 15/10000
188/188 - 15s - loss: 1.7589 - accuracy: 0.4317
dev acc=46.61%
test acc=46.90%
Epoch 16/10000
188/188 - 15s - loss: 1.7609 - accuracy: 0.4310
dev acc=46.68%
test acc=46.97%
Epoch 17/10000
188/188 - 16s - loss: 1.7569 - accuracy: 0.4302
dev acc=46.61%
Epoch 18/10000
188/188 - 15s - loss: 1.7625 - accuracy: 0.4287
dev acc=46.74%
test acc=46.95%
Epoch 19/10000
188/188 - 15s - loss: 1.7662 - accuracy: 0.4277
dev acc=46.72%
Epoch 20/10000
188/188 - 14s - loss: 1.7629 - accuracy: 0.4300
dev acc=46.44%
Epoch 21/10000
188/188 - 16s - loss: 1.7596 - accuracy: 0.4305
dev acc=46.59%
Epoch 22/10000
188/188 - 15s - loss: 1.7633 - accuracy: 0.4311
dev acc=46.14%
Epoch 23/10000
188/188 - 15s - loss: 1.7612 - accuracy: 0.4301
dev acc=46.44%
time used=620.9s
2022-05-11 15:38:36.073093: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert_zh/albert_xlarge_google_zh_183k/albert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert_zh/albert_xlarge_google_zh_183k/albert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='albert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/albert_zh/albert_xlarge_google_zh_183k/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_military': 0, 'news_tech': 1, 'news_sports': 2, 'news_travel': 3, 'news_stock': 4, 'news_finance': 5, 'news_edu': 6, 'news_agriculture': 7, 'news_story': 8, 'news_culture': 9, 'news_game': 10, 'news_house': 11, 'news_car': 12, 'news_world': 13, 'news_entertainment': 14}
index_labels_dict={0: 'news_military', 1: 'news_tech', 2: 'news_sports', 3: 'news_travel', 4: 'news_stock', 5: 'news_finance', 6: 'news_edu', 7: 'news_agriculture', 8: 'news_story', 9: 'news_culture', 10: 'news_game', 11: 'news_house', 12: 'news_car', 13: 'news_world', 14: 'news_entertainment'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-11 15:38:45.094488: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-11 15:38:46.498675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 2048)   43270144    Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 2048)   4096        Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 2048)   0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 2048)   1048576     Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 2048)   4096        Embedding-Position[0][0]         
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 2048)   16785408    Embedding-Norm[0][0]             
                                                                 Embedding-Norm[0][0]             
                                                                 Embedding-Norm[0][0]             
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-FeedForward-Norm[11][
                                                                 Transformer-FeedForward-Norm[11][
                                                                 Transformer-FeedForward-Norm[11][
                                                                 Transformer-FeedForward-Norm[12][
                                                                 Transformer-FeedForward-Norm[12][
                                                                 Transformer-FeedForward-Norm[12][
                                                                 Transformer-FeedForward-Norm[13][
                                                                 Transformer-FeedForward-Norm[13][
                                                                 Transformer-FeedForward-Norm[13][
                                                                 Transformer-FeedForward-Norm[14][
                                                                 Transformer-FeedForward-Norm[14][
                                                                 Transformer-FeedForward-Norm[14][
                                                                 Transformer-FeedForward-Norm[15][
                                                                 Transformer-FeedForward-Norm[15][
                                                                 Transformer-FeedForward-Norm[15][
                                                                 Transformer-FeedForward-Norm[16][
                                                                 Transformer-FeedForward-Norm[16][
                                                                 Transformer-FeedForward-Norm[16][
                                                                 Transformer-FeedForward-Norm[17][
                                                                 Transformer-FeedForward-Norm[17][
                                                                 Transformer-FeedForward-Norm[17][
                                                                 Transformer-FeedForward-Norm[18][
                                                                 Transformer-FeedForward-Norm[18][
                                                                 Transformer-FeedForward-Norm[18][
                                                                 Transformer-FeedForward-Norm[19][
                                                                 Transformer-FeedForward-Norm[19][
                                                                 Transformer-FeedForward-Norm[19][
                                                                 Transformer-FeedForward-Norm[20][
                                                                 Transformer-FeedForward-Norm[20][
                                                                 Transformer-FeedForward-Norm[20][
                                                                 Transformer-FeedForward-Norm[21][
                                                                 Transformer-FeedForward-Norm[21][
                                                                 Transformer-FeedForward-Norm[21][
                                                                 Transformer-FeedForward-Norm[22][
                                                                 Transformer-FeedForward-Norm[22][
                                                                 Transformer-FeedForward-Norm[22][
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 2048)   0           Embedding-Norm[0][0]             
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[0][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[1][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[2][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[3][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[4][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[5][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[6][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[7][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[8][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[9][0
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[10][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[11][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[12][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[13][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[14][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[15][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[16][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[17][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[18][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[19][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[20][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[21][
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward-Norm[22][
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-MultiHeadSelfAttent (None, None, 2048)   4096        Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-FeedForward (FeedFo (None, None, 2048)   33564672    Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
Transformer-FeedForward-Add (Ad (None, None, 2048)   0           Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[0][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[1][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[2][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[3][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[4][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[5][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[6][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[7][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[8][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[9][0]    
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[10][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[11][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[12][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[13][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[14][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[15][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[16][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[17][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[18][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[19][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[20][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[21][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[22][0]   
                                                                 Transformer-MultiHeadSelfAttentio
                                                                 Transformer-FeedForward[23][0]   
__________________________________________________________________________________________________
Transformer-FeedForward-Norm (L (None, None, 2048)   4096        Transformer-FeedForward-Add[0][0]
                                                                 Transformer-FeedForward-Add[1][0]
                                                                 Transformer-FeedForward-Add[2][0]
                                                                 Transformer-FeedForward-Add[3][0]
                                                                 Transformer-FeedForward-Add[4][0]
                                                                 Transformer-FeedForward-Add[5][0]
                                                                 Transformer-FeedForward-Add[6][0]
                                                                 Transformer-FeedForward-Add[7][0]
                                                                 Transformer-FeedForward-Add[8][0]
                                                                 Transformer-FeedForward-Add[9][0]
                                                                 Transformer-FeedForward-Add[10][0
                                                                 Transformer-FeedForward-Add[11][0
                                                                 Transformer-FeedForward-Add[12][0
                                                                 Transformer-FeedForward-Add[13][0
                                                                 Transformer-FeedForward-Add[14][0
                                                                 Transformer-FeedForward-Add[15][0
                                                                 Transformer-FeedForward-Add[16][0
                                                                 Transformer-FeedForward-Add[17][0
                                                                 Transformer-FeedForward-Add[18][0
                                                                 Transformer-FeedForward-Add[19][0
                                                                 Transformer-FeedForward-Add[20][0
                                                                 Transformer-FeedForward-Add[21][0
                                                                 Transformer-FeedForward-Add[22][0
                                                                 Transformer-FeedForward-Add[23][0
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 2048)         0           Transformer-FeedForward-Norm[23][
__________________________________________________________________________________________________
dropout (Dropout)               (None, 2048)         0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           30735       dropout[0][0]                    
==================================================================================================
Total params: 94,715,919
Trainable params: 30,735
Non-trainable params: 94,685,184
__________________________________________________________________________________________________
2022-05-11 15:38:52.139891: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 850s - loss: 1.8646 - accuracy: 0.4363
dev acc=50.54%
test acc=51.17%
Epoch 2/10000
188/188 - 834s - loss: 1.5485 - accuracy: 0.5037
dev acc=51.80%
test acc=51.80%
Epoch 3/10000
188/188 - 834s - loss: 1.4968 - accuracy: 0.5128
dev acc=52.04%
test acc=51.91%
Epoch 4/10000
188/188 - 833s - loss: 1.4691 - accuracy: 0.5191
dev acc=52.44%
test acc=52.20%
Epoch 5/10000
188/188 - 829s - loss: 1.4541 - accuracy: 0.5222
dev acc=52.59%
test acc=52.38%
Epoch 6/10000
188/188 - 826s - loss: 1.4439 - accuracy: 0.5237
dev acc=52.79%
test acc=52.50%
Epoch 7/10000
188/188 - 827s - loss: 1.4376 - accuracy: 0.5262
dev acc=52.70%
Epoch 8/10000
188/188 - 831s - loss: 1.4289 - accuracy: 0.5291
dev acc=52.75%
Epoch 9/10000
188/188 - 831s - loss: 1.4202 - accuracy: 0.5308
dev acc=52.77%
Epoch 10/10000
188/188 - 831s - loss: 1.4188 - accuracy: 0.5329
dev acc=52.77%
Epoch 11/10000
188/188 - 828s - loss: 1.4174 - accuracy: 0.5323
dev acc=52.90%
test acc=52.77%
Epoch 12/10000
188/188 - 830s - loss: 1.4143 - accuracy: 0.5323
dev acc=52.98%
test acc=52.82%
Epoch 13/10000
188/188 - 828s - loss: 1.4085 - accuracy: 0.5338
dev acc=53.13%
test acc=52.87%
Epoch 14/10000
188/188 - 827s - loss: 1.4060 - accuracy: 0.5328
dev acc=52.89%
Epoch 15/10000
188/188 - 825s - loss: 1.4096 - accuracy: 0.5332
dev acc=52.94%
Epoch 16/10000
188/188 - 828s - loss: 1.3998 - accuracy: 0.5348
dev acc=53.04%
Epoch 17/10000
188/188 - 828s - loss: 1.4027 - accuracy: 0.5376
dev acc=52.92%
Epoch 18/10000
188/188 - 828s - loss: 1.4009 - accuracy: 0.5354
dev acc=52.92%
time used=18557.2s
2022-05-11 20:47:57.248737: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/bert/chinese_L-12_H-768_A-12/bert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/bert/chinese_L-12_H-768_A-12/bert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='bert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/bert/chinese_L-12_H-768_A-12/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_military': 0, 'news_story': 1, 'news_agriculture': 2, 'news_game': 3, 'news_sports': 4, 'news_culture': 5, 'news_entertainment': 6, 'news_world': 7, 'news_travel': 8, 'news_tech': 9, 'news_car': 10, 'news_edu': 11, 'news_finance': 12, 'news_stock': 13, 'news_house': 14}
index_labels_dict={0: 'news_military', 1: 'news_story', 2: 'news_agriculture', 3: 'news_game', 4: 'news_sports', 5: 'news_culture', 6: 'news_entertainment', 7: 'news_world', 8: 'news_travel', 9: 'news_tech', 10: 'news_car', 11: 'news_edu', 12: 'news_finance', 13: 'news_stock', 14: 'news_house'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-11 20:48:05.521889: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-11 20:48:06.451394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 768)    16226304    Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 768)          0           Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
dropout (Dropout)               (None, 768)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           11535       dropout[0][0]                    
==================================================================================================
Total params: 101,688,591
Trainable params: 11,535
Non-trainable params: 101,677,056
__________________________________________________________________________________________________
2022-05-11 20:48:10.038823: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 109s - loss: 1.7330 - accuracy: 0.4716
dev acc=53.84%
test acc=53.96%
Epoch 2/10000
188/188 - 97s - loss: 1.4130 - accuracy: 0.5327
dev acc=54.09%
test acc=54.46%
Epoch 3/10000
188/188 - 97s - loss: 1.3814 - accuracy: 0.5376
dev acc=54.25%
test acc=54.77%
Epoch 4/10000
188/188 - 97s - loss: 1.3728 - accuracy: 0.5376
dev acc=54.31%
test acc=54.84%
Epoch 5/10000
188/188 - 98s - loss: 1.3655 - accuracy: 0.5391
dev acc=54.54%
test acc=54.94%
Epoch 6/10000
188/188 - 98s - loss: 1.3629 - accuracy: 0.5396
dev acc=54.52%
Epoch 7/10000
188/188 - 98s - loss: 1.3524 - accuracy: 0.5428
dev acc=54.55%
test acc=54.98%
Epoch 8/10000
188/188 - 98s - loss: 1.3605 - accuracy: 0.5398
dev acc=54.84%
test acc=55.15%
Epoch 9/10000
188/188 - 98s - loss: 1.3561 - accuracy: 0.5408
dev acc=54.97%
test acc=55.16%
Epoch 10/10000
188/188 - 98s - loss: 1.3520 - accuracy: 0.5408
dev acc=55.06%
test acc=55.26%
Epoch 11/10000
188/188 - 98s - loss: 1.3498 - accuracy: 0.5464
dev acc=54.89%
Epoch 12/10000
188/188 - 98s - loss: 1.3507 - accuracy: 0.5416
dev acc=55.04%
Epoch 13/10000
188/188 - 98s - loss: 1.3512 - accuracy: 0.5395
dev acc=55.00%
Epoch 14/10000
188/188 - 98s - loss: 1.3535 - accuracy: 0.5429
dev acc=55.00%
Epoch 15/10000
188/188 - 98s - loss: 1.3522 - accuracy: 0.5413
dev acc=54.91%
time used=2000.9s
2022-05-11 21:21:21.917104: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-BERT-wwm/chinese_rbt3_L-3_H-768_A-12/bert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-BERT-wwm/chinese_rbt3_L-3_H-768_A-12/bert_config_rbt3.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='bert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-BERT-wwm/chinese_rbt3_L-3_H-768_A-12/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_world': 0, 'news_entertainment': 1, 'news_car': 2, 'news_house': 3, 'news_finance': 4, 'news_edu': 5, 'news_agriculture': 6, 'news_tech': 7, 'news_travel': 8, 'news_sports': 9, 'news_stock': 10, 'news_culture': 11, 'news_game': 12, 'news_military': 13, 'news_story': 14}
index_labels_dict={0: 'news_world', 1: 'news_entertainment', 2: 'news_car', 3: 'news_house', 4: 'news_finance', 5: 'news_edu', 6: 'news_agriculture', 7: 'news_tech', 8: 'news_travel', 9: 'news_sports', 10: 'news_stock', 11: 'news_culture', 12: 'news_game', 13: 'news_military', 14: 'news_story'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-11 21:21:29.780055: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-11 21:21:30.694888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 768)    16226304    Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 768)          0           Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 768)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           11535       dropout[0][0]                    
==================================================================================================
Total params: 37,897,743
Trainable params: 11,535
Non-trainable params: 37,886,208
__________________________________________________________________________________________________
2022-05-11 21:21:32.218018: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 29s - loss: 1.9596 - accuracy: 0.4175
dev acc=50.64%
test acc=50.39%
Epoch 2/10000
188/188 - 25s - loss: 1.5817 - accuracy: 0.4939
dev acc=51.27%
test acc=51.66%
Epoch 3/10000
188/188 - 25s - loss: 1.5205 - accuracy: 0.5056
dev acc=51.84%
test acc=52.16%
Epoch 4/10000
188/188 - 25s - loss: 1.4992 - accuracy: 0.5073
dev acc=51.76%
Epoch 5/10000
188/188 - 25s - loss: 1.4924 - accuracy: 0.5098
dev acc=51.86%
test acc=52.67%
Epoch 6/10000
188/188 - 25s - loss: 1.4832 - accuracy: 0.5082
dev acc=52.10%
test acc=52.75%
Epoch 7/10000
188/188 - 25s - loss: 1.4820 - accuracy: 0.5119
dev acc=51.99%
Epoch 8/10000
188/188 - 25s - loss: 1.4773 - accuracy: 0.5112
dev acc=51.87%
Epoch 9/10000
188/188 - 25s - loss: 1.4725 - accuracy: 0.5115
dev acc=52.25%
test acc=53.11%
Epoch 10/10000
188/188 - 25s - loss: 1.4751 - accuracy: 0.5129
dev acc=52.04%
Epoch 11/10000
188/188 - 25s - loss: 1.4716 - accuracy: 0.5147
dev acc=51.99%
Epoch 12/10000
188/188 - 25s - loss: 1.4707 - accuracy: 0.5133
dev acc=52.17%
Epoch 13/10000
188/188 - 25s - loss: 1.4680 - accuracy: 0.5155
dev acc=52.16%
Epoch 14/10000
188/188 - 25s - loss: 1.4668 - accuracy: 0.5135
dev acc=52.17%
time used=511.3s
2022-05-11 21:29:56.518369: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-BERT-wwm/chinese_rbtl3_L-3_H-1024_A-16/bert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-BERT-wwm/chinese_rbtl3_L-3_H-1024_A-16/bert_config_rbtl3.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='bert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-BERT-wwm/chinese_rbtl3_L-3_H-1024_A-16/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_finance': 0, 'news_world': 1, 'news_tech': 2, 'news_house': 3, 'news_story': 4, 'news_car': 5, 'news_stock': 6, 'news_entertainment': 7, 'news_military': 8, 'news_edu': 9, 'news_game': 10, 'news_culture': 11, 'news_sports': 12, 'news_agriculture': 13, 'news_travel': 14}
index_labels_dict={0: 'news_finance', 1: 'news_world', 2: 'news_tech', 3: 'news_house', 4: 'news_story', 5: 'news_car', 6: 'news_stock', 7: 'news_entertainment', 8: 'news_military', 9: 'news_edu', 10: 'news_game', 11: 'news_culture', 12: 'news_sports', 13: 'news_agriculture', 14: 'news_travel'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-11 21:30:04.328868: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-11 21:30:05.232054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 1024)   21635072    Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 1024)   2048        Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 1024)   0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 1024)   524288      Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 1024)   2048        Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 1024)   0           Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 1024)   4198400     Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 1024)   0           Embedding-Dropout[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 1024)   8393728     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 1024)   0           Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 1024)   0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 1024)   2048        Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 1024)   4198400     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 1024)   8393728     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 1024)   0           Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 1024)   0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 1024)   2048        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 1024)   4198400     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 1024)   8393728     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 1024)   0           Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 1024)   0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 1024)   2048        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 1024)         0           Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 1024)         0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           15375       dropout[0][0]                    
==================================================================================================
Total params: 59,967,503
Trainable params: 15,375
Non-trainable params: 59,952,128
__________________________________________________________________________________________________
2022-05-11 21:30:06.984173: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 41s - loss: 1.8417 - accuracy: 0.4396
dev acc=51.20%
test acc=51.89%
Epoch 2/10000
188/188 - 37s - loss: 1.5227 - accuracy: 0.5064
dev acc=52.14%
test acc=52.75%
Epoch 3/10000
188/188 - 38s - loss: 1.4821 - accuracy: 0.5113
dev acc=52.46%
test acc=53.19%
Epoch 4/10000
188/188 - 38s - loss: 1.4687 - accuracy: 0.5152
dev acc=53.11%
test acc=53.11%
Epoch 5/10000
188/188 - 38s - loss: 1.4563 - accuracy: 0.5189
dev acc=52.90%
Epoch 6/10000
188/188 - 38s - loss: 1.4526 - accuracy: 0.5174
dev acc=53.45%
test acc=53.19%
Epoch 7/10000
188/188 - 38s - loss: 1.4527 - accuracy: 0.5181
dev acc=53.52%
test acc=53.38%
Epoch 8/10000
188/188 - 38s - loss: 1.4453 - accuracy: 0.5188
dev acc=53.60%
test acc=53.42%
Epoch 9/10000
188/188 - 38s - loss: 1.4444 - accuracy: 0.5210
dev acc=53.64%
test acc=53.29%
Epoch 10/10000
188/188 - 38s - loss: 1.4452 - accuracy: 0.5200
dev acc=53.62%
Epoch 11/10000
188/188 - 38s - loss: 1.4444 - accuracy: 0.5203
dev acc=53.56%
Epoch 12/10000
188/188 - 38s - loss: 1.4449 - accuracy: 0.5223
dev acc=53.84%
test acc=53.45%
Epoch 13/10000
188/188 - 38s - loss: 1.4420 - accuracy: 0.5226
dev acc=53.56%
Epoch 14/10000
188/188 - 38s - loss: 1.4400 - accuracy: 0.5195
dev acc=53.47%
Epoch 15/10000
188/188 - 38s - loss: 1.4375 - accuracy: 0.5233
dev acc=53.67%
Epoch 16/10000
188/188 - 38s - loss: 1.4355 - accuracy: 0.5245
dev acc=53.86%
test acc=53.66%
Epoch 17/10000
188/188 - 38s - loss: 1.4395 - accuracy: 0.5214
dev acc=53.77%
Epoch 18/10000
188/188 - 38s - loss: 1.4363 - accuracy: 0.5243
dev acc=53.92%
test acc=53.33%
Epoch 19/10000
188/188 - 38s - loss: 1.4388 - accuracy: 0.5208
dev acc=53.94%
test acc=53.34%
Epoch 20/10000
188/188 - 38s - loss: 1.4407 - accuracy: 0.5204
dev acc=53.64%
Epoch 21/10000
188/188 - 38s - loss: 1.4398 - accuracy: 0.5225
dev acc=53.65%
Epoch 22/10000
188/188 - 38s - loss: 1.4404 - accuracy: 0.5227
dev acc=53.77%
Epoch 23/10000
188/188 - 38s - loss: 1.4398 - accuracy: 0.5217
dev acc=53.86%
Epoch 24/10000
188/188 - 38s - loss: 1.4385 - accuracy: 0.5215
dev acc=53.99%
test acc=53.43%
Epoch 25/10000
188/188 - 38s - loss: 1.4394 - accuracy: 0.5218
dev acc=53.82%
Epoch 26/10000
188/188 - 38s - loss: 1.4348 - accuracy: 0.5227
dev acc=53.80%
Epoch 27/10000
188/188 - 38s - loss: 1.4420 - accuracy: 0.5193
dev acc=53.77%
Epoch 28/10000
188/188 - 38s - loss: 1.4419 - accuracy: 0.5204
dev acc=53.71%
Epoch 29/10000
188/188 - 38s - loss: 1.4404 - accuracy: 0.5193
dev acc=53.95%
time used=1497.6s
2022-05-11 21:54:57.367121: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-BERT-wwm/chinese_roberta_wwm_ext_L-12_H-768_A-12/bert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-BERT-wwm/chinese_roberta_wwm_ext_L-12_H-768_A-12/bert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='bert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-BERT-wwm/chinese_roberta_wwm_ext_L-12_H-768_A-12/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_edu': 0, 'news_finance': 1, 'news_world': 2, 'news_travel': 3, 'news_house': 4, 'news_car': 5, 'news_agriculture': 6, 'news_stock': 7, 'news_tech': 8, 'news_game': 9, 'news_culture': 10, 'news_story': 11, 'news_entertainment': 12, 'news_sports': 13, 'news_military': 14}
index_labels_dict={0: 'news_edu', 1: 'news_finance', 2: 'news_world', 3: 'news_travel', 4: 'news_house', 5: 'news_car', 6: 'news_agriculture', 7: 'news_stock', 8: 'news_tech', 9: 'news_game', 10: 'news_culture', 11: 'news_story', 12: 'news_entertainment', 13: 'news_sports', 14: 'news_military'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-11 21:55:05.157583: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-11 21:55:06.036977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 768)    16226304    Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 768)          0           Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
dropout (Dropout)               (None, 768)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           11535       dropout[0][0]                    
==================================================================================================
Total params: 101,688,591
Trainable params: 11,535
Non-trainable params: 101,677,056
__________________________________________________________________________________________________
2022-05-11 21:55:09.887845: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 109s - loss: 1.7061 - accuracy: 0.4820
dev acc=55.47%
test acc=56.01%
Epoch 2/10000
188/188 - 97s - loss: 1.3721 - accuracy: 0.5427
dev acc=55.90%
test acc=56.18%
Epoch 3/10000
188/188 - 98s - loss: 1.3399 - accuracy: 0.5481
dev acc=56.17%
test acc=56.16%
Epoch 4/10000
188/188 - 98s - loss: 1.3308 - accuracy: 0.5484
dev acc=56.48%
test acc=56.42%
Epoch 5/10000
188/188 - 97s - loss: 1.3256 - accuracy: 0.5510
dev acc=56.63%
test acc=56.22%
Epoch 6/10000
188/188 - 98s - loss: 1.3160 - accuracy: 0.5490
dev acc=56.45%
Epoch 7/10000
188/188 - 97s - loss: 1.3105 - accuracy: 0.5525
dev acc=56.47%
Epoch 8/10000
188/188 - 98s - loss: 1.3158 - accuracy: 0.5488
dev acc=56.43%
Epoch 9/10000
188/188 - 97s - loss: 1.3133 - accuracy: 0.5513
dev acc=56.60%
Epoch 10/10000
188/188 - 98s - loss: 1.3085 - accuracy: 0.5519
dev acc=56.56%
time used=1317.5s
2022-05-11 22:16:58.133205: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-BERT-wwm/chinese_roberta_wwm_large_ext_L-24_H-1024_A-16/bert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-BERT-wwm/chinese_roberta_wwm_large_ext_L-24_H-1024_A-16/bert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='bert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-BERT-wwm/chinese_roberta_wwm_large_ext_L-24_H-1024_A-16/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_sports': 0, 'news_game': 1, 'news_world': 2, 'news_finance': 3, 'news_stock': 4, 'news_edu': 5, 'news_story': 6, 'news_culture': 7, 'news_house': 8, 'news_military': 9, 'news_car': 10, 'news_entertainment': 11, 'news_travel': 12, 'news_tech': 13, 'news_agriculture': 14}
index_labels_dict={0: 'news_sports', 1: 'news_game', 2: 'news_world', 3: 'news_finance', 4: 'news_stock', 5: 'news_edu', 6: 'news_story', 7: 'news_culture', 8: 'news_house', 9: 'news_military', 10: 'news_car', 11: 'news_entertainment', 12: 'news_travel', 13: 'news_tech', 14: 'news_agriculture'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-11 22:17:06.180231: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-11 22:17:07.088108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 1024)   21635072    Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 1024)   2048        Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 1024)   0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 1024)   524288      Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 1024)   2048        Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 1024)   0           Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 1024)   4198400     Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 1024)   0           Embedding-Dropout[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 1024)   8393728     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 1024)   0           Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 1024)   0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 1024)   2048        Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 1024)   4198400     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 1024)   8393728     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 1024)   0           Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 1024)   0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 1024)   2048        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 1024)   4198400     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 1024)   8393728     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 1024)   0           Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 1024)   0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 1024)   2048        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 1024)   4198400     Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 1024)   8393728     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 1024)   0           Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 1024)   0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 1024)   2048        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 1024)   4198400     Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 1024)   8393728     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 1024)   0           Transformer-4-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 1024)   0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 1024)   2048        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 1024)   4198400     Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 1024)   8393728     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 1024)   0           Transformer-5-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 1024)   0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 1024)   2048        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 1024)   4198400     Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 1024)   8393728     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 1024)   0           Transformer-6-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 1024)   0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 1024)   2048        Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 1024)   4198400     Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 1024)   8393728     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 1024)   0           Transformer-7-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 1024)   0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 1024)   2048        Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 1024)   4198400     Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 1024)   8393728     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 1024)   0           Transformer-8-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 1024)   0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 1024)   2048        Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 1024)   4198400     Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 1024)   8393728     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 1024)   0           Transformer-9-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 1024)   0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 1024)   2048        Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 1024)   8393728     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 1024)   0           Transformer-10-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 1024)   0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 1024)   2048        Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 1024)   8393728     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 1024)   0           Transformer-11-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 1024)   0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 1024)   2048        Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-11-FeedForward-Norm[0
                                                                 Transformer-11-FeedForward-Norm[0
                                                                 Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-11-FeedForward-Norm[0
                                                                 Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-FeedForward (Fee (None, None, 1024)   8393728     Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-FeedForward-Drop (None, None, 1024)   0           Transformer-12-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-12-FeedForward-Add  (None, None, 1024)   0           Transformer-12-MultiHeadSelfAtten
                                                                 Transformer-12-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-12-FeedForward-Norm (None, None, 1024)   2048        Transformer-12-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-13-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-12-FeedForward-Norm[0
                                                                 Transformer-12-FeedForward-Norm[0
                                                                 Transformer-12-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-13-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-13-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-13-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-12-FeedForward-Norm[0
                                                                 Transformer-13-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-13-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-13-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-13-FeedForward (Fee (None, None, 1024)   8393728     Transformer-13-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-13-FeedForward-Drop (None, None, 1024)   0           Transformer-13-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-13-FeedForward-Add  (None, None, 1024)   0           Transformer-13-MultiHeadSelfAtten
                                                                 Transformer-13-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-13-FeedForward-Norm (None, None, 1024)   2048        Transformer-13-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-14-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-13-FeedForward-Norm[0
                                                                 Transformer-13-FeedForward-Norm[0
                                                                 Transformer-13-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-14-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-14-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-14-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-13-FeedForward-Norm[0
                                                                 Transformer-14-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-14-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-14-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-14-FeedForward (Fee (None, None, 1024)   8393728     Transformer-14-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-14-FeedForward-Drop (None, None, 1024)   0           Transformer-14-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-14-FeedForward-Add  (None, None, 1024)   0           Transformer-14-MultiHeadSelfAtten
                                                                 Transformer-14-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-14-FeedForward-Norm (None, None, 1024)   2048        Transformer-14-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-15-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-14-FeedForward-Norm[0
                                                                 Transformer-14-FeedForward-Norm[0
                                                                 Transformer-14-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-15-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-15-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-15-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-14-FeedForward-Norm[0
                                                                 Transformer-15-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-15-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-15-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-15-FeedForward (Fee (None, None, 1024)   8393728     Transformer-15-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-15-FeedForward-Drop (None, None, 1024)   0           Transformer-15-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-15-FeedForward-Add  (None, None, 1024)   0           Transformer-15-MultiHeadSelfAtten
                                                                 Transformer-15-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-15-FeedForward-Norm (None, None, 1024)   2048        Transformer-15-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-16-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-15-FeedForward-Norm[0
                                                                 Transformer-15-FeedForward-Norm[0
                                                                 Transformer-15-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-16-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-16-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-16-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-15-FeedForward-Norm[0
                                                                 Transformer-16-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-16-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-16-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-16-FeedForward (Fee (None, None, 1024)   8393728     Transformer-16-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-16-FeedForward-Drop (None, None, 1024)   0           Transformer-16-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-16-FeedForward-Add  (None, None, 1024)   0           Transformer-16-MultiHeadSelfAtten
                                                                 Transformer-16-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-16-FeedForward-Norm (None, None, 1024)   2048        Transformer-16-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-17-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-16-FeedForward-Norm[0
                                                                 Transformer-16-FeedForward-Norm[0
                                                                 Transformer-16-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-17-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-17-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-17-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-16-FeedForward-Norm[0
                                                                 Transformer-17-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-17-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-17-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-17-FeedForward (Fee (None, None, 1024)   8393728     Transformer-17-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-17-FeedForward-Drop (None, None, 1024)   0           Transformer-17-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-17-FeedForward-Add  (None, None, 1024)   0           Transformer-17-MultiHeadSelfAtten
                                                                 Transformer-17-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-17-FeedForward-Norm (None, None, 1024)   2048        Transformer-17-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-18-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-17-FeedForward-Norm[0
                                                                 Transformer-17-FeedForward-Norm[0
                                                                 Transformer-17-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-18-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-18-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-18-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-17-FeedForward-Norm[0
                                                                 Transformer-18-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-18-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-18-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-18-FeedForward (Fee (None, None, 1024)   8393728     Transformer-18-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-18-FeedForward-Drop (None, None, 1024)   0           Transformer-18-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-18-FeedForward-Add  (None, None, 1024)   0           Transformer-18-MultiHeadSelfAtten
                                                                 Transformer-18-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-18-FeedForward-Norm (None, None, 1024)   2048        Transformer-18-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-19-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-18-FeedForward-Norm[0
                                                                 Transformer-18-FeedForward-Norm[0
                                                                 Transformer-18-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-19-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-19-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-19-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-18-FeedForward-Norm[0
                                                                 Transformer-19-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-19-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-19-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-19-FeedForward (Fee (None, None, 1024)   8393728     Transformer-19-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-19-FeedForward-Drop (None, None, 1024)   0           Transformer-19-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-19-FeedForward-Add  (None, None, 1024)   0           Transformer-19-MultiHeadSelfAtten
                                                                 Transformer-19-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-19-FeedForward-Norm (None, None, 1024)   2048        Transformer-19-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-20-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-19-FeedForward-Norm[0
                                                                 Transformer-19-FeedForward-Norm[0
                                                                 Transformer-19-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-20-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-20-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-20-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-19-FeedForward-Norm[0
                                                                 Transformer-20-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-20-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-20-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-20-FeedForward (Fee (None, None, 1024)   8393728     Transformer-20-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-20-FeedForward-Drop (None, None, 1024)   0           Transformer-20-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-20-FeedForward-Add  (None, None, 1024)   0           Transformer-20-MultiHeadSelfAtten
                                                                 Transformer-20-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-20-FeedForward-Norm (None, None, 1024)   2048        Transformer-20-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-21-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-20-FeedForward-Norm[0
                                                                 Transformer-20-FeedForward-Norm[0
                                                                 Transformer-20-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-21-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-21-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-21-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-20-FeedForward-Norm[0
                                                                 Transformer-21-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-21-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-21-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-21-FeedForward (Fee (None, None, 1024)   8393728     Transformer-21-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-21-FeedForward-Drop (None, None, 1024)   0           Transformer-21-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-21-FeedForward-Add  (None, None, 1024)   0           Transformer-21-MultiHeadSelfAtten
                                                                 Transformer-21-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-21-FeedForward-Norm (None, None, 1024)   2048        Transformer-21-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-22-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-21-FeedForward-Norm[0
                                                                 Transformer-21-FeedForward-Norm[0
                                                                 Transformer-21-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-22-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-22-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-22-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-21-FeedForward-Norm[0
                                                                 Transformer-22-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-22-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-22-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-22-FeedForward (Fee (None, None, 1024)   8393728     Transformer-22-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-22-FeedForward-Drop (None, None, 1024)   0           Transformer-22-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-22-FeedForward-Add  (None, None, 1024)   0           Transformer-22-MultiHeadSelfAtten
                                                                 Transformer-22-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-22-FeedForward-Norm (None, None, 1024)   2048        Transformer-22-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-23-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-22-FeedForward-Norm[0
                                                                 Transformer-22-FeedForward-Norm[0
                                                                 Transformer-22-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-23-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-23-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-23-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-22-FeedForward-Norm[0
                                                                 Transformer-23-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-23-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-23-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-23-FeedForward (Fee (None, None, 1024)   8393728     Transformer-23-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-23-FeedForward-Drop (None, None, 1024)   0           Transformer-23-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-23-FeedForward-Add  (None, None, 1024)   0           Transformer-23-MultiHeadSelfAtten
                                                                 Transformer-23-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-23-FeedForward-Norm (None, None, 1024)   2048        Transformer-23-FeedForward-Add[0]
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 1024)         0           Transformer-23-FeedForward-Norm[0
__________________________________________________________________________________________________
dropout (Dropout)               (None, 1024)         0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           15375       dropout[0][0]                    
==================================================================================================
Total params: 324,488,207
Trainable params: 15,375
Non-trainable params: 324,472,832
__________________________________________________________________________________________________
2022-05-11 22:17:15.188840: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 314s - loss: 1.6407 - accuracy: 0.4950
dev acc=55.90%
test acc=55.88%
Epoch 2/10000
188/188 - 294s - loss: 1.3536 - accuracy: 0.5493
dev acc=56.48%
test acc=56.27%
Epoch 3/10000
188/188 - 294s - loss: 1.3298 - accuracy: 0.5552
dev acc=56.52%
test acc=56.17%
Epoch 4/10000
188/188 - 294s - loss: 1.3173 - accuracy: 0.5550
dev acc=56.67%
test acc=56.09%
Epoch 5/10000
188/188 - 294s - loss: 1.3103 - accuracy: 0.5554
dev acc=56.54%
Epoch 6/10000
188/188 - 294s - loss: 1.3090 - accuracy: 0.5554
dev acc=56.65%
Epoch 7/10000
188/188 - 294s - loss: 1.3084 - accuracy: 0.5574
dev acc=56.80%
test acc=56.29%
Epoch 8/10000
188/188 - 294s - loss: 1.3089 - accuracy: 0.5578
dev acc=56.71%
Epoch 9/10000
188/188 - 294s - loss: 1.3010 - accuracy: 0.5600
dev acc=56.60%
Epoch 10/10000
188/188 - 294s - loss: 1.3002 - accuracy: 0.5567
dev acc=56.90%
test acc=56.60%
Epoch 11/10000
188/188 - 294s - loss: 1.3025 - accuracy: 0.5582
dev acc=56.82%
Epoch 12/10000
188/188 - 294s - loss: 1.2975 - accuracy: 0.5600
dev acc=56.67%
Epoch 13/10000
188/188 - 294s - loss: 1.2988 - accuracy: 0.5603
dev acc=56.77%
Epoch 14/10000
188/188 - 294s - loss: 1.2978 - accuracy: 0.5592
dev acc=56.88%
Epoch 15/10000
188/188 - 294s - loss: 1.2986 - accuracy: 0.5591
dev acc=56.62%
time used=5503.6s
2022-05-11 23:48:45.340866: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-BERT-wwm/chinese_wwm_ext_L-12_H-768_A-12/bert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-BERT-wwm/chinese_wwm_ext_L-12_H-768_A-12/bert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='bert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-BERT-wwm/chinese_wwm_ext_L-12_H-768_A-12/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_finance': 0, 'news_entertainment': 1, 'news_house': 2, 'news_agriculture': 3, 'news_edu': 4, 'news_game': 5, 'news_military': 6, 'news_story': 7, 'news_car': 8, 'news_sports': 9, 'news_culture': 10, 'news_stock': 11, 'news_travel': 12, 'news_world': 13, 'news_tech': 14}
index_labels_dict={0: 'news_finance', 1: 'news_entertainment', 2: 'news_house', 3: 'news_agriculture', 4: 'news_edu', 5: 'news_game', 6: 'news_military', 7: 'news_story', 8: 'news_car', 9: 'news_sports', 10: 'news_culture', 11: 'news_stock', 12: 'news_travel', 13: 'news_world', 14: 'news_tech'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-11 23:48:53.529069: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-11 23:48:54.845249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 768)    16226304    Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 768)          0           Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
dropout (Dropout)               (None, 768)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           11535       dropout[0][0]                    
==================================================================================================
Total params: 101,688,591
Trainable params: 11,535
Non-trainable params: 101,677,056
__________________________________________________________________________________________________
2022-05-11 23:48:58.512261: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 110s - loss: 1.7819 - accuracy: 0.4682
dev acc=53.99%
test acc=54.40%
Epoch 2/10000
188/188 - 98s - loss: 1.4159 - accuracy: 0.5347
dev acc=55.08%
test acc=55.38%
Epoch 3/10000
188/188 - 98s - loss: 1.3767 - accuracy: 0.5427
dev acc=55.51%
test acc=55.70%
Epoch 4/10000
188/188 - 98s - loss: 1.3642 - accuracy: 0.5432
dev acc=55.51%
Epoch 5/10000
188/188 - 98s - loss: 1.3538 - accuracy: 0.5443
dev acc=55.51%
Epoch 6/10000
188/188 - 98s - loss: 1.3474 - accuracy: 0.5459
dev acc=55.81%
test acc=55.90%
Epoch 7/10000
188/188 - 98s - loss: 1.3403 - accuracy: 0.5487
dev acc=55.90%
test acc=55.92%
Epoch 8/10000
188/188 - 98s - loss: 1.3421 - accuracy: 0.5470
dev acc=55.66%
Epoch 9/10000
188/188 - 98s - loss: 1.3396 - accuracy: 0.5477
dev acc=55.75%
Epoch 10/10000
188/188 - 98s - loss: 1.3375 - accuracy: 0.5477
dev acc=55.62%
Epoch 11/10000
188/188 - 98s - loss: 1.3347 - accuracy: 0.5500
dev acc=56.02%
test acc=55.96%
Epoch 12/10000
188/188 - 99s - loss: 1.3375 - accuracy: 0.5485
dev acc=56.11%
test acc=55.87%
Epoch 13/10000
188/188 - 98s - loss: 1.3344 - accuracy: 0.5491
dev acc=56.05%
Epoch 14/10000
188/188 - 99s - loss: 1.3379 - accuracy: 0.5466
dev acc=55.94%
Epoch 15/10000
188/188 - 99s - loss: 1.3384 - accuracy: 0.5484
dev acc=56.15%
test acc=55.96%
Epoch 16/10000
188/188 - 99s - loss: 1.3348 - accuracy: 0.5497
dev acc=56.05%
Epoch 17/10000
188/188 - 98s - loss: 1.3344 - accuracy: 0.5479
dev acc=55.94%
Epoch 18/10000
188/188 - 99s - loss: 1.3365 - accuracy: 0.5465
dev acc=55.92%
Epoch 19/10000
188/188 - 99s - loss: 1.3321 - accuracy: 0.5500
dev acc=55.98%
Epoch 20/10000
188/188 - 99s - loss: 1.3344 - accuracy: 0.5494
dev acc=55.90%
time used=2569.2s
2022-05-12 00:31:38.139905: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-ELECTRA/chinese_electra_base_L-12_H-768_A-12/electra_base', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-ELECTRA/chinese_electra_base_L-12_H-768_A-12/config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='electra', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-ELECTRA/chinese_electra_base_L-12_H-768_A-12/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_tech': 0, 'news_story': 1, 'news_house': 2, 'news_entertainment': 3, 'news_travel': 4, 'news_game': 5, 'news_sports': 6, 'news_edu': 7, 'news_culture': 8, 'news_military': 9, 'news_world': 10, 'news_agriculture': 11, 'news_stock': 12, 'news_car': 13, 'news_finance': 14}
index_labels_dict={0: 'news_tech', 1: 'news_story', 2: 'news_house', 3: 'news_entertainment', 4: 'news_travel', 5: 'news_game', 6: 'news_sports', 7: 'news_edu', 8: 'news_culture', 9: 'news_military', 10: 'news_world', 11: 'news_agriculture', 12: 'news_stock', 13: 'news_car', 14: 'news_finance'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-12 00:31:46.191234: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-12 00:31:47.663673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 768)    16226304    Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 768)          0           Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
dropout (Dropout)               (None, 768)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           11535       dropout[0][0]                    
==================================================================================================
Total params: 101,688,591
Trainable params: 11,535
Non-trainable params: 101,677,056
__________________________________________________________________________________________________
2022-05-12 00:31:51.428932: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 110s - loss: 2.5272 - accuracy: 0.1644
dev acc=19.45%
test acc=18.87%
Epoch 2/10000
188/188 - 98s - loss: 2.4318 - accuracy: 0.2078
dev acc=21.25%
test acc=21.01%
Epoch 3/10000
188/188 - 98s - loss: 2.3813 - accuracy: 0.2237
dev acc=22.71%
test acc=22.39%
Epoch 4/10000
188/188 - 99s - loss: 2.3511 - accuracy: 0.2326
dev acc=24.27%
test acc=24.14%
Epoch 5/10000
188/188 - 98s - loss: 2.3297 - accuracy: 0.2417
dev acc=25.58%
test acc=25.07%
Epoch 6/10000
188/188 - 99s - loss: 2.3153 - accuracy: 0.2462
dev acc=26.16%
test acc=25.71%
Epoch 7/10000
188/188 - 98s - loss: 2.3016 - accuracy: 0.2523
dev acc=26.61%
test acc=26.16%
Epoch 8/10000
188/188 - 99s - loss: 2.2899 - accuracy: 0.2533
dev acc=27.10%
test acc=26.76%
Epoch 9/10000
188/188 - 99s - loss: 2.2823 - accuracy: 0.2547
dev acc=27.60%
test acc=27.11%
Epoch 10/10000
188/188 - 99s - loss: 2.2791 - accuracy: 0.2562
dev acc=28.05%
test acc=27.67%
Epoch 11/10000
188/188 - 98s - loss: 2.2762 - accuracy: 0.2576
dev acc=28.17%
test acc=28.11%
Epoch 12/10000
188/188 - 98s - loss: 2.2694 - accuracy: 0.2630
dev acc=28.67%
test acc=28.52%
Epoch 13/10000
188/188 - 98s - loss: 2.2632 - accuracy: 0.2629
dev acc=28.65%
Epoch 14/10000
188/188 - 98s - loss: 2.2618 - accuracy: 0.2631
dev acc=29.07%
test acc=28.97%
Epoch 15/10000
188/188 - 99s - loss: 2.2598 - accuracy: 0.2640
dev acc=29.18%
test acc=29.02%
Epoch 16/10000
188/188 - 99s - loss: 2.2566 - accuracy: 0.2636
dev acc=29.22%
test acc=29.14%
Epoch 17/10000
188/188 - 99s - loss: 2.2568 - accuracy: 0.2642
dev acc=29.46%
test acc=29.44%
Epoch 18/10000
188/188 - 98s - loss: 2.2548 - accuracy: 0.2680
dev acc=29.84%
test acc=29.54%
Epoch 19/10000
188/188 - 99s - loss: 2.2518 - accuracy: 0.2638
dev acc=29.76%
Epoch 20/10000
188/188 - 99s - loss: 2.2507 - accuracy: 0.2662
dev acc=29.69%
Epoch 21/10000
188/188 - 99s - loss: 2.2503 - accuracy: 0.2673
dev acc=29.74%
Epoch 22/10000
188/188 - 99s - loss: 2.2494 - accuracy: 0.2670
dev acc=29.42%
Epoch 23/10000
188/188 - 99s - loss: 2.2489 - accuracy: 0.2673
dev acc=30.04%
test acc=29.93%
Epoch 24/10000
188/188 - 99s - loss: 2.2457 - accuracy: 0.2679
dev acc=30.17%
test acc=29.85%
Epoch 25/10000
188/188 - 99s - loss: 2.2448 - accuracy: 0.2681
dev acc=30.15%
Epoch 26/10000
188/188 - 99s - loss: 2.2444 - accuracy: 0.2705
dev acc=30.17%
Epoch 27/10000
188/188 - 99s - loss: 2.2426 - accuracy: 0.2684
dev acc=30.43%
test acc=30.56%
Epoch 28/10000
188/188 - 99s - loss: 2.2427 - accuracy: 0.2685
dev acc=30.51%
test acc=30.35%
Epoch 29/10000
188/188 - 99s - loss: 2.2429 - accuracy: 0.2665
dev acc=30.27%
Epoch 30/10000
188/188 - 99s - loss: 2.2445 - accuracy: 0.2679
dev acc=30.28%
Epoch 31/10000
188/188 - 99s - loss: 2.2420 - accuracy: 0.2710
dev acc=30.28%
Epoch 32/10000
188/188 - 99s - loss: 2.2413 - accuracy: 0.2700
dev acc=30.42%
Epoch 33/10000
188/188 - 99s - loss: 2.2408 - accuracy: 0.2689
dev acc=30.38%
time used=4476.1s
2022-05-12 01:46:17.762654: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-ELECTRA/chinese_electra_small_L-12_H-256_A-4/electra_small', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-ELECTRA/chinese_electra_small_L-12_H-256_A-4/config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='electra', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/Chinese-ELECTRA/chinese_electra_small_L-12_H-256_A-4/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_story': 0, 'news_world': 1, 'news_entertainment': 2, 'news_car': 3, 'news_agriculture': 4, 'news_sports': 5, 'news_stock': 6, 'news_finance': 7, 'news_travel': 8, 'news_military': 9, 'news_tech': 10, 'news_edu': 11, 'news_house': 12, 'news_game': 13, 'news_culture': 14}
index_labels_dict={0: 'news_story', 1: 'news_world', 2: 'news_entertainment', 3: 'news_car', 4: 'news_agriculture', 5: 'news_sports', 6: 'news_stock', 7: 'news_finance', 8: 'news_travel', 9: 'news_military', 10: 'news_tech', 11: 'news_edu', 12: 'news_house', 13: 'news_game', 14: 'news_culture'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-12 01:46:26.026557: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-12 01:46:27.371253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 128)    2704384     Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 128)    0           Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Embedding-Mapping (Dense)       (None, None, 256)    33024       Embedding-Dropout[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 256)    263168      Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 256)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 256)    0           Embedding-Mapping[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 256)    512         Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 256)    525568      Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 256)    0           Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 256)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 256)    512         Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 256)    263168      Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 256)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 256)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 256)    512         Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 256)    525568      Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 256)    0           Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 256)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 256)    512         Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 256)    263168      Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 256)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 256)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 256)    512         Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 256)    525568      Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 256)    0           Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 256)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 256)    512         Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 256)    263168      Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 256)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 256)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 256)    512         Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 256)    525568      Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 256)    0           Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 256)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 256)    512         Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 256)    263168      Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 256)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 256)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 256)    512         Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 256)    525568      Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 256)    0           Transformer-4-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 256)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 256)    512         Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 256)    263168      Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 256)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 256)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 256)    512         Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 256)    525568      Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 256)    0           Transformer-5-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 256)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 256)    512         Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 256)    263168      Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 256)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 256)    0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 256)    512         Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 256)    525568      Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 256)    0           Transformer-6-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 256)    0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 256)    512         Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 256)    263168      Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 256)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 256)    0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 256)    512         Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 256)    525568      Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 256)    0           Transformer-7-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 256)    0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 256)    512         Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 256)    263168      Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 256)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 256)    0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 256)    512         Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 256)    525568      Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 256)    0           Transformer-8-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 256)    0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 256)    512         Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 256)    263168      Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 256)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 256)    0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 256)    512         Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 256)    525568      Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 256)    0           Transformer-9-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 256)    0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 256)    512         Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 256)    263168      Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 256)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 256)    0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 256)    512         Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 256)    525568      Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 256)    0           Transformer-10-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 256)    0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 256)    512         Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 256)    263168      Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 256)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 256)    0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 256)    512         Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 256)    525568      Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 256)    0           Transformer-11-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 256)    0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 256)    512         Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 256)          0           Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
dropout (Dropout)               (None, 256)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           3855        dropout[0][0]                    
==================================================================================================
Total params: 12,284,431
Trainable params: 3,855
Non-trainable params: 12,280,576
__________________________________________________________________________________________________
2022-05-12 01:46:30.723494: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 40s - loss: 2.5510 - accuracy: 0.1603
dev acc=20.99%
test acc=20.80%
Epoch 2/10000
188/188 - 27s - loss: 2.4630 - accuracy: 0.2060
dev acc=23.26%
test acc=22.57%
Epoch 3/10000
188/188 - 28s - loss: 2.4188 - accuracy: 0.2174
dev acc=24.72%
test acc=24.14%
Epoch 4/10000
188/188 - 27s - loss: 2.3885 - accuracy: 0.2251
dev acc=25.79%
test acc=25.46%
Epoch 5/10000
188/188 - 27s - loss: 2.3681 - accuracy: 0.2292
dev acc=26.44%
test acc=26.12%
Epoch 6/10000
188/188 - 27s - loss: 2.3519 - accuracy: 0.2360
dev acc=27.04%
test acc=26.74%
Epoch 7/10000
188/188 - 27s - loss: 2.3444 - accuracy: 0.2360
dev acc=27.27%
test acc=27.29%
Epoch 8/10000
188/188 - 28s - loss: 2.3355 - accuracy: 0.2407
dev acc=28.09%
test acc=27.73%
Epoch 9/10000
188/188 - 28s - loss: 2.3285 - accuracy: 0.2416
dev acc=28.28%
test acc=28.05%
Epoch 10/10000
188/188 - 28s - loss: 2.3254 - accuracy: 0.2423
dev acc=28.52%
test acc=28.35%
Epoch 11/10000
188/188 - 28s - loss: 2.3222 - accuracy: 0.2427
dev acc=28.71%
test acc=28.72%
Epoch 12/10000
188/188 - 28s - loss: 2.3166 - accuracy: 0.2448
dev acc=28.94%
test acc=28.76%
Epoch 13/10000
188/188 - 27s - loss: 2.3165 - accuracy: 0.2444
dev acc=29.01%
test acc=29.02%
Epoch 14/10000
188/188 - 27s - loss: 2.3164 - accuracy: 0.2442
dev acc=29.16%
test acc=29.18%
Epoch 15/10000
188/188 - 28s - loss: 2.3131 - accuracy: 0.2457
dev acc=29.33%
test acc=29.37%
Epoch 16/10000
188/188 - 27s - loss: 2.3106 - accuracy: 0.2489
dev acc=29.54%
test acc=29.70%
Epoch 17/10000
188/188 - 28s - loss: 2.3080 - accuracy: 0.2499
dev acc=29.55%
test acc=29.59%
Epoch 18/10000
188/188 - 28s - loss: 2.3100 - accuracy: 0.2468
dev acc=29.42%
Epoch 19/10000
188/188 - 28s - loss: 2.3069 - accuracy: 0.2482
dev acc=29.74%
test acc=29.68%
Epoch 20/10000
188/188 - 28s - loss: 2.3080 - accuracy: 0.2475
dev acc=29.61%
Epoch 21/10000
188/188 - 28s - loss: 2.3053 - accuracy: 0.2469
dev acc=29.61%
Epoch 22/10000
188/188 - 28s - loss: 2.3069 - accuracy: 0.2464
dev acc=29.63%
Epoch 23/10000
188/188 - 28s - loss: 2.3018 - accuracy: 0.2517
dev acc=29.87%
test acc=29.75%
Epoch 24/10000
188/188 - 27s - loss: 2.3067 - accuracy: 0.2452
dev acc=29.72%
Epoch 25/10000
188/188 - 27s - loss: 2.3022 - accuracy: 0.2504
dev acc=29.80%
Epoch 26/10000
188/188 - 28s - loss: 2.3049 - accuracy: 0.2466
dev acc=30.10%
test acc=29.96%
Epoch 27/10000
188/188 - 27s - loss: 2.3053 - accuracy: 0.2467
dev acc=29.82%
Epoch 28/10000
188/188 - 27s - loss: 2.3093 - accuracy: 0.2456
dev acc=29.84%
Epoch 29/10000
188/188 - 27s - loss: 2.3061 - accuracy: 0.2491
dev acc=30.02%
Epoch 30/10000
188/188 - 27s - loss: 2.3054 - accuracy: 0.2476
dev acc=30.08%
Epoch 31/10000
188/188 - 27s - loss: 2.3073 - accuracy: 0.2454
dev acc=30.13%
test acc=30.40%
Epoch 32/10000
188/188 - 27s - loss: 2.3031 - accuracy: 0.2470
dev acc=30.25%
test acc=30.48%
Epoch 33/10000
188/188 - 27s - loss: 2.3003 - accuracy: 0.2495
dev acc=30.23%
Epoch 34/10000
188/188 - 27s - loss: 2.3061 - accuracy: 0.2483
dev acc=30.27%
test acc=30.30%
Epoch 35/10000
188/188 - 28s - loss: 2.3033 - accuracy: 0.2480
dev acc=29.82%
Epoch 36/10000
188/188 - 27s - loss: 2.3042 - accuracy: 0.2489
dev acc=30.17%
Epoch 37/10000
188/188 - 28s - loss: 2.3046 - accuracy: 0.2467
dev acc=30.15%
Epoch 38/10000
188/188 - 27s - loss: 2.3067 - accuracy: 0.2471
dev acc=30.34%
test acc=30.33%
Epoch 39/10000
188/188 - 27s - loss: 2.3032 - accuracy: 0.2463
dev acc=30.28%
Epoch 40/10000
188/188 - 27s - loss: 2.3043 - accuracy: 0.2494
dev acc=30.42%
test acc=30.32%
Epoch 41/10000
188/188 - 28s - loss: 2.3030 - accuracy: 0.2499
dev acc=30.32%
Epoch 42/10000
188/188 - 28s - loss: 2.2990 - accuracy: 0.2523
dev acc=30.13%
Epoch 43/10000
188/188 - 27s - loss: 2.3042 - accuracy: 0.2491
dev acc=30.36%
Epoch 44/10000
188/188 - 28s - loss: 2.2999 - accuracy: 0.2530
dev acc=30.19%
Epoch 45/10000
188/188 - 27s - loss: 2.3038 - accuracy: 0.2463
dev acc=30.17%
time used=1968.9s
2022-05-12 02:19:10.412432: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/ELECTRA/electra_tiny/model.ckpt-1000000', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/ELECTRA/electra_tiny/bert_config_tiny.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='electra', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/ELECTRA/electra_tiny/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_edu': 0, 'news_travel': 1, 'news_agriculture': 2, 'news_house': 3, 'news_tech': 4, 'news_finance': 5, 'news_military': 6, 'news_stock': 7, 'news_car': 8, 'news_culture': 9, 'news_world': 10, 'news_story': 11, 'news_sports': 12, 'news_entertainment': 13, 'news_game': 14}
index_labels_dict={0: 'news_edu', 1: 'news_travel', 2: 'news_agriculture', 3: 'news_house', 4: 'news_tech', 5: 'news_finance', 6: 'news_military', 7: 'news_stock', 8: 'news_car', 9: 'news_culture', 10: 'news_world', 11: 'news_story', 12: 'news_sports', 13: 'news_entertainment', 14: 'news_game'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-12 02:19:18.655984: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-12 02:19:19.761297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 312)    6591936     Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 312)    624         Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 312)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 312)    159744      Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 312)    624         Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 312)    0           Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 312)    390624      Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 312)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 312)    0           Embedding-Dropout[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 312)    624         Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 312)    750312      Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 312)    0           Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 312)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 312)    624         Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 312)    390624      Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 312)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 312)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 312)    624         Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 312)    750312      Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 312)    0           Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 312)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 312)    624         Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 312)    390624      Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 312)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 312)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 312)    624         Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 312)    750312      Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 312)    0           Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 312)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 312)    624         Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 312)    390624      Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 312)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 312)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 312)    624         Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 312)    750312      Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 312)    0           Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 312)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 312)    624         Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 312)          0           Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 312)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           4695        dropout[0][0]                    
==================================================================================================
Total params: 11,326,359
Trainable params: 4,695
Non-trainable params: 11,321,664
__________________________________________________________________________________________________
2022-05-12 02:19:21.274261: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 21s - loss: 2.5259 - accuracy: 0.1781
dev acc=23.05%
test acc=22.02%
Epoch 2/10000
188/188 - 16s - loss: 2.4116 - accuracy: 0.2262
dev acc=25.47%
test acc=24.87%
Epoch 3/10000
188/188 - 16s - loss: 2.3532 - accuracy: 0.2433
dev acc=27.01%
test acc=26.51%
Epoch 4/10000
188/188 - 16s - loss: 2.3159 - accuracy: 0.2539
dev acc=28.05%
test acc=27.72%
Epoch 5/10000
188/188 - 16s - loss: 2.2942 - accuracy: 0.2589
dev acc=28.95%
test acc=28.68%
Epoch 6/10000
188/188 - 16s - loss: 2.2802 - accuracy: 0.2624
dev acc=29.35%
test acc=29.37%
Epoch 7/10000
188/188 - 14s - loss: 2.2648 - accuracy: 0.2683
dev acc=29.87%
test acc=29.86%
Epoch 8/10000
188/188 - 16s - loss: 2.2524 - accuracy: 0.2686
dev acc=30.34%
test acc=30.48%
Epoch 9/10000
188/188 - 16s - loss: 2.2522 - accuracy: 0.2711
dev acc=30.55%
test acc=30.81%
Epoch 10/10000
188/188 - 16s - loss: 2.2423 - accuracy: 0.2744
dev acc=30.79%
test acc=31.30%
Epoch 11/10000
188/188 - 16s - loss: 2.2387 - accuracy: 0.2744
dev acc=31.17%
test acc=31.44%
Epoch 12/10000
188/188 - 15s - loss: 2.2377 - accuracy: 0.2766
dev acc=31.39%
test acc=31.68%
Epoch 13/10000
188/188 - 15s - loss: 2.2373 - accuracy: 0.2770
dev acc=31.58%
test acc=31.77%
Epoch 14/10000
188/188 - 15s - loss: 2.2341 - accuracy: 0.2743
dev acc=31.77%
test acc=32.14%
Epoch 15/10000
188/188 - 16s - loss: 2.2297 - accuracy: 0.2780
dev acc=32.12%
test acc=32.05%
Epoch 16/10000
188/188 - 17s - loss: 2.2261 - accuracy: 0.2779
dev acc=32.10%
Epoch 17/10000
188/188 - 15s - loss: 2.2231 - accuracy: 0.2785
dev acc=32.37%
test acc=32.32%
Epoch 18/10000
188/188 - 13s - loss: 2.2266 - accuracy: 0.2796
dev acc=32.42%
test acc=32.28%
Epoch 19/10000
188/188 - 15s - loss: 2.2229 - accuracy: 0.2814
dev acc=32.63%
test acc=32.60%
Epoch 20/10000
188/188 - 13s - loss: 2.2255 - accuracy: 0.2759
dev acc=32.53%
Epoch 21/10000
188/188 - 13s - loss: 2.2243 - accuracy: 0.2791
dev acc=32.81%
test acc=32.52%
Epoch 22/10000
188/188 - 14s - loss: 2.2223 - accuracy: 0.2788
dev acc=32.40%
Epoch 23/10000
188/188 - 14s - loss: 2.2236 - accuracy: 0.2803
dev acc=32.40%
Epoch 24/10000
188/188 - 14s - loss: 2.2224 - accuracy: 0.2805
dev acc=32.46%
Epoch 25/10000
188/188 - 16s - loss: 2.2198 - accuracy: 0.2808
dev acc=32.70%
Epoch 26/10000
188/188 - 15s - loss: 2.2196 - accuracy: 0.2783
dev acc=32.81%
time used=708.1s
2022-05-12 02:31:02.053421: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/roberta_zh/roberta_zh_l12/bert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/roberta_zh/roberta_zh_l12/bert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='bert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/roberta_zh/roberta_zh_l12/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_military': 0, 'news_world': 1, 'news_entertainment': 2, 'news_game': 3, 'news_culture': 4, 'news_house': 5, 'news_story': 6, 'news_finance': 7, 'news_sports': 8, 'news_edu': 9, 'news_tech': 10, 'news_agriculture': 11, 'news_travel': 12, 'news_stock': 13, 'news_car': 14}
index_labels_dict={0: 'news_military', 1: 'news_world', 2: 'news_entertainment', 3: 'news_game', 4: 'news_culture', 5: 'news_house', 6: 'news_story', 7: 'news_finance', 8: 'news_sports', 9: 'news_edu', 10: 'news_tech', 11: 'news_agriculture', 12: 'news_travel', 13: 'news_stock', 14: 'news_car'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-12 02:31:09.928966: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-12 02:31:10.735662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 768)    16226304    Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 768)          0           Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
dropout (Dropout)               (None, 768)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           11535       dropout[0][0]                    
==================================================================================================
Total params: 101,688,591
Trainable params: 11,535
Non-trainable params: 101,677,056
__________________________________________________________________________________________________
2022-05-12 02:31:14.301489: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 109s - loss: 1.7999 - accuracy: 0.4682
dev acc=54.20%
test acc=54.94%
Epoch 2/10000
188/188 - 98s - loss: 1.4249 - accuracy: 0.5345
dev acc=55.34%
test acc=55.49%
Epoch 3/10000
188/188 - 98s - loss: 1.3824 - accuracy: 0.5407
dev acc=55.45%
test acc=55.70%
Epoch 4/10000
188/188 - 98s - loss: 1.3683 - accuracy: 0.5416
dev acc=55.55%
test acc=55.83%
Epoch 5/10000
188/188 - 98s - loss: 1.3600 - accuracy: 0.5412
dev acc=55.23%
Epoch 6/10000
188/188 - 98s - loss: 1.3519 - accuracy: 0.5440
dev acc=55.45%
Epoch 7/10000
188/188 - 98s - loss: 1.3446 - accuracy: 0.5464
dev acc=55.68%
test acc=56.03%
Epoch 8/10000
188/188 - 98s - loss: 1.3508 - accuracy: 0.5437
dev acc=55.87%
test acc=56.21%
Epoch 9/10000
188/188 - 98s - loss: 1.3450 - accuracy: 0.5447
dev acc=55.66%
Epoch 10/10000
188/188 - 98s - loss: 1.3410 - accuracy: 0.5461
dev acc=55.75%
Epoch 11/10000
188/188 - 98s - loss: 1.3391 - accuracy: 0.5490
dev acc=55.94%
test acc=55.89%
Epoch 12/10000
188/188 - 98s - loss: 1.3404 - accuracy: 0.5453
dev acc=55.68%
Epoch 13/10000
188/188 - 98s - loss: 1.3395 - accuracy: 0.5457
dev acc=55.72%
Epoch 14/10000
188/188 - 98s - loss: 1.3423 - accuracy: 0.5459
dev acc=55.90%
Epoch 15/10000
188/188 - 98s - loss: 1.3390 - accuracy: 0.5475
dev acc=55.75%
Epoch 16/10000
188/188 - 98s - loss: 1.3386 - accuracy: 0.5477
dev acc=55.70%
time used=2062.1s
2022-05-12 03:05:27.486301: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/roberta_zh/roberta_zh_L-6-H-768_A-12/bert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/roberta_zh/roberta_zh_L-6-H-768_A-12/bert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='bert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/roberta_zh/roberta_zh_L-6-H-768_A-12/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_sports': 0, 'news_story': 1, 'news_world': 2, 'news_travel': 3, 'news_house': 4, 'news_edu': 5, 'news_military': 6, 'news_game': 7, 'news_culture': 8, 'news_finance': 9, 'news_stock': 10, 'news_agriculture': 11, 'news_tech': 12, 'news_entertainment': 13, 'news_car': 14}
index_labels_dict={0: 'news_sports', 1: 'news_story', 2: 'news_world', 3: 'news_travel', 4: 'news_house', 5: 'news_edu', 6: 'news_military', 7: 'news_game', 8: 'news_culture', 9: 'news_finance', 10: 'news_stock', 11: 'news_agriculture', 12: 'news_tech', 13: 'news_entertainment', 14: 'news_car'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-12 03:05:35.476461: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-12 03:05:36.274501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 768)    16226304    Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 768)          0           Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 768)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           11535       dropout[0][0]                    
==================================================================================================
Total params: 59,161,359
Trainable params: 11,535
Non-trainable params: 59,149,824
__________________________________________________________________________________________________
2022-05-12 03:05:38.527238: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 56s - loss: 1.8702 - accuracy: 0.4321
dev acc=51.61%
test acc=52.28%
Epoch 2/10000
188/188 - 49s - loss: 1.5275 - accuracy: 0.5093
dev acc=53.41%
test acc=53.28%
Epoch 3/10000
188/188 - 49s - loss: 1.4842 - accuracy: 0.5134
dev acc=53.73%
test acc=53.85%
Epoch 4/10000
188/188 - 49s - loss: 1.4665 - accuracy: 0.5157
dev acc=54.01%
test acc=54.01%
Epoch 5/10000
188/188 - 49s - loss: 1.4629 - accuracy: 0.5168
dev acc=53.71%
Epoch 6/10000
188/188 - 49s - loss: 1.4530 - accuracy: 0.5204
dev acc=53.79%
Epoch 7/10000
188/188 - 49s - loss: 1.4507 - accuracy: 0.5199
dev acc=53.71%
Epoch 8/10000
188/188 - 49s - loss: 1.4484 - accuracy: 0.5224
dev acc=53.90%
Epoch 9/10000
188/188 - 49s - loss: 1.4437 - accuracy: 0.5219
dev acc=54.22%
test acc=54.35%
Epoch 10/10000
188/188 - 49s - loss: 1.4456 - accuracy: 0.5204
dev acc=54.18%
Epoch 11/10000
188/188 - 49s - loss: 1.4402 - accuracy: 0.5224
dev acc=54.39%
test acc=54.28%
Epoch 12/10000
188/188 - 49s - loss: 1.4426 - accuracy: 0.5208
dev acc=54.46%
test acc=54.27%
Epoch 13/10000
188/188 - 49s - loss: 1.4414 - accuracy: 0.5206
dev acc=54.40%
Epoch 14/10000
188/188 - 49s - loss: 1.4443 - accuracy: 0.5216
dev acc=54.33%
Epoch 15/10000
188/188 - 50s - loss: 1.4454 - accuracy: 0.5187
dev acc=54.63%
test acc=54.19%
Epoch 16/10000
188/188 - 50s - loss: 1.4420 - accuracy: 0.5220
dev acc=54.46%
Epoch 17/10000
188/188 - 50s - loss: 1.4437 - accuracy: 0.5211
dev acc=54.48%
Epoch 18/10000
188/188 - 49s - loss: 1.4394 - accuracy: 0.5198
dev acc=54.37%
Epoch 19/10000
188/188 - 49s - loss: 1.4419 - accuracy: 0.5236
dev acc=54.48%
Epoch 20/10000
188/188 - 49s - loss: 1.4442 - accuracy: 0.5206
dev acc=54.48%
time used=1345.0s
2022-05-12 03:27:55.891379: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/roberta_zh/roeberta_zh_L-24_H-1024_A-16/roberta_zh_large_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/roberta_zh/roeberta_zh_L-24_H-1024_A-16/bert_config_large.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='bert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/roberta_zh/roeberta_zh_L-24_H-1024_A-16/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_travel': 0, 'news_finance': 1, 'news_military': 2, 'news_edu': 3, 'news_game': 4, 'news_stock': 5, 'news_tech': 6, 'news_car': 7, 'news_house': 8, 'news_story': 9, 'news_agriculture': 10, 'news_entertainment': 11, 'news_culture': 12, 'news_world': 13, 'news_sports': 14}
index_labels_dict={0: 'news_travel', 1: 'news_finance', 2: 'news_military', 3: 'news_edu', 4: 'news_game', 5: 'news_stock', 6: 'news_tech', 7: 'news_car', 8: 'news_house', 9: 'news_story', 10: 'news_agriculture', 11: 'news_entertainment', 12: 'news_culture', 13: 'news_world', 14: 'news_sports'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-12 03:28:04.025502: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-12 03:28:04.879358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 1024)   21635072    Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 1024)   2048        Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 1024)   0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 1024)   524288      Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 1024)   2048        Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 1024)   0           Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 1024)   4198400     Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 1024)   0           Embedding-Dropout[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 1024)   8393728     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 1024)   0           Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 1024)   0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 1024)   2048        Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 1024)   4198400     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 1024)   8393728     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 1024)   0           Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 1024)   0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 1024)   2048        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 1024)   4198400     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 1024)   8393728     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 1024)   0           Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 1024)   0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 1024)   2048        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 1024)   4198400     Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 1024)   8393728     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 1024)   0           Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 1024)   0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 1024)   2048        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 1024)   4198400     Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 1024)   8393728     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 1024)   0           Transformer-4-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 1024)   0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 1024)   2048        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 1024)   4198400     Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 1024)   8393728     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 1024)   0           Transformer-5-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 1024)   0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 1024)   2048        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 1024)   4198400     Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 1024)   8393728     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 1024)   0           Transformer-6-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 1024)   0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 1024)   2048        Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 1024)   4198400     Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 1024)   8393728     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 1024)   0           Transformer-7-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 1024)   0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 1024)   2048        Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 1024)   4198400     Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 1024)   8393728     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 1024)   0           Transformer-8-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 1024)   0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 1024)   2048        Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 1024)   4198400     Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 1024)   0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 1024)   2048        Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 1024)   8393728     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 1024)   0           Transformer-9-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 1024)   0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 1024)   2048        Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 1024)   8393728     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 1024)   0           Transformer-10-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 1024)   0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 1024)   2048        Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 1024)   8393728     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 1024)   0           Transformer-11-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 1024)   0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 1024)   2048        Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-11-FeedForward-Norm[0
                                                                 Transformer-11-FeedForward-Norm[0
                                                                 Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-11-FeedForward-Norm[0
                                                                 Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-FeedForward (Fee (None, None, 1024)   8393728     Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-FeedForward-Drop (None, None, 1024)   0           Transformer-12-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-12-FeedForward-Add  (None, None, 1024)   0           Transformer-12-MultiHeadSelfAtten
                                                                 Transformer-12-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-12-FeedForward-Norm (None, None, 1024)   2048        Transformer-12-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-13-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-12-FeedForward-Norm[0
                                                                 Transformer-12-FeedForward-Norm[0
                                                                 Transformer-12-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-13-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-13-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-13-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-12-FeedForward-Norm[0
                                                                 Transformer-13-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-13-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-13-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-13-FeedForward (Fee (None, None, 1024)   8393728     Transformer-13-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-13-FeedForward-Drop (None, None, 1024)   0           Transformer-13-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-13-FeedForward-Add  (None, None, 1024)   0           Transformer-13-MultiHeadSelfAtten
                                                                 Transformer-13-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-13-FeedForward-Norm (None, None, 1024)   2048        Transformer-13-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-14-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-13-FeedForward-Norm[0
                                                                 Transformer-13-FeedForward-Norm[0
                                                                 Transformer-13-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-14-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-14-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-14-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-13-FeedForward-Norm[0
                                                                 Transformer-14-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-14-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-14-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-14-FeedForward (Fee (None, None, 1024)   8393728     Transformer-14-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-14-FeedForward-Drop (None, None, 1024)   0           Transformer-14-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-14-FeedForward-Add  (None, None, 1024)   0           Transformer-14-MultiHeadSelfAtten
                                                                 Transformer-14-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-14-FeedForward-Norm (None, None, 1024)   2048        Transformer-14-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-15-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-14-FeedForward-Norm[0
                                                                 Transformer-14-FeedForward-Norm[0
                                                                 Transformer-14-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-15-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-15-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-15-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-14-FeedForward-Norm[0
                                                                 Transformer-15-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-15-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-15-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-15-FeedForward (Fee (None, None, 1024)   8393728     Transformer-15-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-15-FeedForward-Drop (None, None, 1024)   0           Transformer-15-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-15-FeedForward-Add  (None, None, 1024)   0           Transformer-15-MultiHeadSelfAtten
                                                                 Transformer-15-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-15-FeedForward-Norm (None, None, 1024)   2048        Transformer-15-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-16-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-15-FeedForward-Norm[0
                                                                 Transformer-15-FeedForward-Norm[0
                                                                 Transformer-15-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-16-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-16-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-16-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-15-FeedForward-Norm[0
                                                                 Transformer-16-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-16-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-16-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-16-FeedForward (Fee (None, None, 1024)   8393728     Transformer-16-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-16-FeedForward-Drop (None, None, 1024)   0           Transformer-16-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-16-FeedForward-Add  (None, None, 1024)   0           Transformer-16-MultiHeadSelfAtten
                                                                 Transformer-16-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-16-FeedForward-Norm (None, None, 1024)   2048        Transformer-16-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-17-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-16-FeedForward-Norm[0
                                                                 Transformer-16-FeedForward-Norm[0
                                                                 Transformer-16-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-17-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-17-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-17-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-16-FeedForward-Norm[0
                                                                 Transformer-17-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-17-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-17-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-17-FeedForward (Fee (None, None, 1024)   8393728     Transformer-17-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-17-FeedForward-Drop (None, None, 1024)   0           Transformer-17-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-17-FeedForward-Add  (None, None, 1024)   0           Transformer-17-MultiHeadSelfAtten
                                                                 Transformer-17-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-17-FeedForward-Norm (None, None, 1024)   2048        Transformer-17-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-18-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-17-FeedForward-Norm[0
                                                                 Transformer-17-FeedForward-Norm[0
                                                                 Transformer-17-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-18-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-18-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-18-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-17-FeedForward-Norm[0
                                                                 Transformer-18-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-18-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-18-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-18-FeedForward (Fee (None, None, 1024)   8393728     Transformer-18-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-18-FeedForward-Drop (None, None, 1024)   0           Transformer-18-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-18-FeedForward-Add  (None, None, 1024)   0           Transformer-18-MultiHeadSelfAtten
                                                                 Transformer-18-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-18-FeedForward-Norm (None, None, 1024)   2048        Transformer-18-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-19-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-18-FeedForward-Norm[0
                                                                 Transformer-18-FeedForward-Norm[0
                                                                 Transformer-18-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-19-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-19-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-19-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-18-FeedForward-Norm[0
                                                                 Transformer-19-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-19-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-19-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-19-FeedForward (Fee (None, None, 1024)   8393728     Transformer-19-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-19-FeedForward-Drop (None, None, 1024)   0           Transformer-19-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-19-FeedForward-Add  (None, None, 1024)   0           Transformer-19-MultiHeadSelfAtten
                                                                 Transformer-19-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-19-FeedForward-Norm (None, None, 1024)   2048        Transformer-19-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-20-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-19-FeedForward-Norm[0
                                                                 Transformer-19-FeedForward-Norm[0
                                                                 Transformer-19-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-20-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-20-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-20-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-19-FeedForward-Norm[0
                                                                 Transformer-20-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-20-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-20-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-20-FeedForward (Fee (None, None, 1024)   8393728     Transformer-20-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-20-FeedForward-Drop (None, None, 1024)   0           Transformer-20-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-20-FeedForward-Add  (None, None, 1024)   0           Transformer-20-MultiHeadSelfAtten
                                                                 Transformer-20-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-20-FeedForward-Norm (None, None, 1024)   2048        Transformer-20-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-21-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-20-FeedForward-Norm[0
                                                                 Transformer-20-FeedForward-Norm[0
                                                                 Transformer-20-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-21-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-21-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-21-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-20-FeedForward-Norm[0
                                                                 Transformer-21-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-21-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-21-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-21-FeedForward (Fee (None, None, 1024)   8393728     Transformer-21-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-21-FeedForward-Drop (None, None, 1024)   0           Transformer-21-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-21-FeedForward-Add  (None, None, 1024)   0           Transformer-21-MultiHeadSelfAtten
                                                                 Transformer-21-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-21-FeedForward-Norm (None, None, 1024)   2048        Transformer-21-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-22-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-21-FeedForward-Norm[0
                                                                 Transformer-21-FeedForward-Norm[0
                                                                 Transformer-21-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-22-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-22-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-22-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-21-FeedForward-Norm[0
                                                                 Transformer-22-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-22-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-22-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-22-FeedForward (Fee (None, None, 1024)   8393728     Transformer-22-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-22-FeedForward-Drop (None, None, 1024)   0           Transformer-22-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-22-FeedForward-Add  (None, None, 1024)   0           Transformer-22-MultiHeadSelfAtten
                                                                 Transformer-22-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-22-FeedForward-Norm (None, None, 1024)   2048        Transformer-22-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-23-MultiHeadSelfAtt (None, None, 1024)   4198400     Transformer-22-FeedForward-Norm[0
                                                                 Transformer-22-FeedForward-Norm[0
                                                                 Transformer-22-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-23-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-23-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-23-MultiHeadSelfAtt (None, None, 1024)   0           Transformer-22-FeedForward-Norm[0
                                                                 Transformer-23-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-23-MultiHeadSelfAtt (None, None, 1024)   2048        Transformer-23-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-23-FeedForward (Fee (None, None, 1024)   8393728     Transformer-23-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-23-FeedForward-Drop (None, None, 1024)   0           Transformer-23-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-23-FeedForward-Add  (None, None, 1024)   0           Transformer-23-MultiHeadSelfAtten
                                                                 Transformer-23-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-23-FeedForward-Norm (None, None, 1024)   2048        Transformer-23-FeedForward-Add[0]
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 1024)         0           Transformer-23-FeedForward-Norm[0
__________________________________________________________________________________________________
dropout (Dropout)               (None, 1024)         0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           15375       dropout[0][0]                    
==================================================================================================
Total params: 324,488,207
Trainable params: 15,375
Non-trainable params: 324,472,832
__________________________________________________________________________________________________
2022-05-12 03:28:12.228986: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 312s - loss: 1.7326 - accuracy: 0.4724
dev acc=54.70%
test acc=54.73%
Epoch 2/10000
188/188 - 293s - loss: 1.4091 - accuracy: 0.5372
dev acc=55.45%
test acc=55.52%
Epoch 3/10000
188/188 - 293s - loss: 1.3786 - accuracy: 0.5434
dev acc=55.32%
Epoch 4/10000
188/188 - 293s - loss: 1.3661 - accuracy: 0.5422
dev acc=55.27%
Epoch 5/10000
188/188 - 293s - loss: 1.3579 - accuracy: 0.5448
dev acc=55.43%
Epoch 6/10000
188/188 - 293s - loss: 1.3526 - accuracy: 0.5460
dev acc=55.60%
test acc=55.79%
Epoch 7/10000
188/188 - 293s - loss: 1.3554 - accuracy: 0.5475
dev acc=55.53%
Epoch 8/10000
188/188 - 293s - loss: 1.3536 - accuracy: 0.5461
dev acc=55.68%
test acc=55.98%
Epoch 9/10000
188/188 - 293s - loss: 1.3464 - accuracy: 0.5475
dev acc=55.77%
test acc=55.97%
Epoch 10/10000
188/188 - 293s - loss: 1.3487 - accuracy: 0.5459
dev acc=55.77%
Epoch 11/10000
188/188 - 293s - loss: 1.3451 - accuracy: 0.5462
dev acc=55.58%
Epoch 12/10000
188/188 - 293s - loss: 1.3471 - accuracy: 0.5464
dev acc=55.92%
test acc=55.88%
Epoch 13/10000
188/188 - 293s - loss: 1.3432 - accuracy: 0.5469
dev acc=55.90%
Epoch 14/10000
188/188 - 293s - loss: 1.3421 - accuracy: 0.5479
dev acc=55.57%
Epoch 15/10000
188/188 - 293s - loss: 1.3476 - accuracy: 0.5466
dev acc=55.72%
Epoch 16/10000
188/188 - 293s - loss: 1.3502 - accuracy: 0.5460
dev acc=55.73%
Epoch 17/10000
188/188 - 293s - loss: 1.3446 - accuracy: 0.5502
dev acc=55.49%
time used=6141.2s
2022-05-12 05:10:20.584030: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_roberta_L-4_H-312_A-12/bert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_roberta_L-4_H-312_A-12/bert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='bert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_roberta_L-4_H-312_A-12/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_culture': 0, 'news_finance': 1, 'news_world': 2, 'news_game': 3, 'news_edu': 4, 'news_stock': 5, 'news_tech': 6, 'news_sports': 7, 'news_house': 8, 'news_entertainment': 9, 'news_travel': 10, 'news_military': 11, 'news_agriculture': 12, 'news_story': 13, 'news_car': 14}
index_labels_dict={0: 'news_culture', 1: 'news_finance', 2: 'news_world', 3: 'news_game', 4: 'news_edu', 5: 'news_stock', 6: 'news_tech', 7: 'news_sports', 8: 'news_house', 9: 'news_entertainment', 10: 'news_travel', 11: 'news_military', 12: 'news_agriculture', 13: 'news_story', 14: 'news_car'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-12 05:10:28.776385: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-12 05:10:29.584351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 128)    2704384     Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Mapping (Dense)       (None, None, 312)    40248       Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 312)    390624      Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 312)    0           Embedding-Mapping[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 312)    624         Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 312)    780312      Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 312)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 312)    624         Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 312)    390624      Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 312)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 312)    624         Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 312)    780312      Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 312)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 312)    624         Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 312)    390624      Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 312)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 312)    624         Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 312)    780312      Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 312)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 312)    624         Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 312)    390624      Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 312)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 312)    624         Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 312)    780312      Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 312)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 312)    624         Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 312)          0           Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 312)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           4695        dropout[0][0]                    
==================================================================================================
Total params: 7,504,111
Trainable params: 4,695
Non-trainable params: 7,499,416
__________________________________________________________________________________________________
2022-05-12 05:10:31.092242: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 18s - loss: 2.1589 - accuracy: 0.3302
dev acc=43.76%
test acc=44.07%
Epoch 2/10000
188/188 - 12s - loss: 1.8692 - accuracy: 0.4074
dev acc=44.98%
test acc=46.06%
Epoch 3/10000
188/188 - 14s - loss: 1.8142 - accuracy: 0.4194
dev acc=45.54%
test acc=46.72%
Epoch 4/10000
188/188 - 15s - loss: 1.7905 - accuracy: 0.4236
dev acc=45.97%
test acc=47.13%
Epoch 5/10000
188/188 - 17s - loss: 1.7887 - accuracy: 0.4219
dev acc=46.33%
test acc=47.25%
Epoch 6/10000
188/188 - 15s - loss: 1.7822 - accuracy: 0.4247
dev acc=46.35%
test acc=47.32%
Epoch 7/10000
188/188 - 14s - loss: 1.7802 - accuracy: 0.4256
dev acc=46.23%
Epoch 8/10000
188/188 - 17s - loss: 1.7744 - accuracy: 0.4262
dev acc=46.42%
test acc=47.48%
Epoch 9/10000
188/188 - 17s - loss: 1.7732 - accuracy: 0.4278
dev acc=46.33%
Epoch 10/10000
188/188 - 17s - loss: 1.7793 - accuracy: 0.4255
dev acc=46.51%
test acc=47.66%
Epoch 11/10000
188/188 - 17s - loss: 1.7721 - accuracy: 0.4261
dev acc=46.59%
test acc=47.88%
Epoch 12/10000
188/188 - 14s - loss: 1.7787 - accuracy: 0.4277
dev acc=46.63%
test acc=47.55%
Epoch 13/10000
188/188 - 13s - loss: 1.7761 - accuracy: 0.4251
dev acc=46.66%
test acc=47.39%
Epoch 14/10000
188/188 - 17s - loss: 1.7781 - accuracy: 0.4266
dev acc=46.55%
Epoch 15/10000
188/188 - 17s - loss: 1.7720 - accuracy: 0.4289
dev acc=46.72%
test acc=47.50%
Epoch 16/10000
188/188 - 17s - loss: 1.7746 - accuracy: 0.4265
dev acc=46.53%
Epoch 17/10000
188/188 - 15s - loss: 1.7729 - accuracy: 0.4270
dev acc=46.53%
Epoch 18/10000
188/188 - 14s - loss: 1.7746 - accuracy: 0.4258
dev acc=46.63%
Epoch 19/10000
188/188 - 13s - loss: 1.7746 - accuracy: 0.4259
dev acc=46.61%
Epoch 20/10000
188/188 - 12s - loss: 1.7762 - accuracy: 0.4284
dev acc=46.74%
test acc=47.79%
Epoch 21/10000
188/188 - 17s - loss: 1.7735 - accuracy: 0.4282
dev acc=46.65%
Epoch 22/10000
188/188 - 17s - loss: 1.7742 - accuracy: 0.4265
dev acc=46.46%
Epoch 23/10000
188/188 - 18s - loss: 1.7723 - accuracy: 0.4274
dev acc=46.63%
Epoch 24/10000
188/188 - 16s - loss: 1.7752 - accuracy: 0.4254
dev acc=46.98%
test acc=47.67%
Epoch 25/10000
188/188 - 16s - loss: 1.7699 - accuracy: 0.4288
dev acc=46.78%
Epoch 26/10000
188/188 - 13s - loss: 1.7772 - accuracy: 0.4252
dev acc=46.61%
Epoch 27/10000
188/188 - 15s - loss: 1.7725 - accuracy: 0.4294
dev acc=46.66%
Epoch 28/10000
188/188 - 13s - loss: 1.7697 - accuracy: 0.4274
dev acc=46.76%
Epoch 29/10000
188/188 - 15s - loss: 1.7724 - accuracy: 0.4255
dev acc=46.55%
time used=700.5s
2022-05-12 05:22:04.749261: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_roberta_L-4_H-312_A-12_K-104/bert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_roberta_L-4_H-312_A-12_K-104/bert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='bert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_roberta_L-4_H-312_A-12_K-104/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_finance': 0, 'news_house': 1, 'news_travel': 2, 'news_military': 3, 'news_stock': 4, 'news_agriculture': 5, 'news_sports': 6, 'news_culture': 7, 'news_world': 8, 'news_story': 9, 'news_tech': 10, 'news_edu': 11, 'news_game': 12, 'news_entertainment': 13, 'news_car': 14}
index_labels_dict={0: 'news_finance', 1: 'news_house', 2: 'news_travel', 3: 'news_military', 4: 'news_stock', 5: 'news_agriculture', 6: 'news_sports', 7: 'news_culture', 8: 'news_world', 9: 'news_story', 10: 'news_tech', 11: 'news_edu', 12: 'news_game', 13: 'news_entertainment', 14: 'news_car'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-12 05:22:13.055701: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-12 05:22:13.858268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 128)    2704384     Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Mapping (Dense)       (None, None, 312)    40248       Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 312)    976560      Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 312)    0           Embedding-Mapping[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 312)    624         Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 312)    780312      Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 312)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 312)    624         Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 312)    976560      Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 312)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 312)    624         Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 312)    780312      Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 312)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 312)    624         Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 312)    976560      Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 312)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 312)    624         Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 312)    780312      Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 312)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 312)    624         Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 312)    976560      Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 312)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 312)    624         Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 312)    780312      Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 312)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 312)    624         Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 312)          0           Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 312)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           4695        dropout[0][0]                    
==================================================================================================
Total params: 9,847,855
Trainable params: 4,695
Non-trainable params: 9,843,160
__________________________________________________________________________________________________
2022-05-12 05:22:15.306176: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 21s - loss: 2.0765 - accuracy: 0.3455
dev acc=45.35%
test acc=45.76%
Epoch 2/10000
188/188 - 17s - loss: 1.8115 - accuracy: 0.4234
dev acc=46.51%
test acc=46.99%
Epoch 3/10000
188/188 - 15s - loss: 1.7776 - accuracy: 0.4307
dev acc=46.81%
test acc=47.42%
Epoch 4/10000
188/188 - 15s - loss: 1.7627 - accuracy: 0.4333
dev acc=46.93%
test acc=47.60%
Epoch 5/10000
188/188 - 15s - loss: 1.7612 - accuracy: 0.4330
dev acc=47.28%
test acc=47.91%
Epoch 6/10000
188/188 - 16s - loss: 1.7577 - accuracy: 0.4337
dev acc=47.26%
Epoch 7/10000
188/188 - 16s - loss: 1.7571 - accuracy: 0.4354
dev acc=47.30%
test acc=47.95%
Epoch 8/10000
188/188 - 17s - loss: 1.7543 - accuracy: 0.4348
dev acc=47.32%
test acc=47.94%
Epoch 9/10000
188/188 - 15s - loss: 1.7507 - accuracy: 0.4362
dev acc=47.28%
Epoch 10/10000
188/188 - 15s - loss: 1.7570 - accuracy: 0.4325
dev acc=47.30%
Epoch 11/10000
188/188 - 16s - loss: 1.7531 - accuracy: 0.4339
dev acc=47.41%
test acc=48.10%
Epoch 12/10000
188/188 - 14s - loss: 1.7589 - accuracy: 0.4346
dev acc=47.54%
test acc=48.08%
Epoch 13/10000
188/188 - 14s - loss: 1.7548 - accuracy: 0.4330
dev acc=47.11%
Epoch 14/10000
188/188 - 16s - loss: 1.7590 - accuracy: 0.4329
dev acc=47.47%
Epoch 15/10000
188/188 - 16s - loss: 1.7556 - accuracy: 0.4347
dev acc=47.69%
test acc=47.98%
Epoch 16/10000
188/188 - 17s - loss: 1.7520 - accuracy: 0.4383
dev acc=47.64%
Epoch 17/10000
188/188 - 18s - loss: 1.7504 - accuracy: 0.4332
dev acc=47.64%
Epoch 18/10000
188/188 - 17s - loss: 1.7556 - accuracy: 0.4366
dev acc=47.53%
Epoch 19/10000
188/188 - 17s - loss: 1.7493 - accuracy: 0.4366
dev acc=47.71%
test acc=47.98%
Epoch 20/10000
188/188 - 16s - loss: 1.7569 - accuracy: 0.4352
dev acc=47.75%
test acc=48.04%
Epoch 21/10000
188/188 - 17s - loss: 1.7570 - accuracy: 0.4333
dev acc=47.47%
Epoch 22/10000
188/188 - 17s - loss: 1.7537 - accuracy: 0.4362
dev acc=47.40%
Epoch 23/10000
188/188 - 17s - loss: 1.7547 - accuracy: 0.4349
dev acc=47.34%
Epoch 24/10000
188/188 - 14s - loss: 1.7567 - accuracy: 0.4327
dev acc=47.53%
Epoch 25/10000
188/188 - 17s - loss: 1.7540 - accuracy: 0.4342
dev acc=47.66%
time used=642.9s
2022-05-12 05:32:51.414626: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_roberta_L-6_H-384_A-12/bert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_roberta_L-6_H-384_A-12/bert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='bert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_roberta_L-6_H-384_A-12/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_edu': 0, 'news_car': 1, 'news_travel': 2, 'news_tech': 3, 'news_world': 4, 'news_military': 5, 'news_agriculture': 6, 'news_house': 7, 'news_entertainment': 8, 'news_culture': 9, 'news_stock': 10, 'news_game': 11, 'news_story': 12, 'news_finance': 13, 'news_sports': 14}
index_labels_dict={0: 'news_edu', 1: 'news_car', 2: 'news_travel', 3: 'news_tech', 4: 'news_world', 5: 'news_military', 6: 'news_agriculture', 7: 'news_house', 8: 'news_entertainment', 9: 'news_culture', 10: 'news_stock', 11: 'news_game', 12: 'news_story', 13: 'news_finance', 14: 'news_sports'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-12 05:32:59.303149: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-12 05:33:00.123858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 128)    2704384     Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Mapping (Dense)       (None, None, 384)    49536       Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 384)    591360      Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 384)    0           Embedding-Mapping[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 384)    768         Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 384)    1181568     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 384)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 384)    768         Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 384)    591360      Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 384)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 384)    768         Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 384)    1181568     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 384)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 384)    768         Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 384)    591360      Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 384)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 384)    768         Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 384)    1181568     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 384)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 384)    768         Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 384)    591360      Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 384)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 384)    768         Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 384)    1181568     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 384)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 384)    768         Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 384)    591360      Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 384)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 384)    768         Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 384)    1181568     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 384)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 384)    768         Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 384)    591360      Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 384)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 384)    768         Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 384)    1181568     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 384)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 384)    768         Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 384)          0           Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 384)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           5775        dropout[0][0]                    
==================================================================================================
Total params: 13,472,527
Trainable params: 5,775
Non-trainable params: 13,466,752
__________________________________________________________________________________________________
2022-05-12 05:33:01.897704: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 26s - loss: 2.1291 - accuracy: 0.3411
dev acc=44.10%
test acc=44.67%
Epoch 2/10000
188/188 - 20s - loss: 1.8229 - accuracy: 0.4217
dev acc=46.23%
test acc=46.43%
Epoch 3/10000
188/188 - 20s - loss: 1.7649 - accuracy: 0.4323
dev acc=46.21%
Epoch 4/10000
188/188 - 20s - loss: 1.7500 - accuracy: 0.4367
dev acc=46.72%
test acc=47.61%
Epoch 5/10000
188/188 - 20s - loss: 1.7400 - accuracy: 0.4407
dev acc=47.04%
test acc=47.93%
Epoch 6/10000
188/188 - 20s - loss: 1.7288 - accuracy: 0.4414
dev acc=47.10%
test acc=48.16%
Epoch 7/10000
188/188 - 20s - loss: 1.7317 - accuracy: 0.4394
dev acc=47.34%
test acc=48.37%
Epoch 8/10000
188/188 - 20s - loss: 1.7337 - accuracy: 0.4383
dev acc=46.96%
Epoch 9/10000
188/188 - 20s - loss: 1.7223 - accuracy: 0.4411
dev acc=47.17%
Epoch 10/10000
188/188 - 20s - loss: 1.7251 - accuracy: 0.4421
dev acc=47.17%
Epoch 11/10000
188/188 - 20s - loss: 1.7271 - accuracy: 0.4382
dev acc=47.08%
Epoch 12/10000
188/188 - 20s - loss: 1.7223 - accuracy: 0.4429
dev acc=46.95%
time used=391.5s
2022-05-12 05:39:26.156566: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_roberta_L-6_H-384_A-12_K-128/bert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_roberta_L-6_H-384_A-12_K-128/bert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='bert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_roberta_L-6_H-384_A-12_K-128/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_finance': 0, 'news_culture': 1, 'news_story': 2, 'news_agriculture': 3, 'news_house': 4, 'news_edu': 5, 'news_military': 6, 'news_travel': 7, 'news_world': 8, 'news_entertainment': 9, 'news_stock': 10, 'news_sports': 11, 'news_car': 12, 'news_game': 13, 'news_tech': 14}
index_labels_dict={0: 'news_finance', 1: 'news_culture', 2: 'news_story', 3: 'news_agriculture', 4: 'news_house', 5: 'news_edu', 6: 'news_military', 7: 'news_travel', 8: 'news_world', 9: 'news_entertainment', 10: 'news_stock', 11: 'news_sports', 12: 'news_car', 13: 'news_game', 14: 'news_tech'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-12 05:39:34.225975: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-12 05:39:35.098800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 128)    2704384     Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Mapping (Dense)       (None, None, 384)    49536       Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 384)    1478400     Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 384)    0           Embedding-Mapping[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 384)    768         Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 384)    1181568     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 384)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 384)    768         Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 384)    1478400     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 384)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 384)    768         Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 384)    1181568     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 384)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 384)    768         Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 384)    1478400     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 384)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 384)    768         Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 384)    1181568     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 384)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 384)    768         Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 384)    1478400     Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 384)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 384)    768         Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 384)    1181568     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 384)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 384)    768         Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 384)    1478400     Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 384)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 384)    768         Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 384)    1181568     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 384)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 384)    768         Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 384)    1478400     Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 384)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 384)    768         Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 384)    1181568     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 384)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 384)    768         Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 384)          0           Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 384)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           5775        dropout[0][0]                    
==================================================================================================
Total params: 18,794,767
Trainable params: 5,775
Non-trainable params: 18,788,992
__________________________________________________________________________________________________
2022-05-12 05:39:36.959488: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 32s - loss: 2.0829 - accuracy: 0.3409
dev acc=45.33%
test acc=45.35%
Epoch 2/10000
188/188 - 26s - loss: 1.7896 - accuracy: 0.4298
dev acc=46.55%
test acc=47.18%
Epoch 3/10000
188/188 - 26s - loss: 1.7434 - accuracy: 0.4389
dev acc=47.40%
test acc=47.58%
Epoch 4/10000
188/188 - 26s - loss: 1.7357 - accuracy: 0.4420
dev acc=47.43%
test acc=47.68%
Epoch 5/10000
188/188 - 26s - loss: 1.7275 - accuracy: 0.4431
dev acc=47.66%
test acc=47.90%
Epoch 6/10000
188/188 - 26s - loss: 1.7186 - accuracy: 0.4444
dev acc=47.81%
test acc=48.19%
Epoch 7/10000
188/188 - 26s - loss: 1.7192 - accuracy: 0.4449
dev acc=48.03%
test acc=48.21%
Epoch 8/10000
188/188 - 26s - loss: 1.7233 - accuracy: 0.4436
dev acc=47.75%
Epoch 9/10000
188/188 - 26s - loss: 1.7167 - accuracy: 0.4452
dev acc=47.73%
Epoch 10/10000
188/188 - 26s - loss: 1.7161 - accuracy: 0.4481
dev acc=48.11%
test acc=48.36%
Epoch 11/10000
188/188 - 26s - loss: 1.7166 - accuracy: 0.4452
dev acc=48.11%
Epoch 12/10000
188/188 - 26s - loss: 1.7157 - accuracy: 0.4464
dev acc=48.05%
Epoch 13/10000
188/188 - 26s - loss: 1.7174 - accuracy: 0.4485
dev acc=48.05%
Epoch 14/10000
188/188 - 26s - loss: 1.7175 - accuracy: 0.4471
dev acc=48.24%
test acc=48.37%
Epoch 15/10000
188/188 - 26s - loss: 1.7189 - accuracy: 0.4452
dev acc=47.96%
Epoch 16/10000
188/188 - 26s - loss: 1.7161 - accuracy: 0.4462
dev acc=47.81%
Epoch 17/10000
188/188 - 26s - loss: 1.7137 - accuracy: 0.4467
dev acc=48.18%
Epoch 18/10000
188/188 - 26s - loss: 1.7130 - accuracy: 0.4440
dev acc=48.29%
test acc=48.40%
Epoch 19/10000
188/188 - 26s - loss: 1.7181 - accuracy: 0.4432
dev acc=47.94%
Epoch 20/10000
188/188 - 26s - loss: 1.7142 - accuracy: 0.4458
dev acc=48.14%
Epoch 21/10000
188/188 - 26s - loss: 1.7194 - accuracy: 0.4472
dev acc=48.14%
Epoch 22/10000
188/188 - 26s - loss: 1.7161 - accuracy: 0.4448
dev acc=47.92%
Epoch 23/10000
188/188 - 26s - loss: 1.7122 - accuracy: 0.4475
dev acc=48.14%
time used=889.2s
2022-05-12 05:54:18.804256: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_simbert_L-12_H-768_A-12/bert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_simbert_L-12_H-768_A-12/bert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='bert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_simbert_L-12_H-768_A-12/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_car': 0, 'news_military': 1, 'news_game': 2, 'news_tech': 3, 'news_house': 4, 'news_entertainment': 5, 'news_finance': 6, 'news_travel': 7, 'news_world': 8, 'news_stock': 9, 'news_story': 10, 'news_culture': 11, 'news_sports': 12, 'news_agriculture': 13, 'news_edu': 14}
index_labels_dict={0: 'news_car', 1: 'news_military', 2: 'news_game', 3: 'news_tech', 4: 'news_house', 5: 'news_entertainment', 6: 'news_finance', 7: 'news_travel', 8: 'news_world', 9: 'news_stock', 10: 'news_story', 11: 'news_culture', 12: 'news_sports', 13: 'news_agriculture', 14: 'news_edu'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-12 05:54:26.834337: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-12 05:54:27.704851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 768)    10510080    Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 768)          0           Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
dropout (Dropout)               (None, 768)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           11535       dropout[0][0]                    
==================================================================================================
Total params: 95,972,367
Trainable params: 11,535
Non-trainable params: 95,960,832
__________________________________________________________________________________________________
2022-05-12 05:54:31.263866: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 109s - loss: 1.7460 - accuracy: 0.4723
dev acc=54.24%
test acc=55.05%
Epoch 2/10000
188/188 - 98s - loss: 1.4032 - accuracy: 0.5350
dev acc=55.15%
test acc=55.55%
Epoch 3/10000
188/188 - 97s - loss: 1.3726 - accuracy: 0.5409
dev acc=55.19%
test acc=55.76%
Epoch 4/10000
188/188 - 97s - loss: 1.3651 - accuracy: 0.5419
dev acc=55.57%
test acc=55.82%
Epoch 5/10000
188/188 - 98s - loss: 1.3571 - accuracy: 0.5430
dev acc=55.49%
Epoch 6/10000
188/188 - 98s - loss: 1.3514 - accuracy: 0.5412
dev acc=55.90%
test acc=55.95%
Epoch 7/10000
188/188 - 98s - loss: 1.3460 - accuracy: 0.5454
dev acc=55.70%
Epoch 8/10000
188/188 - 98s - loss: 1.3483 - accuracy: 0.5447
dev acc=55.98%
test acc=55.81%
Epoch 9/10000
188/188 - 98s - loss: 1.3460 - accuracy: 0.5459
dev acc=55.88%
Epoch 10/10000
188/188 - 98s - loss: 1.3462 - accuracy: 0.5433
dev acc=56.07%
test acc=55.65%
Epoch 11/10000
188/188 - 98s - loss: 1.3399 - accuracy: 0.5491
dev acc=55.96%
Epoch 12/10000
188/188 - 98s - loss: 1.3453 - accuracy: 0.5455
dev acc=55.81%
Epoch 13/10000
188/188 - 98s - loss: 1.3433 - accuracy: 0.5447
dev acc=55.88%
Epoch 14/10000
188/188 - 98s - loss: 1.3475 - accuracy: 0.5442
dev acc=55.85%
Epoch 15/10000
188/188 - 98s - loss: 1.3448 - accuracy: 0.5471
dev acc=55.92%
time used=1942.5s
2022-05-12 06:26:44.839498: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_simbert_L-4_H-312_A-12/bert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_simbert_L-4_H-312_A-12/bert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='bert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_simbert_L-4_H-312_A-12/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_agriculture': 0, 'news_entertainment': 1, 'news_culture': 2, 'news_car': 3, 'news_sports': 4, 'news_game': 5, 'news_finance': 6, 'news_military': 7, 'news_world': 8, 'news_travel': 9, 'news_edu': 10, 'news_house': 11, 'news_tech': 12, 'news_story': 13, 'news_stock': 14}
index_labels_dict={0: 'news_agriculture', 1: 'news_entertainment', 2: 'news_culture', 3: 'news_car', 4: 'news_sports', 5: 'news_game', 6: 'news_finance', 7: 'news_military', 8: 'news_world', 9: 'news_travel', 10: 'news_edu', 11: 'news_house', 12: 'news_tech', 13: 'news_story', 14: 'news_stock'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-12 06:26:52.661933: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-12 06:26:53.568524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 128)    1751680     Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Mapping (Dense)       (None, None, 312)    40248       Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 312)    390624      Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 312)    0           Embedding-Mapping[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 312)    624         Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 312)    780312      Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 312)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 312)    624         Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 312)    390624      Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 312)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 312)    624         Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 312)    780312      Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 312)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 312)    624         Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 312)    390624      Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 312)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 312)    624         Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 312)    780312      Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 312)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 312)    624         Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 312)    390624      Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 312)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 312)    624         Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 312)    780312      Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 312)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 312)    624         Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 312)          0           Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 312)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           4695        dropout[0][0]                    
==================================================================================================
Total params: 6,551,407
Trainable params: 4,695
Non-trainable params: 6,546,712
__________________________________________________________________________________________________
2022-05-12 06:26:54.980221: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 17s - loss: 2.1325 - accuracy: 0.3320
dev acc=44.51%
test acc=45.56%
Epoch 2/10000
188/188 - 14s - loss: 1.8505 - accuracy: 0.4136
dev acc=45.88%
test acc=46.63%
Epoch 3/10000
188/188 - 16s - loss: 1.8074 - accuracy: 0.4223
dev acc=46.65%
test acc=47.40%
Epoch 4/10000
188/188 - 15s - loss: 1.7875 - accuracy: 0.4240
dev acc=46.72%
test acc=47.55%
Epoch 5/10000
188/188 - 15s - loss: 1.7864 - accuracy: 0.4249
dev acc=46.85%
test acc=47.43%
Epoch 6/10000
188/188 - 17s - loss: 1.7812 - accuracy: 0.4251
dev acc=47.13%
test acc=47.69%
Epoch 7/10000
188/188 - 17s - loss: 1.7754 - accuracy: 0.4270
dev acc=47.19%
test acc=47.95%
Epoch 8/10000
188/188 - 16s - loss: 1.7777 - accuracy: 0.4266
dev acc=47.26%
test acc=47.81%
Epoch 9/10000
188/188 - 16s - loss: 1.7782 - accuracy: 0.4278
dev acc=47.08%
Epoch 10/10000
188/188 - 16s - loss: 1.7779 - accuracy: 0.4267
dev acc=47.26%
Epoch 11/10000
188/188 - 17s - loss: 1.7734 - accuracy: 0.4268
dev acc=46.98%
Epoch 12/10000
188/188 - 15s - loss: 1.7761 - accuracy: 0.4281
dev acc=47.26%
Epoch 13/10000
188/188 - 14s - loss: 1.7791 - accuracy: 0.4253
dev acc=47.19%
time used=339.3s
2022-05-12 06:32:27.374337: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_simbert_L-6_H-384_A-12/bert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_simbert_L-6_H-384_A-12/bert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='bert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_simbert_L-6_H-384_A-12/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_entertainment': 0, 'news_game': 1, 'news_sports': 2, 'news_story': 3, 'news_stock': 4, 'news_military': 5, 'news_car': 6, 'news_travel': 7, 'news_agriculture': 8, 'news_finance': 9, 'news_edu': 10, 'news_world': 11, 'news_culture': 12, 'news_house': 13, 'news_tech': 14}
index_labels_dict={0: 'news_entertainment', 1: 'news_game', 2: 'news_sports', 3: 'news_story', 4: 'news_stock', 5: 'news_military', 6: 'news_car', 7: 'news_travel', 8: 'news_agriculture', 9: 'news_finance', 10: 'news_edu', 11: 'news_world', 12: 'news_culture', 13: 'news_house', 14: 'news_tech'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-12 06:32:35.318135: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-12 06:32:36.222960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 128)    1751680     Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Mapping (Dense)       (None, None, 384)    49536       Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 384)    591360      Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
                                                                 Embedding-Mapping[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 384)    0           Embedding-Mapping[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 384)    768         Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 384)    1181568     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 384)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 384)    768         Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 384)    591360      Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 384)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 384)    768         Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 384)    1181568     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 384)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 384)    768         Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 384)    591360      Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 384)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 384)    768         Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 384)    1181568     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 384)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 384)    768         Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 384)    591360      Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 384)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 384)    768         Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 384)    1181568     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 384)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 384)    768         Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 384)    591360      Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 384)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 384)    768         Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 384)    1181568     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 384)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 384)    768         Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 384)    591360      Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 384)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 384)    768         Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 384)    1181568     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 384)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 384)    768         Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 384)          0           Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 384)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           5775        dropout[0][0]                    
==================================================================================================
Total params: 12,519,823
Trainable params: 5,775
Non-trainable params: 12,514,048
__________________________________________________________________________________________________
2022-05-12 06:32:37.976240: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 26s - loss: 2.0796 - accuracy: 0.3522
dev acc=46.48%
test acc=47.13%
Epoch 2/10000
188/188 - 20s - loss: 1.7854 - accuracy: 0.4333
dev acc=47.25%
test acc=48.11%
Epoch 3/10000
188/188 - 20s - loss: 1.7393 - accuracy: 0.4411
dev acc=47.45%
test acc=48.40%
Epoch 4/10000
188/188 - 20s - loss: 1.7308 - accuracy: 0.4438
dev acc=47.75%
test acc=48.69%
Epoch 5/10000
188/188 - 20s - loss: 1.7249 - accuracy: 0.4463
dev acc=47.69%
Epoch 6/10000
188/188 - 20s - loss: 1.7177 - accuracy: 0.4463
dev acc=48.09%
test acc=48.98%
Epoch 7/10000
188/188 - 20s - loss: 1.7218 - accuracy: 0.4461
dev acc=48.18%
test acc=49.04%
Epoch 8/10000
188/188 - 20s - loss: 1.7263 - accuracy: 0.4441
dev acc=48.39%
test acc=49.35%
Epoch 9/10000
188/188 - 20s - loss: 1.7143 - accuracy: 0.4461
dev acc=48.50%
test acc=49.16%
Epoch 10/10000
188/188 - 20s - loss: 1.7162 - accuracy: 0.4487
dev acc=48.35%
Epoch 11/10000
188/188 - 20s - loss: 1.7125 - accuracy: 0.4514
dev acc=48.22%
Epoch 12/10000
188/188 - 20s - loss: 1.7148 - accuracy: 0.4457
dev acc=48.26%
Epoch 13/10000
188/188 - 20s - loss: 1.7139 - accuracy: 0.4493
dev acc=48.37%
Epoch 14/10000
188/188 - 20s - loss: 1.7175 - accuracy: 0.4457
dev acc=48.56%
test acc=49.20%
Epoch 15/10000
188/188 - 20s - loss: 1.7144 - accuracy: 0.4476
dev acc=48.48%
Epoch 16/10000
188/188 - 20s - loss: 1.7183 - accuracy: 0.4455
dev acc=48.50%
Epoch 17/10000
188/188 - 20s - loss: 1.7118 - accuracy: 0.4489
dev acc=48.59%
test acc=49.22%
Epoch 18/10000
188/188 - 20s - loss: 1.7153 - accuracy: 0.4471
dev acc=48.54%
Epoch 19/10000
188/188 - 20s - loss: 1.7151 - accuracy: 0.4499
dev acc=48.37%
Epoch 20/10000
188/188 - 20s - loss: 1.7106 - accuracy: 0.4498
dev acc=48.61%
test acc=49.27%
Epoch 21/10000
188/188 - 20s - loss: 1.7170 - accuracy: 0.4464
dev acc=48.63%
test acc=49.26%
Epoch 22/10000
188/188 - 20s - loss: 1.7154 - accuracy: 0.4481
dev acc=48.39%
Epoch 23/10000
188/188 - 20s - loss: 1.7153 - accuracy: 0.4484
dev acc=48.39%
Epoch 24/10000
188/188 - 20s - loss: 1.7120 - accuracy: 0.4490
dev acc=48.26%
Epoch 25/10000
188/188 - 20s - loss: 1.7137 - accuracy: 0.4473
dev acc=48.52%
Epoch 26/10000
188/188 - 20s - loss: 1.7147 - accuracy: 0.4465
dev acc=48.41%
time used=813.5s
2022-05-12 06:46:04.231227: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_wobert_L-12_H-768_A-12/bert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_wobert_L-12_H-768_A-12/bert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='bert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_wobert_L-12_H-768_A-12/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_edu': 0, 'news_entertainment': 1, 'news_military': 2, 'news_game': 3, 'news_car': 4, 'news_stock': 5, 'news_finance': 6, 'news_agriculture': 7, 'news_story': 8, 'news_sports': 9, 'news_culture': 10, 'news_house': 11, 'news_travel': 12, 'news_tech': 13, 'news_world': 14}
index_labels_dict={0: 'news_edu', 1: 'news_entertainment', 2: 'news_military', 3: 'news_game', 4: 'news_car', 5: 'news_stock', 6: 'news_finance', 7: 'news_agriculture', 8: 'news_story', 9: 'news_sports', 10: 'news_culture', 11: 'news_house', 12: 'news_travel', 13: 'news_tech', 14: 'news_world'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-12 06:46:12.268385: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-12 06:46:13.146980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 768)    25794048    Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 768)          0           Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
dropout (Dropout)               (None, 768)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           11535       dropout[0][0]                    
==================================================================================================
Total params: 111,256,335
Trainable params: 11,535
Non-trainable params: 111,244,800
__________________________________________________________________________________________________
2022-05-12 06:46:16.887358: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 108s - loss: 1.7640 - accuracy: 0.4720
dev acc=54.76%
test acc=55.43%
Epoch 2/10000
188/188 - 97s - loss: 1.4072 - accuracy: 0.5358
dev acc=55.19%
test acc=56.07%
Epoch 3/10000
188/188 - 97s - loss: 1.3682 - accuracy: 0.5400
dev acc=55.73%
test acc=56.19%
Epoch 4/10000
188/188 - 98s - loss: 1.3542 - accuracy: 0.5440
dev acc=55.70%
Epoch 5/10000
188/188 - 98s - loss: 1.3494 - accuracy: 0.5455
dev acc=55.88%
test acc=55.99%
Epoch 6/10000
188/188 - 98s - loss: 1.3414 - accuracy: 0.5451
dev acc=55.90%
test acc=56.14%
Epoch 7/10000
188/188 - 98s - loss: 1.3362 - accuracy: 0.5475
dev acc=56.09%
test acc=56.21%
Epoch 8/10000
188/188 - 98s - loss: 1.3376 - accuracy: 0.5460
dev acc=55.75%
Epoch 9/10000
188/188 - 98s - loss: 1.3369 - accuracy: 0.5465
dev acc=55.77%
Epoch 10/10000
188/188 - 98s - loss: 1.3321 - accuracy: 0.5472
dev acc=55.79%
Epoch 11/10000
188/188 - 98s - loss: 1.3310 - accuracy: 0.5496
dev acc=55.96%
Epoch 12/10000
188/188 - 98s - loss: 1.3351 - accuracy: 0.5459
dev acc=55.87%
time used=1572.6s
2022-05-12 07:12:20.264724: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_wobert_plus_L-12_H-768_A-12/bert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_wobert_plus_L-12_H-768_A-12/bert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='bert', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_wobert_plus_L-12_H-768_A-12/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_culture': 0, 'news_world': 1, 'news_entertainment': 2, 'news_travel': 3, 'news_car': 4, 'news_house': 5, 'news_edu': 6, 'news_agriculture': 7, 'news_story': 8, 'news_sports': 9, 'news_stock': 10, 'news_finance': 11, 'news_game': 12, 'news_military': 13, 'news_tech': 14}
index_labels_dict={0: 'news_culture', 1: 'news_world', 2: 'news_entertainment', 3: 'news_travel', 4: 'news_car', 5: 'news_house', 6: 'news_edu', 7: 'news_agriculture', 8: 'news_story', 9: 'news_sports', 10: 'news_stock', 11: 'news_finance', 12: 'news_game', 13: 'news_military', 14: 'news_tech'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	8
49	7
48	10
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	175
36	173
35	232
34	318
33	609
32	3432
31	2668
30	2386
29	2214
28	2255
27	2157
26	2235
25	2196
24	2307
23	2329
22	2199
21	2225
20	2012
19	2267
18	1894
17	1970
16	1774
15	1598
14	1486
13	1197
12	964
11	792
10	545
9	333
8	145
7	129
6	21
5	2
4	4
max_sent_len=50
2022-05-12 07:12:28.566886: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-12 07:12:29.487219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 768)    38400000    Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 768)          0           Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
dropout (Dropout)               (None, 768)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           11535       dropout[0][0]                    
==================================================================================================
Total params: 123,862,287
Trainable params: 11,535
Non-trainable params: 123,850,752
__________________________________________________________________________________________________
2022-05-12 07:12:33.450928: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 109s - loss: 1.7491 - accuracy: 0.4726
dev acc=54.12%
test acc=54.83%
Epoch 2/10000
188/188 - 97s - loss: 1.4167 - accuracy: 0.5314
dev acc=54.82%
test acc=55.33%
Epoch 3/10000
188/188 - 98s - loss: 1.3821 - accuracy: 0.5368
dev acc=55.06%
test acc=55.32%
Epoch 4/10000
188/188 - 98s - loss: 1.3713 - accuracy: 0.5390
dev acc=55.23%
test acc=55.57%
Epoch 5/10000
188/188 - 98s - loss: 1.3677 - accuracy: 0.5401
dev acc=55.30%
test acc=55.52%
Epoch 6/10000
188/188 - 98s - loss: 1.3590 - accuracy: 0.5393
dev acc=55.42%
test acc=55.64%
Epoch 7/10000
188/188 - 98s - loss: 1.3561 - accuracy: 0.5424
dev acc=55.66%
test acc=55.68%
Epoch 8/10000
188/188 - 98s - loss: 1.3573 - accuracy: 0.5411
dev acc=55.25%
Epoch 9/10000
188/188 - 98s - loss: 1.3562 - accuracy: 0.5412
dev acc=55.49%
Epoch 10/10000
188/188 - 98s - loss: 1.3497 - accuracy: 0.5411
dev acc=55.38%
Epoch 11/10000
188/188 - 98s - loss: 1.3515 - accuracy: 0.5435
dev acc=55.27%
Epoch 12/10000
188/188 - 98s - loss: 1.3550 - accuracy: 0.5420
dev acc=55.62%
time used=1606.7s
2022-05-12 07:39:10.724976: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
2.6.0
2.6.0
0.11.3
Namespace(batch_size=256, checkpoint_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_wonezha_L-12_H-768_A-12/bert_model.ckpt', config_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_wonezha_L-12_H-768_A-12/bert_config.json', dropout=0.5, epochs=10000, freeze=True, gpu='3', learning_rate=0.001, max_sent_len=None, max_sent_len_ratio=0.99971, model_type='nezha', patience=5, seed=2022, vocab_path='/data0/nfs_data/zhaoxi9/pretrained_language_model/zhuiyi/chinese_wonezha_L-12_H-768_A-12/vocab.txt')
loading data...
train size=48024
dev size=5336
test size=10000
labels_index_dict={'news_finance': 0, 'news_entertainment': 1, 'news_edu': 2, 'news_game': 3, 'news_military': 4, 'news_travel': 5, 'news_agriculture': 6, 'news_culture': 7, 'news_sports': 8, 'news_car': 9, 'news_world': 10, 'news_story': 11, 'news_stock': 12, 'news_tech': 13, 'news_house': 14}
index_labels_dict={0: 'news_finance', 1: 'news_entertainment', 2: 'news_edu', 3: 'news_game', 4: 'news_military', 5: 'news_travel', 6: 'news_agriculture', 7: 'news_culture', 8: 'news_sports', 9: 'news_car', 10: 'news_world', 11: 'news_story', 12: 'news_stock', 13: 'news_tech', 14: 'news_house'}
max_sent_len=147
147	1
60	1
56	1
55	2
54	1
53	1
52	6
51	3
50	9
49	7
48	9
47	11
46	67
45	11
44	28
43	42
42	102
41	97
40	133
39	124
38	125
37	176
36	174
35	232
34	320
33	610
32	3435
31	2667
30	2381
29	2218
28	2257
27	2157
26	2234
25	2196
24	2305
23	2329
22	2202
21	2220
20	2013
19	2272
18	1887
17	1977
16	1768
15	1598
14	1485
13	1197
12	963
11	792
10	546
9	331
8	146
7	129
6	21
5	1
4	4
max_sent_len=50
2022-05-12 07:39:18.795279: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-12 07:39:19.594943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22139 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 768)    25794048    Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Embedding-Relative-Position (Re (None, None, 64)     8256        Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
                                                                 Embedding-Dropout[0][0]          
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] 
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 768)          0           Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
dropout (Dropout)               (None, 768)          0           lambda[0][0]                     
__________________________________________________________________________________________________
dense (Dense)                   (None, 15)           11535       dropout[0][0]                    
==================================================================================================
Total params: 110,871,375
Trainable params: 11,535
Non-trainable params: 110,859,840
__________________________________________________________________________________________________
2022-05-12 07:39:23.432844: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/10000
188/188 - 116s - loss: 2.1992 - accuracy: 0.3768
dev acc=51.12%
test acc=51.42%
Epoch 2/10000
188/188 - 104s - loss: 1.7381 - accuracy: 0.5011
dev acc=53.95%
test acc=54.05%
Epoch 3/10000
188/188 - 104s - loss: 1.5753 - accuracy: 0.5182
dev acc=54.70%
test acc=55.12%
Epoch 4/10000
188/188 - 104s - loss: 1.5044 - accuracy: 0.5235
dev acc=55.00%
test acc=55.24%
Epoch 5/10000
188/188 - 105s - loss: 1.4724 - accuracy: 0.5256
dev acc=55.10%
test acc=55.47%
Epoch 6/10000
188/188 - 104s - loss: 1.4518 - accuracy: 0.5271
dev acc=55.40%
test acc=55.62%
Epoch 7/10000
188/188 - 104s - loss: 1.4408 - accuracy: 0.5296
dev acc=55.45%
test acc=55.70%
Epoch 8/10000
188/188 - 104s - loss: 1.4296 - accuracy: 0.5309
dev acc=55.49%
test acc=55.83%
Epoch 9/10000
188/188 - 104s - loss: 1.4243 - accuracy: 0.5301
dev acc=55.58%
test acc=55.78%
Epoch 10/10000
188/188 - 104s - loss: 1.4204 - accuracy: 0.5314
dev acc=55.51%
Epoch 11/10000
188/188 - 104s - loss: 1.4134 - accuracy: 0.5313
dev acc=55.55%
Epoch 12/10000
188/188 - 104s - loss: 1.4139 - accuracy: 0.5336
dev acc=55.77%
test acc=55.95%
Epoch 13/10000
188/188 - 104s - loss: 1.4143 - accuracy: 0.5335
dev acc=55.68%
Epoch 14/10000
188/188 - 104s - loss: 1.4137 - accuracy: 0.5320
dev acc=55.57%
Epoch 15/10000
188/188 - 104s - loss: 1.4080 - accuracy: 0.5320
dev acc=55.73%
Epoch 16/10000
188/188 - 104s - loss: 1.4117 - accuracy: 0.5350
dev acc=55.73%
Epoch 17/10000
188/188 - 104s - loss: 1.4068 - accuracy: 0.5327
dev acc=55.85%
test acc=56.07%
Epoch 18/10000
188/188 - 104s - loss: 1.4104 - accuracy: 0.5340
dev acc=55.68%
Epoch 19/10000
188/188 - 104s - loss: 1.4042 - accuracy: 0.5341
dev acc=55.66%
Epoch 20/10000
188/188 - 104s - loss: 1.4035 - accuracy: 0.5323
dev acc=55.70%
Epoch 21/10000
188/188 - 104s - loss: 1.4013 - accuracy: 0.5362
dev acc=55.81%
Epoch 22/10000
188/188 - 104s - loss: 1.4018 - accuracy: 0.5335
dev acc=55.70%
time used=3042.6s
2022-05-12 08:29:56.875360: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
